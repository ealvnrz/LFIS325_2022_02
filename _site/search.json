[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LFIS325: Estad칤stica para Ciencias F칤sicas",
    "section": "",
    "text": "Es un curso te칩rico/pr치ctico de modalidad presencial, de nivel intermedio, cuya misi칩n es introducir al estudiante a las principales herramientas de an치lisis. El curso est치 orientado a desarrollar en el estudiante la capacidad de transformar los datos de que dispone tanto para extraer informaci칩n 칰til como tambi칠n para facilitar las conclusiones. Aprender치 los conceptos de la teor칤a de la probabilidad y la inferencia estad칤stica que se utilizan para interpretar datos experimentales. Abordar치 los problemas tanto desde una perspectiva te칩rica como con trabajos pr치cticos."
  },
  {
    "objectID": "index.html#softwares",
    "href": "index.html#softwares",
    "title": "LFIS325: Estad칤stica para Ciencias F칤sicas",
    "section": "Softwares",
    "text": "Softwares\nPara la mayor칤a de las aplicaciones utilizaremos R, por lo que se sugiere utilizar un IDE como RStudio.\nPara la entrega de informes y talleres que requieran uso de programaci칩n, se recomienda el uso de Rmarkdown, Jupyter Notebook o \\(\\LaTeX\\) para la confecci칩n de documentos a entregar."
  },
  {
    "objectID": "pages/calendario.html",
    "href": "pages/calendario.html",
    "title": "Calendario",
    "section": "",
    "text": "Semana\nFecha\nPreparaci칩n\nPresentaci칩n\nMaterial adicional\nPrueba\nTrabajo Final\n\n\n\n\n1\n25-26/08\n游닀\n游둰勇끂n游늶\n\n\n\n\n2\n01-02/09\n游닀\n游둰勇끂n游늶\n\n\n\n\n3\n08-09/09\n游닀\n游둰勇끂n游늶\n\n\n\n\n4\n15-16/09\n游닀\n游둰勇끂n游늶\n\n\n\n\n5\n29-30/09\n游닀\n游둰勇끂n游늶\n游닇\n\n\n\n6\n06-07/10\n游닀\n游둰勇끂n游늶\n\n\n\n\n7\n13-14/10\n游닀\n游둰勇끂n游늶\n\n\n\n\n8\n20-21/10\n游닀\n游둰勇끂n游늶\n\n\n\n\n9\n27-28/10\n游닀\n游둰勇끂n游늶\n\n\n\n\n10\n10-11/11\n游닀\n游둰勇끂n游늶\n\n\n\n\n11\n17-18/11\n游닀\n游둰勇끂n游늶\n\n\n\n\n12\n24-25/11\n游닀\n游둰勇끂n游늶\n\n\n\n\n13\n01-02/11\n游닀\n游둰勇끂n游늶\n\n游늾\n\n\n14\n08-09/12\n游닀\n游둰勇끂n游늶\n游닇\n\n\n\n15\n15-16/12\n游닀\n游둰勇끂n游늶\n\n\n\n\n16\n22-23/12\n游닀\n游둰勇끂n游늶\n\n游늾"
  },
  {
    "objectID": "pages/evaluaciones.html",
    "href": "pages/evaluaciones.html",
    "title": "Evaluaciones",
    "section": "",
    "text": "Tipo de evaluaci칩n\nPorcentaje que corresponde\n\n\n\n\nEvaluaciones sumativas (2)\n50%\n\n\nPresentaciones\n30%\n\n\nTrabajo final\n20%"
  },
  {
    "objectID": "pages/programa.html",
    "href": "pages/programa.html",
    "title": "Programa del curso",
    "section": "",
    "text": "Introducci칩n a la Estad칤stica\n\nProceso Estad칤stico\nTipos de muestreo\nMedidas de tendencia muestral\nTablas de frecuencia\nTipos de gr치ficos\n\n\n\nProbabilidad en la ciencia\n\nDecisi칩n y probabilidad\nTeorema de Bayes\nInferencia y probabilidad\nAn치lisis de error simple\nUso de la estad칤stica\n\n\n\nModelamiento de datos: Estimaci칩n de par치metros\n\nEl m칠todo de la probabilidad m치xima\nM칤nimos cuadrados\nAn치lisis Bayesiano\nModelamiento Monte Carlo\nModelo de modelos y combinaci칩n de conjunto de datos\n\n\n\nDetecci칩n y B칰squeda\n\nDetecci칩n\nCat치logos y efectos de selecci칩n\nEl l칤mite de confusi칩n\n\n\n\nEstad칤stica en 1D y 2D\n\nTransformaciones de datos\nAn치lisis de Fourier\nFiltrado\nCorrelacionamiento\nEstad칤stica sobre una superficie\nRepresentaci칩n del cielo\nFunci칩n correlaci칩n angular de dos puntos\nEl espectro de potencia angular\n\n\n\nCadenas de Markov Monte Carlo\n\nAlgoritmo de Metr칩polis-Hastings\nComparaci칩n de modelos\nAplicaciones."
  },
  {
    "objectID": "pages/week1.html",
    "href": "pages/week1.html",
    "title": "Semana 1",
    "section": "",
    "text": "Leer el programa del curso"
  },
  {
    "objectID": "pages/week1.html#presentaci칩n",
    "href": "pages/week1.html#presentaci칩n",
    "title": "Semana 1",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week1.html#material-adicional",
    "href": "pages/week1.html#material-adicional",
    "title": "Semana 1",
    "section": "Material adicional",
    "text": "Material adicional\nSecciones 1.1 y 1.2 de Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "pages/week10.html#presentaci칩n",
    "href": "pages/week10.html#presentaci칩n",
    "title": "Semana 10",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week10.html#material-adicional",
    "href": "pages/week10.html#material-adicional",
    "title": "Semana 10",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week11.html#presentaci칩n",
    "href": "pages/week11.html#presentaci칩n",
    "title": "Semana 11",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week11.html#material-adicional",
    "href": "pages/week11.html#material-adicional",
    "title": "Semana 11",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week12.html#presentaci칩n",
    "href": "pages/week12.html#presentaci칩n",
    "title": "Semana 12",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week12.html#material-adicional",
    "href": "pages/week12.html#material-adicional",
    "title": "Semana 12",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week13.html#presentaci칩n",
    "href": "pages/week13.html#presentaci칩n",
    "title": "Semana 13",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week13.html#material-adicional",
    "href": "pages/week13.html#material-adicional",
    "title": "Semana 13",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week14.html#presentaci칩n",
    "href": "pages/week14.html#presentaci칩n",
    "title": "Semana 14",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week14.html#material-adicional",
    "href": "pages/week14.html#material-adicional",
    "title": "Semana 14",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week15.html#presentaci칩n",
    "href": "pages/week15.html#presentaci칩n",
    "title": "Semana 15",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week15.html#material-adicional",
    "href": "pages/week15.html#material-adicional",
    "title": "Semana 15",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week16.html#presentaci칩n",
    "href": "pages/week16.html#presentaci칩n",
    "title": "Semana 16",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week16.html#material-adicional",
    "href": "pages/week16.html#material-adicional",
    "title": "Semana 16",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week2.html",
    "href": "pages/week2.html",
    "title": "Semana 2",
    "section": "",
    "text": "Leer cap칤tulo 1 de Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "pages/week2.html#presentaci칩n",
    "href": "pages/week2.html#presentaci칩n",
    "title": "Semana 2",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week2.html#material-adicional",
    "href": "pages/week2.html#material-adicional",
    "title": "Semana 2",
    "section": "Material adicional",
    "text": "Material adicional\nSecciones 1.3 y 1.4 de Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "pages/week3.html",
    "href": "pages/week3.html",
    "title": "Semana 3",
    "section": "",
    "text": "Leer cap칤tulo 2 de Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "pages/week3.html#presentaci칩n",
    "href": "pages/week3.html#presentaci칩n",
    "title": "Semana 3",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week3.html#material-adicional",
    "href": "pages/week3.html#material-adicional",
    "title": "Semana 3",
    "section": "Material adicional",
    "text": "Material adicional\n\nGu칤a de ejercicios: estad칤stica descriptiva\nGu칤a de ejercicios: c치lculo de probabilidad\nGu칤a de ejercicios adicionales: estad칤stica descriptiva"
  },
  {
    "objectID": "pages/week4.html#presentaci칩n",
    "href": "pages/week4.html#presentaci칩n",
    "title": "Semana 4",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week4.html#material-adicional",
    "href": "pages/week4.html#material-adicional",
    "title": "Semana 4",
    "section": "Material adicional",
    "text": "Material adicional\n\nPauta Prueba #1 2020\nPauta Prueba #1 2021"
  },
  {
    "objectID": "pages/week5.html#presentaci칩n",
    "href": "pages/week5.html#presentaci칩n",
    "title": "Semana 5",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week5.html#material-adicional",
    "href": "pages/week5.html#material-adicional",
    "title": "Semana 5",
    "section": "Material adicional",
    "text": "Material adicional\nPauta Prueba #1 2022"
  },
  {
    "objectID": "pages/week6.html",
    "href": "pages/week6.html",
    "title": "Semana 6",
    "section": "",
    "text": "Leer cap칤tulo 3, Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "pages/week6.html#presentaci칩n",
    "href": "pages/week6.html#presentaci칩n",
    "title": "Semana 6",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week6.html#material-adicional",
    "href": "pages/week6.html#material-adicional",
    "title": "Semana 6",
    "section": "Material adicional",
    "text": "Material adicional\nLeer cap칤tulo 4, Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "pages/week7.html",
    "href": "pages/week7.html",
    "title": "Semana 7",
    "section": "",
    "text": "Leer cap칤tulo 5, Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "pages/week7.html#presentaci칩n",
    "href": "pages/week7.html#presentaci칩n",
    "title": "Semana 7",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week7.html#material-adicional",
    "href": "pages/week7.html#material-adicional",
    "title": "Semana 7",
    "section": "Material adicional",
    "text": "Material adicional\n\nLeer cap칤tulo 2, Statistical data analysis, Glen Cowan.\nPauta Prueba #2 2020\nPauta Prueba #2 2021\nGu칤a de ejercicios: Variables aleatorias\nGu칤a de ejercicios: Variables aleatorias continuas\nGu칤a de ejercicios: Distribuciones\nGu칤a de ejercicios: Distribuciones continuas\nGu칤a de ejercicios: Variables aleatorias bivariadas"
  },
  {
    "objectID": "pages/week8.html#presentaci칩n",
    "href": "pages/week8.html#presentaci칩n",
    "title": "Semana 8",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week8.html#material-adicional",
    "href": "pages/week8.html#material-adicional",
    "title": "Semana 8",
    "section": "Material adicional",
    "text": "Material adicional\n\nGu칤a de ejercicios I.C."
  },
  {
    "objectID": "pages/week9.html#presentaci칩n",
    "href": "pages/week9.html#presentaci칩n",
    "title": "Semana 9",
    "section": "Presentaci칩n",
    "text": "Presentaci칩n\nVer presentaci칩n en pantalla completa"
  },
  {
    "objectID": "pages/week9.html#material-adicional",
    "href": "pages/week9.html#material-adicional",
    "title": "Semana 9",
    "section": "Material adicional",
    "text": "Material adicional\n\nGu칤a de ejercicios: test de hip칩tesis para la media\nGu칤a de ejercicios: test de hip칩tesis para la proporci칩n y varianza"
  },
  {
    "objectID": "slides/lec_week1.html#descripci칩n-del-curso",
    "href": "slides/lec_week1.html#descripci칩n-del-curso",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Descripci칩n del curso",
    "text": "Descripci칩n del curso\nEs un curso te칩rico/pr치ctico de modalidad presencial, de nivel intermedio, cuya misi칩n es introducir al estudiante a las principales herramientas de an치lisis. El curso est치 orientado a desarrollar en el estudiante la capacidad de transformar los datos de que dispone tanto para extraer informaci칩n 칰til como tambi칠n para facilitar las conclusiones. Aprender치 los conceptos de la teor칤a de la probabilidad y la inferencia estad칤stica que se utilizan para interpretar datos experimentales. Abordar치 los problemas tanto desde una perspectiva te칩rica como con trabajos pr치cticos."
  },
  {
    "objectID": "slides/lec_week1.html#horario-de-clases",
    "href": "slides/lec_week1.html#horario-de-clases",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Horario de clases",
    "text": "Horario de clases\n\n\n\n\nD칤a\nHorario\nLugar\n\n\n\n\nC치tedra #1\nJueves\n08:30 am - 10:00 am\nJuan Mouat\n\n\nC치tedra #2\nViernes\n14:30 pm - 16:00 pm\nJuan Mouat\n\n\nC치tedra #3\nViernes\n16:15 pm - 17:45 pm\nJuan Mouat\n\n\n\nP치gina del curso\nUtilizaremos el Aula Virtual/Google Classroom y el sitio https://lfis325-2022-02.netlify.com/. Ambas p치ginas tendr치n la misma informaci칩n, sin embargo, para efectos de entrega de informes, el medio oficial ser치 el Aula Virtual/Google Classroom."
  },
  {
    "objectID": "slides/lec_week1.html#softwares",
    "href": "slides/lec_week1.html#softwares",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Softwares",
    "text": "Softwares\nPara la mayor칤a de las aplicaciones utilizaremos R, por lo que se sugiere utilizar un IDE como RStudio.\nPara la entrega de informes y talleres que requieran uso de programaci칩n, se recomienda el uso de Rmarkdown, Jupyter Notebook o \\(\\LaTeX\\) para la confecci칩n de documentos a entregar."
  },
  {
    "objectID": "slides/lec_week1.html#bibliograf칤a",
    "href": "slides/lec_week1.html#bibliograf칤a",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Bibliograf칤a",
    "text": "Bibliograf칤a\nLa bibliograf칤a principal del curso es:\n\n\n\n\n\n\nStatistical Data Analysis, Glen Cowan.\n\n\n\n\n\n\n\nPractical Statistics for Astronomers, J.V. Wall, C.R. Jenkins\n\n\n\n\n\n\n\nProbability and Statistics for Engineering and the Sciences, 9th Edition.Jay L. Devore"
  },
  {
    "objectID": "slides/lec_week1.html#introducci칩n-a-la-estad칤stica",
    "href": "slides/lec_week1.html#introducci칩n-a-la-estad칤stica",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Introducci칩n a la Estad칤stica",
    "text": "Introducci칩n a la Estad칤stica\n\nProceso Estad칤stico\nTipos de muestreo\nMedidas de tendencia muestral\nTablas de frecuencia\nTipos de gr치ficos"
  },
  {
    "objectID": "slides/lec_week1.html#probabilidad-en-la-ciencia",
    "href": "slides/lec_week1.html#probabilidad-en-la-ciencia",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Probabilidad en la ciencia",
    "text": "Probabilidad en la ciencia\n\nDecisi칩n y probabilidad\nTeorema de Bayes\nInferencia y probabilidad\nAn치lisis de error simple\nUso de la estad칤stica"
  },
  {
    "objectID": "slides/lec_week1.html#modelamiento-de-datos-estimaci칩n-de-par치metros",
    "href": "slides/lec_week1.html#modelamiento-de-datos-estimaci칩n-de-par치metros",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Modelamiento de datos: Estimaci칩n de par치metros",
    "text": "Modelamiento de datos: Estimaci칩n de par치metros\n\nEl m칠todo de la probabilidad m치xima\nM칤nimos cuadrados\nAn치lisis Bayesiano\nModelamiento Monte Carlo\nModelo de modelos y combinaci칩n de conjunto de datos"
  },
  {
    "objectID": "slides/lec_week1.html#detecci칩n-y-b칰squeda",
    "href": "slides/lec_week1.html#detecci칩n-y-b칰squeda",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Detecci칩n y B칰squeda",
    "text": "Detecci칩n y B칰squeda\n\nDetecci칩n\nCat치logos y efectos de selecci칩n\nEl l칤mite de confusi칩n"
  },
  {
    "objectID": "slides/lec_week1.html#estad칤stica-en-1d-y-2d",
    "href": "slides/lec_week1.html#estad칤stica-en-1d-y-2d",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Estad칤stica en 1D y 2D",
    "text": "Estad칤stica en 1D y 2D\n\nTransformaciones de datos\nAn치lisis de Fourier\nFiltrado\nCorrelacionamiento\nEstad칤stica sobre una superficie\nRepresentaci칩n del cielo\nFunci칩n correlaci칩n angular de dos puntos\nEl espectro de potencia angular"
  },
  {
    "objectID": "slides/lec_week1.html#cadenas-de-markov-monte-carlo",
    "href": "slides/lec_week1.html#cadenas-de-markov-monte-carlo",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Cadenas de Markov Monte Carlo",
    "text": "Cadenas de Markov Monte Carlo\n\nAlgoritmo de Metr칩polis-Hastings\nComparaci칩n de modelos\nAplicaciones."
  },
  {
    "objectID": "slides/lec_week1.html#ponderaciones",
    "href": "slides/lec_week1.html#ponderaciones",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Ponderaciones",
    "text": "Ponderaciones\nLa metodolog칤a de evaluaci칩n es la siguiente:\n\n\n\nTipo de evaluaci칩n\nPorcentaje que corresponde\n\n\n\n\nEvaluaciones sumativas (2)\n50%\n\n\nPresentaciones\n30%\n\n\nTrabajo final\n20%"
  },
  {
    "objectID": "slides/lec_week1.html#metodolog칤a-del-curso",
    "href": "slides/lec_week1.html#metodolog칤a-del-curso",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Metodolog칤a del curso",
    "text": "Metodolog칤a del curso\n\nAntes de cada clase, se mandar치 una lectura de preparaci칩n para la sesi칩n\nEl enfoque principal ser치 te칩rico, pero sin dejar de lado las aplicaciones\nSe pondr치 a disposici칩n material adicional para estudiar:\n\nEjemplos y ejercicios te칩ricos\nC칩digos\n\nEl curso ser치 mayoritariamente autocontenido, pero requiere al menos conocimiento b치sico c치lculo y programaci칩n."
  },
  {
    "objectID": "slides/lec_week1.html#proceso-estad칤stico",
    "href": "slides/lec_week1.html#proceso-estad칤stico",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Proceso estad칤stico",
    "text": "Proceso estad칤stico\nLa estad칤stica es la ciencia encargada de la descripci칩n, organizaci칩n, presentaci칩n de datos y adem치s la obtenci칩n de conclusiones basadas en los datos experimentales; a esto le llamamos inferencia, la cual es inductiva debido a que se proyecta de lo espec칤fico hacia lo general.\nAl ser una ciencia, esta se rige al m칠todo cient칤fico."
  },
  {
    "objectID": "slides/lec_week1.html#diagrama-del-proceso-estad칤stico",
    "href": "slides/lec_week1.html#diagrama-del-proceso-estad칤stico",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Diagrama del proceso estad칤stico",
    "text": "Diagrama del proceso estad칤stico"
  },
  {
    "objectID": "slides/lec_week1.html#por-qu칠-el-uso-de-la-estad칤stica-en-f칤sica",
    "href": "slides/lec_week1.html#por-qu칠-el-uso-de-la-estad칤stica-en-f칤sica",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "쯇or qu칠 el uso de la estad칤stica en f칤sica?",
    "text": "쯇or qu칠 el uso de la estad칤stica en f칤sica?\n\nCuantificaci칩n del error: Errores propios y externos, 쯈u칠 significan?.\n쮺칩mo pueden nuestros datos ser utilizados de la mejor manera posible? o 쯇ueden ser usados?.\nCorrelaciones, test de hip칩tesis y modelaci칩n: 쮺omo se procede?.\nMuestras incompletas, datos de un experimento que no puede ser replicado 쮺칩mo se trabajo con este tipo de datos?.\nLa presentaci칩n de datos y conclusi칩n siempre vienen -inherentemente- en t칠rminos estad칤sticos.\nEl proceso de decisi칩n no puede ser realizado sin una metodolog칤a, sin importar que tan bueno sea el experimento."
  },
  {
    "objectID": "slides/lec_week1.html#procedimiento-de-un-experimento",
    "href": "slides/lec_week1.html#procedimiento-de-un-experimento",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Procedimiento de un experimento",
    "text": "Procedimiento de un experimento\n\nObservar: Registrar u obtener los datos.\nReducir: Limpiar los datos para eliminar efectos experimentales: correcci칩n flat-field, calibraci칩n, etc.\nAnalizar: Obtener los n칰meros desde una base de datos limpia: intensidades, posiciones, etc칠tera. A partir de esta informaci칩n, producir indicadores que permitan comparar o modelar. Estos indicadores son llamados estad칤sticos y determinan el dise침o de experimentos.\nConclusi칩n: Llevar a cabo un procedimiento para llegar a una conclusi칩n. Test de hip칩tesis; correlaci칩n, modelos, etc.\nReflexi칩n: 쯈u칠 se aprendi칩? 쮼s la decisi칩n viable? 쮽ue inesperada? 쮼n qu칠 paso del experimiento se debe examinar para verificar? 쯈u칠 es necesario para confirmar un resultado inesperado? 쮺칩mo se debe realizar el siguiente experimento? 쯉e extender치 la hip칩tesis realizada o se sugerir치 una nueva hip칩tesis?"
  },
  {
    "objectID": "slides/lec_week1.html#diagrama-de-una-muestra",
    "href": "slides/lec_week1.html#diagrama-de-una-muestra",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Diagrama de una muestra",
    "text": "Diagrama de una muestra"
  },
  {
    "objectID": "slides/lec_week1.html#tipos-de-muestreo",
    "href": "slides/lec_week1.html#tipos-de-muestreo",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Tipos de muestreo",
    "text": "Tipos de muestreo\n\nMuestreo no Probabil칤stico: Los resultados obtenidos s칩lo representan las caracter칤sticas de los elementos muestrados y no de la poblaci칩n\n\nMuestreo por conveniencia\nMuestreo consecutivo\nMuestreo por cuotas\nMuestreo de bola de nieve. (Muestreo en cadena).\n\nMuestreo Probabil칤stico: Cada uno de los elementos de la poblaci칩n de inter칠s, o poblaci칩n objetivo, tiene una probabilidad conocida (frecuentemente igual) de ser elegidos en la muestra."
  },
  {
    "objectID": "slides/lec_week1.html#muestreo-aleatorio-simple-y-sistem치tico",
    "href": "slides/lec_week1.html#muestreo-aleatorio-simple-y-sistem치tico",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Muestreo aleatorio simple y sistem치tico",
    "text": "Muestreo aleatorio simple y sistem치tico\n\nMuestreo Aleatorio Simple: Los elementos se escogen en forma individual y al azar de la totalidad de la poblaci칩n, es decir, se escogen sin ning칰n privilegio y cada uno posee la misma probabilidad de formar parte de la muestra en cada una de las posibles muestras.\nMuestreo Aleatorio Sistem치tico: Existe un plan de muestreo al azar, en la cual se eligen los elementos de la poblaci칩n a intervalos uniformes, a partir de un listado (ordenado), tal como elegir cada \\(k-\\)칠simo elemento despu칠s de un arranque aleatorio."
  },
  {
    "objectID": "slides/lec_week1.html#muestreo-estratificado",
    "href": "slides/lec_week1.html#muestreo-estratificado",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Muestreo estratificado",
    "text": "Muestreo estratificado\n\nMuestreo Aleatorio Estratificado: La caracter칤stica que se est치 midiendo en la poblaci칩n objetivo, presenta mucha dispersi칩n en grupos identificados, por lo tanto, lo primero que se debe hacer es estratificar los elementos de la poblaci칩n en subgrupos y excluyentes de acuerdo al comportamiento que presenta la caracter칤stica dentro de estos grupos.\n\n\n\n\n\n\n\nPosterior a la clasificaci칩n de los elementos de la poblaci칩n en grupos, se obtiene por separado una muestra aleatoria simple o sistem치tica de cada estrato."
  },
  {
    "objectID": "slides/lec_week1.html#tipos-de-variable",
    "href": "slides/lec_week1.html#tipos-de-variable",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Tipos de variable",
    "text": "Tipos de variable\n\nVariables Cualitativas: Cuando los elementos de una poblaci칩n son clasificados en categor칤as o clases excluyentes, se habla de variables cualitativas. Ejemplos: estado civil, lugar de procedencia, marca de art칤culos, etc.\nVariables Cuantitativas (o Num칠ricas): Si los posibles valores para los elementos de una poblaci칩n, son cantidades o n칰meros, se habla de variables cuantitativas. Ejemplos: kms por litro de gasolina de un auto, temperatura, duraci칩n de un examen, etc.\n\nDiscretas: Se habla de variables discretas, cuando el conjunto de valores posibles es finito o infinito numerable. Ejemplos: Cantidad de cr칤as por camada, n칰mero de alumnos por carrera, etc.\nContinuas: Son aquellas que pueden asumir infinitos valores. Ejemplos: sueldo de una persona, tiempo que tarda un animal en alcanzar un peso previamente determinado, etc."
  },
  {
    "objectID": "slides/lec_week1.html#escalas-de-medici칩n",
    "href": "slides/lec_week1.html#escalas-de-medici칩n",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Escalas de medici칩n",
    "text": "Escalas de medici칩n\n\nEscalas de Medici칩n para variables cualitativas:\n\nNominal: Es aquella escala en donde las categor칤as (o los posibles valores de la variable), no pueden ser ordenadas en un sentido de magnitud. Ejemplos: colores, profesi칩n, etc.\nOrdinal: Cuando las categor칤as admiten una ordenaci칩n (no alfab칠tica), se habla de escala ordinal. Ejemplo: Nivel Socio-econ칩mico (alto, medio o bajo), sistema de evaluaci칩n cualitativa (insuficiente, suficiente, bueno, muy bueno), etc.\n\nEscalas de Medici칩n para variables cuantitativas:\n\nIntervalar: Son aquellas que poseen un punto de referencia (o cero) relativo, en el sentido de que si se cambia de unidad de medici칩n, el punto de referencia difiere entre una unidad de medida y otra. Ejemplo: temperatura (Celcius - Fahrenheit).\nRaz칩n: Son aquellas que poseen un cero absoluto (es decir, 칰nico). Incluso permiten hacer comparaciones por cocientes. Ejemplo: Peso de una persona, distancias, etc."
  },
  {
    "objectID": "slides/lec_week1.html#organizaci칩n-de-los-datos",
    "href": "slides/lec_week1.html#organizaci칩n-de-los-datos",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Organizaci칩n de los datos",
    "text": "Organizaci칩n de los datos\nLa organizaci칩n de los datos trata de acomodar 칠stos para que puedan revelar sus caracter칤sticas informativas fundamentales y de esta manera simplificar los an치lisis para la obtenci칩n de conclusiones.\nEl uso de frecuencia es m치s natural en datos cualitativos o discretos, pues en estos casos es sencillo contar el n칰mero de veces que aparece un mismo dato en la poblaci칩n (muestra) de 칠stos, en este caso se habla de tabla de frecuencia no agrupadas.\nSin Embargo, cuando se trabaja con datos cuantitativos en escala continua, es muy posible que exista un conjunto de n칰meros distintos lo suficientemente grande, como para hacer impracticable lo anterior, en este 칰ltimo caso se procede a crear agrupaciones convenientes para los datos observados, en este caso se habla de tabla de frecuencia agrupadas."
  },
  {
    "objectID": "slides/lec_week1.html#tabla-de-frecuencia",
    "href": "slides/lec_week1.html#tabla-de-frecuencia",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Tabla de frecuencia",
    "text": "Tabla de frecuencia\nEn las tablas de frecuencia cada categor칤a tiene una frecuencia observada, este c치lculo es siempre posible en datos cualitativos, sin embargo, si la cantidad de categor칤as es grande, deja de ser un resumen adecuado para los datos.\nLas respuestas observadas en la poblaci칩n (muestra), se denominar치n clases, y se simbolizan por: \\(C_1,C_2,\\dots, C_k\\) donde \\(k\\) es la cantidad de categor칤as (respuestas) distintas."
  },
  {
    "objectID": "slides/lec_week1.html#frecuencia-absoluta",
    "href": "slides/lec_week1.html#frecuencia-absoluta",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Frecuencia absoluta",
    "text": "Frecuencia absoluta\nSe llama frecuencia absoluta de la clase \\(C_i\\), al n칰mero de elementos de la poblaci칩n (muestra) que pertenecen a la clase \\(C_i\\). Este n칰mero lo denotaremos por \\(n_i\\) y cumplen la propiedad:\n\\[\\sum_{i=1}^{k} n_i =n\\]\nEn donde \\(n\\) es el tama침o de la poblaci칩n o muestra, seg칰n sea el caso."
  },
  {
    "objectID": "slides/lec_week1.html#frecuencia-relativa",
    "href": "slides/lec_week1.html#frecuencia-relativa",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Frecuencia relativa",
    "text": "Frecuencia relativa\nSe llama frecuencia relativa de la clase \\(C_i\\), a la cantidad de elementos en la poblaci칩n (muestra) que pertenecen a la clase \\(C_i\\), relativo al total de elementos en la poblaci칩n (muestra). Este n칰mero lo denotaremos por \\(f_i\\) y cumplen la propiedad:\n\\[f_i=\\dfrac{n_i}{n} \\Rightarrow \\sum_{i=1}^{k} f_i = \\sum_{i=1}^{k}\\dfrac{n_i}{n} = 1\\]"
  },
  {
    "objectID": "slides/lec_week1.html#frecuencia-absoluta-acumulada",
    "href": "slides/lec_week1.html#frecuencia-absoluta-acumulada",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Frecuencia absoluta acumulada",
    "text": "Frecuencia absoluta acumulada\nSe llama frecuencia absoluta acumulada hasta la clase \\(C_i\\), al n칰mero total de elementos en la poblaci칩n (muestra) que pertenecen a las clases \\(C_1,C_2,\\dots,C_i\\). Este n칰mero lo denotaremos por \\(N_i\\) y cumplen la propiedad:\n\\[N_i=n_1+n_2+\\dots+n_i=\\sum_{j=1}^{i}n_j, \\hspace{10pt} j=1,2,\\dots,i, \\hspace{10pt} i=1,2,\\dots,k\\] y,\n\\[N_k=n_1+n_2+\\dots+n_i+\\dots+n_k=n\\]"
  },
  {
    "objectID": "slides/lec_week1.html#frecuencia-relativa-acumulada",
    "href": "slides/lec_week1.html#frecuencia-relativa-acumulada",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Frecuencia relativa acumulada",
    "text": "Frecuencia relativa acumulada\nSe llama frecuencia relativa acumulada hasta la clase \\(C_i\\), a la cantidad de elementos en la poblaci칩n (muestra) que pertenecen a las clases \\(C_1,C_2,\\dots,C_i\\), con respecto al total de elementos en la poblaci칩n (muestra). Este n칰mero lo denotaremos por \\(F_i\\) y cumplen la propiedad:\n\\[F_i=f_1+f_2+\\dots+f_i=\\sum_{j=1}^{i}f_j, \\hspace{10pt} j=1,2,\\dots,i, \\hspace{10pt} i=1,2,\\dots,k\\]"
  },
  {
    "objectID": "slides/lec_week1.html#ejemplo-enunciado",
    "href": "slides/lec_week1.html#ejemplo-enunciado",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Ejemplo: enunciado",
    "text": "Ejemplo: enunciado\nEn un conjunto de resultados experimentales, se desea determinar la clasificaci칩n de los resultados obtenidos. Estos son clasificados como: Malos (M), Regulares (R), Buenos (B) y Excelentes (E). Los datos son:\n\n\n\nB\nR\nB\nE\nE\nE\nM\nB\nE\nR\n\n\nR\nM\nM\nR\nR\nM\nR\nB\nB\nB\n\n\nB\nB\nE\nB\nB\nB\nE\nB\nE\nR\n\n\nE\nM\nB\nB\nE\nB\nB\nB\nB\nB\n\n\nM\nR\nM\nB\nB\nB\nB\nE\nM\nR"
  },
  {
    "objectID": "slides/lec_week1.html#ejemplo-respuesta",
    "href": "slides/lec_week1.html#ejemplo-respuesta",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Ejemplo: respuesta",
    "text": "Ejemplo: respuesta\n\n\n\n\nFrecuencias\n\nFrecuencias\nAcumuladas\n\n\n\n\nClasificaci칩n\nAbsoluta\nRelativa\nAbsoluta\nRelativa\n\n\nMalo\n8\n16%\n8\n16%\n\n\nRegular\n9\n18%\n17\n34%\n\n\nBuenos\n23\n46%\n40\n80%\n\n\nExcelentes\n10\n20%\n50\n100%"
  },
  {
    "objectID": "slides/lec_week1.html#tablas-para-variables-continuas",
    "href": "slides/lec_week1.html#tablas-para-variables-continuas",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Tablas para variables continuas",
    "text": "Tablas para variables continuas\nEn variables continuas, la organizaci칩n de datos es un poco m치s compleja: se dividen los datos en \\(k\\) grupos o segmentos disjuntos. Estos grupos representan las clases y se determina la frecuencia de datos asociado a cada grupo, conformando una tabla de frecuencia agrupada.\nEn este tipo de datos las clases est치n compuestas por intervalos, luego es necesario buscar un representante de la frecuencia asociada a este intervalo , el cual se conoce como marca de clase. Es com칰n utilizar como marca de clase al valor medio del segmento (intervalo)."
  },
  {
    "objectID": "slides/lec_week1.html#construcci칩n-de-la-tabla-de-frecuencia",
    "href": "slides/lec_week1.html#construcci칩n-de-la-tabla-de-frecuencia",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Construcci칩n de la tabla de frecuencia",
    "text": "Construcci칩n de la tabla de frecuencia\nEn la construcci칩n de una tabla de frecuencia, lo primero que se tiene que tener claro es la cantidad de segmentos (intervalos) a considerar. Lo m치s com칰n es utilizar como una primera aproximaci칩n la regla de Sturges.\n\nRegla de Sturges: El n칰mero de clases \\(k= 3,3 * \\log(n)+1\\), donde \\(n\\) es la cantidad de datos que se desea organizar.\nAmplitud: Para determinar la amplitud de las clases \\(a\\), se debe calcular el rango \\(R_D\\), que es la diferencia entre el dato mayor (\\(\\max x_i\\)) y el dato menor (\\(\\min x_i\\)). Tambi칠n es necesario determinar \\(u\\), la unidad m칤nima de conteo de los datos. Luego, la amplitud estar치 dada por:\n\n\n\\[ a=\\dfrac{R_D +u}{k}\\]"
  },
  {
    "objectID": "slides/lec_week1.html#rango-de-la-tabla",
    "href": "slides/lec_week1.html#rango-de-la-tabla",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Rango de la tabla",
    "text": "Rango de la tabla\nUna vez determinada la amplitud \\(a\\), se procede a determinar el rango de la tabla \\(R_T\\), que es la multiplicaci칩n entre la cantidad de clases que se est치n utilizando y la amplitud. Para determinar los l칤mites te칩ricos de las clases, se comienza con el l칤mite inferior de la primera clase (\\(LI_1\\)), el cual se calcula como:\n\\[LI_1=\\min x_i - \\dfrac{D}{2}\\]\nDonde la diferencia \\(D=R_T-R_D\\), en el caso que el 칰ltimo d칤gito de \\(D\\) no sea par, se realiza un ajuste conveniente.\nPosteriormente, se suma la amplitud a \\(LI_1\\) obteni칠ndose el limite superior de esta clase (\\(LS_1\\)), el que tambi칠n ser치 el l칤mite inferior de la segunda clase, \\(LI_2=LS_1\\). Este 칰ltimo se considera abierto para su clase y cerrado para la segunda clase, luego para los siguientes intervalos se realiza el mismo procedimiento anterior."
  },
  {
    "objectID": "slides/lec_week1.html#tabla-general",
    "href": "slides/lec_week1.html#tabla-general",
    "title": "Estad칤stica para ciencias f칤sicas",
    "section": "Tabla general",
    "text": "Tabla general\n\n\n\n\nFrecuencias\n\nFrecuencias\nAcumuladas\n\n\n\n\nClases\nAbsoluta\nRelativa\nAbsoluta\nRelativa\n\n\n\\([LI_1-LS_1[\\)\n\\(n_1\\)\n\\(f_1\\)\n\\(N_1\\)\n\\(F_1\\)\n\n\n\\([LI_2-LS_2[\\)\n\\(n_2\\)\n\\(f_2\\)\n\\(N_2\\)\n\\(F_2\\)\n\n\n\\([LI_3-LS_3[\\)\n\\(n_3\\)\n\\(f_3\\)\n\\(N_3\\)\n\\(F_3\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\([LI_k-LS_k[\\)\n\\(n_k\\)\n\\(f_k\\)\n\\(N_k\\)\n\\(F_k\\)"
  },
  {
    "objectID": "slides/lec_week2.html#media-aritm칠tica",
    "href": "slides/lec_week2.html#media-aritm칠tica",
    "title": "Estad칤stica descriptiva",
    "section": "Media aritm칠tica",
    "text": "Media aritm칠tica\nSe define como el cociente de la suma de todos los valores entre el n칰mero total de valores. Las expresiones para c치lculo de la media de una poblaci칩n y de una muestra son, respectivamente:\n\\[\\mu = \\dfrac{\\sum_{i=1}^{N}X_i}{N}, \\hspace{20pt} \\overline{x} = \\dfrac{\\sum_{i=1}^{n} X_i}{n}\\]\nCuando se tiene a disposici칩n s칩lo los datos agrupados, se utiliza el punto medio de cada clase como aproximaci칩n de todos los valores contenidos en ella. El punto medio o marca de clase se representa por \\(m_i\\), en donde el sub칤ndice \\(i\\) indica la clase \\(i-\\)칠sima, y se utiliza \\(n_i\\) para representar la frecuencia absoluta observada en la clase respectiva. En tal caso, las expresiones son:\n\\[\\mu = \\dfrac{\\sum_{i=1}^{k} n_i m_i}{N}, \\hspace{20pt} \\overline{x} = \\dfrac{\\sum_{i=1}^{k} n_i m_i}{n}\\]"
  },
  {
    "objectID": "slides/lec_week2.html#mediana",
    "href": "slides/lec_week2.html#mediana",
    "title": "Estad칤stica descriptiva",
    "section": "Mediana",
    "text": "Mediana\nEs el valor que ocupa el lugar central de estos cuando se ordenan en orden de magnitud. Para conjunto de datos, con un n칰mero par de elementos, la mediana se calcula como el promedio de los valores centrales. En caso de trabajar con datos dispersos, la expresi칩n para determinar la posici칩n de la mediana en el conjunto (ordenado) es:\n\\[Me=\\begin{cases}\nX_{\\left(\\dfrac{n+1}{2}\\right)}, &\\text{Si }n\\text{ es impar} \\\\\n\\dfrac{1}{2}\\left(X_{\\left(\\dfrac{n}{2}\\right)} + X_{\\left(\\dfrac{n}{2}+1\\right)}\\right), &\\text{Si }n\\text{ es par}\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/lec_week2.html#mediana-datos-agrupados",
    "href": "slides/lec_week2.html#mediana-datos-agrupados",
    "title": "Estad칤stica descriptiva",
    "section": "Mediana: datos agrupados",
    "text": "Mediana: datos agrupados\nPara datos agrupados, en primer lugar es necesario determinar la clase que contiene el valor de la mediana, para despu칠s determinar la posici칩n de la mediana dentro de la clase mediante interpolaci칩n. La clase que contiene la mediana es la primera clase cuya frecuencia acumulada es mayor o igual a la mitad de los datos. Una vez que se identifica esta clase, se determina el valor interpolado de la mediana, empleando la siguiente expresi칩n:\n\\[Me=LI_i+ \\left( \\dfrac{\\dfrac{n}{2}-N_{i-1}}{n_i}\\right)a_i\\]\nEn donde \\(LI_i\\) es l칤mite inferior de la clase que contiene a la mediana, \\(n\\) el n칰mero total de observaciones, \\(a_i\\) la amplitud de clase, \\(N_{i-1}\\) la frecuencia acumulada anterior a la clase que contiene la mediana y \\(n_i\\) el n칰mero de observaciones de la clase que contiene la mediana."
  },
  {
    "objectID": "slides/lec_week2.html#moda",
    "href": "slides/lec_week2.html#moda",
    "title": "Estad칤stica descriptiva",
    "section": "Moda",
    "text": "Moda\nSe define como el valor o clase que se presenta con mayor frecuencia. Para datos agrupados, se utiliza interpolaci칩n dentro de la clase modal, de acuerdo a la siguiente expresi칩n:\n\\[Mo=LI_i+ \\left(\\dfrac{d_1}{d_1+d_2} \\right)a_i\\] en donde,\n\n\\(LI_i\\) es el l칤mite inferior de la clase que contiene la moda.\n\\(d_1\\) es la diferencia entre la frecuencia de la clase modal y la frecuencia de la clase que le precede.\n\\(d_2\\) es la diferencia entre la frecuencia de la clase modal y la frecuencia de la clase que le sigue.\n\\(a_i\\) es la amplitud de clase."
  },
  {
    "objectID": "slides/lec_week2.html#cuantiles",
    "href": "slides/lec_week2.html#cuantiles",
    "title": "Estad칤stica descriptiva",
    "section": "Cuantiles",
    "text": "Cuantiles\nSon medidas de posici칩n que dividen los datos en grupos bajo los cuales se encuentra una determinada proporci칩n de 칠stos, por lo se requiere que los datos se encuentren en al menos escala ordinal.\n\nCuartil: \\(Q_{i}=X_{\\left(\\dfrac{i(n+1)}{4}\\right)}, \\hspace{20pt} i:1,2,\\dots,4\\)\nQuintil: \\(K_{i}=X_{\\left(\\dfrac{i(n+1)}{5}\\right)}, \\hspace{20pt} i:1,2,\\dots,5\\)\nDecil: \\(D_{i}=X_{\\left(\\dfrac{i(n+1)}{10}\\right)}, \\hspace{20pt} i:1,2,\\dots,10\\)\nPercentil: \\(P_{i}=X_{\\left(\\dfrac{i(n+1)}{100}\\right)}, \\hspace{20pt} i:1,2,\\dots,100\\)"
  },
  {
    "objectID": "slides/lec_week2.html#cuantiles-datos-agrupados",
    "href": "slides/lec_week2.html#cuantiles-datos-agrupados",
    "title": "Estad칤stica descriptiva",
    "section": "Cuantiles: datos agrupados",
    "text": "Cuantiles: datos agrupados\nPara datos agrupados, la f칩rmula se modifica de acuerdo con el punto fraccionario de inter칠s. Para utilizar esta expresi칩n modificada, en primer lugar se determina la clase que contiene el punto de inter칠s, de acuerdo con las frecuencias acumuladas, y despu칠s se lleva a cabo una interpolaci칩n como en el caso anterior de la mediana."
  },
  {
    "objectID": "slides/lec_week2.html#interpolaci칩n",
    "href": "slides/lec_week2.html#interpolaci칩n",
    "title": "Estad칤stica descriptiva",
    "section": "Interpolaci칩n",
    "text": "Interpolaci칩n\nEn este caso se observa que \\(y=\\mathbf{y}, x=P_{78}, x_2 - x_1= LS-LI=a\\) y \\(y_2 - y_1 = N_i - N_{i-1}=n_i\\)\nLuego despejando \\(x=P_{78}\\), se obtiene una expresi칩n para el c치lculo de percentiles en datos agrupados:\n\\[x=P_{78}=x_1+\\left( \\dfrac{y-y_1}{y_2 - y_1} \\right)(x_2 - x_1)=LI+\\left(\\dfrac{y- N_{i-1}}{n_i}\\right)a\\]\npero \\(y\\) no es otra cosa que \\(\\dfrac{n \\times j}{100}\\), donde \\(j\\) es el percentil \\(j-\\)칠simo. Por lo que podemos generalizar la ecuaci칩n anterior como:\n\\[P_j=LI+\\left(\\dfrac{\\dfrac{n\\times j}{100}-N_{i-1}}{n_i}\\right)a=LI+\\left(\\dfrac{\\dfrac{j}{100}-F_{i-1}}{f_i}\\right)a\\]"
  },
  {
    "objectID": "slides/lec_week2.html#ejemplo",
    "href": "slides/lec_week2.html#ejemplo",
    "title": "Estad칤stica descriptiva",
    "section": "Ejemplo",
    "text": "Ejemplo\nLa siguiente tabla resume los tiempos de espera antes de obtener ciertos resultados experimentales.\n\nCalcular el percentil 80"
  },
  {
    "objectID": "slides/lec_week2.html#rango",
    "href": "slides/lec_week2.html#rango",
    "title": "Estad칤stica descriptiva",
    "section": "Rango",
    "text": "Rango\nEs la diferencia entre el mayor y menor valor del conjunto de datos.\n\\[R= \\begin{cases}\\max x_i - \\min x_i,  &\\text{datos dispersos} \\\\ LS_k -LI_1,  &\\text{datos agrupados} \\end{cases}\\]"
  },
  {
    "objectID": "slides/lec_week2.html#desviaci칩n-media",
    "href": "slides/lec_week2.html#desviaci칩n-media",
    "title": "Estad칤stica descriptiva",
    "section": "Desviaci칩n media",
    "text": "Desviaci칩n media\nEs la media (promedio) del valor absoluto de la diferencia entre cada uno de los datos y el promedio del grupo.\n\\[DM= \\begin{cases}\\sum_{i=1}^{n}\\dfrac{|x_i- \\overline{x}|}{n}, &\\text{datos dispersos}\\\\\\sum_{i=1}^{k} f_i |m_i - \\overline{x}|,&\\text{datos agrupados}\\end{cases}\\]"
  },
  {
    "objectID": "slides/lec_week2.html#varianza",
    "href": "slides/lec_week2.html#varianza",
    "title": "Estad칤stica descriptiva",
    "section": "Varianza",
    "text": "Varianza\n\\((V[X],\\sigma^2)\\) La varianza es similar a la desviaci칩n media porque se basa en la diferencia entre cada uno de los valores del conjunto de datos y la media del grupo. Su f칩rmula es, para su c치lculo poblacional y muestral, respectivamente:\n\\[V[X]=\\sigma^2=\\sum_{i=1}^{N} \\dfrac{(x_i-\\mu)^2}{N}\\]\n\\[S^2=\\sum_{i=1}^{n} \\dfrac{(x_i-\\overline{x})^2}{n-1}\\]"
  },
  {
    "objectID": "slides/lec_week2.html#desviaci칩n-est치ndar-o-t칤pica",
    "href": "slides/lec_week2.html#desviaci칩n-est치ndar-o-t칤pica",
    "title": "Estad칤stica descriptiva",
    "section": "Desviaci칩n est치ndar o t칤pica",
    "text": "Desviaci칩n est치ndar o t칤pica\nSe utiliza con mayor frecuencia la ra칤z cuadrada de la varianza, representada mediante la letra griega \\(\\sigma\\) para el caso poblacional y \\(S\\) para una muestra:\n\\[\\sigma=\\sqrt{V[X]}\\]\n\\[S=\\sqrt{S^2}\\]"
  },
  {
    "objectID": "slides/lec_week2.html#coeficiente-de-variaci칩n",
    "href": "slides/lec_week2.html#coeficiente-de-variaci칩n",
    "title": "Estad칤stica descriptiva",
    "section": "Coeficiente de variaci칩n",
    "text": "Coeficiente de variaci칩n\nRelaci칩n entre la desviaci칩n est치ndar y su media. Tiene por f칩rmula, para su c치lculo poblacional y muestral, respectivamente:\n\\[CV=\\dfrac{\\sigma}{\\mu}\\]\n\\[CV=\\dfrac{S}{\\overline{x}}\\]\nRepresenta la desviaci칩n est치ndar como proporci칩n (o porcentaje) de la media, por lo que es de gran utilidad al comparar dos poblaciones o muestras, pues no posee unidades lo que elimina el efecto de la magnitud de las variables medidas."
  },
  {
    "objectID": "slides/lec_week2.html#rango-intercuartil",
    "href": "slides/lec_week2.html#rango-intercuartil",
    "title": "Estad칤stica descriptiva",
    "section": "Rango intercuartil",
    "text": "Rango intercuartil\nEs la diferencia entre los percentiles 75 y 25, esto es RIQ (IQR)\\(= Q_3 - Q_1\\).\n\nRepresenta el 50% central de la poblaci칩n"
  },
  {
    "objectID": "slides/lec_week2.html#histograma",
    "href": "slides/lec_week2.html#histograma",
    "title": "Estad칤stica descriptiva",
    "section": "Histograma",
    "text": "Histograma"
  },
  {
    "objectID": "slides/lec_week2.html#ojiva",
    "href": "slides/lec_week2.html#ojiva",
    "title": "Estad칤stica descriptiva",
    "section": "Ojiva",
    "text": "Ojiva"
  },
  {
    "objectID": "slides/lec_week2.html#diagrama-de-caja-box-plot",
    "href": "slides/lec_week2.html#diagrama-de-caja-box-plot",
    "title": "Estad칤stica descriptiva",
    "section": "Diagrama de caja / box-plot",
    "text": "Diagrama de caja / box-plot"
  },
  {
    "objectID": "slides/lec_week2.html#construcci칩n-box-plot",
    "href": "slides/lec_week2.html#construcci칩n-box-plot",
    "title": "Estad칤stica descriptiva",
    "section": "Construcci칩n box-plot",
    "text": "Construcci칩n box-plot\n\nCalcular: Mediana, Cuartil 1 y 3 \\((Q_{1},Q_{3})\\)\nCalcular Rango Intercuartil (RIQ/IQR): \\(RIQ=Q_{3}-Q_{1}\\)\nCalcular bigotes interiores (superiores e inferiores):\n\nBigote inferior: \\(\\max (Q_{1}-1,5* RIQ, x_{1})\\)\nBigote superior: \\(\\min (Q_{3}+1,5* RIQ, x_{n})\\)\n\nCalcular bigotes exteriores (superiores e inferiores):\n\nBigote inferior: \\(Q_{1}-3* RIQ\\)\nBigote superior: \\(Q_{3}+3* RIQ\\)\n\nMarcar datos outliers:\n\nObservaciones entre bigote interior y exterior, se consideran sospechosos de ser outliers.\nObservaciones pasados los bigotes exteriores son outliers."
  },
  {
    "objectID": "slides/lec_week2.html#ejercicio",
    "href": "slides/lec_week2.html#ejercicio",
    "title": "Estad칤stica descriptiva",
    "section": "Ejercicio",
    "text": "Ejercicio\nDatos:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(7,12\\)\n\\(7,89\\)\n\\(10,12\\)\n\\(8,88\\)\n\\(10,02\\)\n\\(9,91\\)\n\\(9,95\\)\n\\(9,90\\)\n\n\n\n\n\\(10,23\\)\n\\(9,12\\)\n\\(9,99\\)\n\\(12,40\\)\n\\(8,65\\)\n\\(10,05\\)\n\\(10,50\\)\n\\(9,87\\)\n\n\n\\(8,54\\)\n\\(9,72\\)\n\\(11,09\\)\n\\(11,52\\)\n\\(12,30\\)\n\\(11,53\\)\n\\(16,40\\)\n\\(13,24\\)\n\n\n\n\nRealizar un diagrama de caja"
  },
  {
    "objectID": "slides/lec_week3.html#escuelas-de-probabilidad",
    "href": "slides/lec_week3.html#escuelas-de-probabilidad",
    "title": "Probabilidades",
    "section": "Escuelas de probabilidad",
    "text": "Escuelas de probabilidad\n\nEnfoque cl치sico\nEnfoque frecuentista\nEnfoque bayesiano"
  },
  {
    "objectID": "slides/lec_week3.html#enfoque-cl치sico",
    "href": "slides/lec_week3.html#enfoque-cl치sico",
    "title": "Probabilidades",
    "section": "Enfoque cl치sico",
    "text": "Enfoque cl치sico\nEste enfoque tambi칠n llamado enfoque apriori tiene por caracter칤stica principal la asignaci칩n igualitaria de una medida de ocurrencia para un resultado de un experimento aleatorio (experimento equiprobable).\n\nEsta asignaci칩n de probabilidad se determina antes de observar los resultados experimentales.\n\n\n\n쮸lg칰n ejemplo?"
  },
  {
    "objectID": "slides/lec_week3.html#enfoque-frecuentista",
    "href": "slides/lec_week3.html#enfoque-frecuentista",
    "title": "Probabilidades",
    "section": "Enfoque frecuentista",
    "text": "Enfoque frecuentista\nEste enfoque tambi칠n llamado enfoque emp칤rico, determina la medida de ocurrencia con base en la proporci칩n de veces que ocurre un resultado favorable en un determinado n칰mero de observaciones o experimentos. Este enfoque no asigna probabilidades a priori a los posibles resultados de un experimento aleatorio.\n\n\n쮸lg칰n ejemplo?"
  },
  {
    "objectID": "slides/lec_week3.html#enfoque-bayesiano",
    "href": "slides/lec_week3.html#enfoque-bayesiano",
    "title": "Probabilidades",
    "section": "Enfoque bayesiano",
    "text": "Enfoque bayesiano\nEste enfoque tambi칠n llamado enfoque subjetivo, determina la medida de ocurrencia en base a una expectativa razonable basado en el conocimiento del investigador.\nEl enfoque bayesiano es particularmente 칰til cuando se tiene poca informaci칩n del experimento, y este puede ser realizado para actualizador mis probabilidades, esto debido a que cada realizaci칩n del experimento aleatorio me otorgar치 informaci칩n adicional para determinar correctamente mis probabilidades."
  },
  {
    "objectID": "slides/lec_week3.html#principio-de-multiplicaci칩n",
    "href": "slides/lec_week3.html#principio-de-multiplicaci칩n",
    "title": "Probabilidades",
    "section": "Principio de multiplicaci칩n",
    "text": "Principio de multiplicaci칩n\nSupongamos que un procedimiento \\(1\\), puede hacerse de \\(n_1\\) maneras. Supongamos que un segundo procedimiento \\(2\\), se puede hacer de \\(n_2\\) maneras. Tambi칠n supongamos que cada una de las maneras de efectuar 1 puede ser seguida por cualquiera de las \\(n_2\\) de efectuar 2.\nEntonces el procedimiento que consta de \\(1\\) seguido por \\(2\\) se puede hacer de \\(n_1 \\times n_2\\) maneras. De igual manera podemos generalizar lo anterior a cualquier n칰mero de procedimientos."
  },
  {
    "objectID": "slides/lec_week3.html#principio-de-adici칩n",
    "href": "slides/lec_week3.html#principio-de-adici칩n",
    "title": "Probabilidades",
    "section": "Principio de adici칩n",
    "text": "Principio de adici칩n\nSupongamos que un procedimiento \\(1\\), se puede hacerse de \\(n_1\\) maneras, y que un segundo procedimiento \\(2\\), se puede hacer de \\(n_2\\) maneras. Supongamos adem치s que no es posible que ambos procedimientos, 1 y 2, se realicen. Entonces el n칰mero de maneras como se puede hacer el procedimiento 1 칩 2 es de \\(n_1 + n_2\\).\n\nFactorial: Sea \\(n \\in \\mathbb{N}\\), entonces se define \\(n\\) factorial como \\(n \\times (n-1) \\times (n-2) \\times \\dots \\times 1\\), y se simboliza por \\(n!\\) Empleado en situaciones donde una vez seleccionado un elemento, 칠ste puede ser nuevamente seleccionado.\n\n\nEjemplo\nConsidere un grupo de personas conformado por 15 hombres y 10 mujeres. Se eligen 3 personas al azar, la primera ser치 la presidente de la comisi칩n, la segunda vicepresidente y la tercera secretario. 쮻e cuantas formas se puede conformar la comisi칩n?"
  },
  {
    "objectID": "slides/lec_week3.html#permutaci칩n",
    "href": "slides/lec_week3.html#permutaci칩n",
    "title": "Probabilidades",
    "section": "Permutaci칩n",
    "text": "Permutaci칩n\nSe define la permutaci칩n de \\(r\\) elementos sobre \\(n\\) como el n칰mero de arreglos distintos que se pueden hacer con \\(r\\) elementos de un total de \\(n\\), importando el orden en el que salen los elementos, se simboliza por:\n\\[P_{r}^{n}=\\dfrac{n!}{(n-r)!}\\]\n\nEjemplo\nUn directorio compuesto por: Presidente, Secretario y Tesorero se debe elegir de un total de 10 candidatos. 쮺u치ntos directorios diferentes se pueden conformar?\n\\[P_{3}^{10}=\\dfrac{10!}{(10-3)!}=\\dfrac{10!}{7!}=720\\]"
  },
  {
    "objectID": "slides/lec_week3.html#combinatoria",
    "href": "slides/lec_week3.html#combinatoria",
    "title": "Probabilidades",
    "section": "Combinatoria",
    "text": "Combinatoria\nSe define la combinatoria de \\(r\\) elementos sobre \\(n\\) como el n칰mero de arreglos distintos que se pueden hacer con \\(r\\) elementos de un total de \\(n\\) sin importar el orden en que son asignados. Esta expresi칩n se anota por:\n\\[C_{r}^{n}={{n}\\choose{r}}=\\dfrac{n!}{r!(n-r)!}\\]\n\nEjemplo\nPara formar un comit칠 se van a elegir a tres personas de un total de 10. El n칰mero de grupos diferentes de tres personas que podr칤an elegirse, sin importar el orden en el que cada uno de los grupos est치 dado por:\n\\[C_{3}^{10}=\\dfrac{10!}{3!(10-3)!}=\\dfrac{10!}{3!7!}=\\dfrac{720}{6}=120\\]"
  },
  {
    "objectID": "slides/lec_week3.html#clasificaci칩n-del-espacio-muestral",
    "href": "slides/lec_week3.html#clasificaci칩n-del-espacio-muestral",
    "title": "Probabilidades",
    "section": "Clasificaci칩n del espacio muestral",
    "text": "Clasificaci칩n del espacio muestral\n\nDiscreto\n\nNumerable: Finito o Infinito.\n\nContinuo\n\nNo numerable: Acotado o No acotado."
  },
  {
    "objectID": "slides/lec_week3.html#definici칩n-formal-de-probabilidad",
    "href": "slides/lec_week3.html#definici칩n-formal-de-probabilidad",
    "title": "Probabilidades",
    "section": "Definici칩n formal de probabilidad",
    "text": "Definici칩n formal de probabilidad\nEl par \\((\\Omega,\\Sigma)\\) se dice espacio medible, y la funci칩n \\(\\mathbb{P}:\\Sigma \\rightarrow \\mathbb{R}^{+}\\), es una medida de probabilidad si satisface:\n\n\\(0\\leq \\mathbb{P}[A] \\leq 1, \\forall A \\in \\Sigma\\)\n\\(\\mathbb{P}[\\Omega]=1\\)\nDados \\(\\displaystyle A_1,A_2,\\dots \\in \\Sigma \\Rightarrow \\mathbb{P}\\left[ \\bigcup_{i=1}^{n} A_n \\right] = \\sum_{i=1}^{n} \\mathbb{P}[A_i], \\hspace{5pt} \\forall i\\)"
  },
  {
    "objectID": "slides/lec_week3.html#algunas-propiedades",
    "href": "slides/lec_week3.html#algunas-propiedades",
    "title": "Probabilidades",
    "section": "Algunas propiedades",
    "text": "Algunas propiedades\n\n\\(\\mathbb{P}[A]+\\mathbb{P}[A^c]=\\mathbb{P}[\\Omega]\\)\n\\(\\mathbb{P}[\\phi]=1-\\mathbb{P}[\\phi^c]=1-\\mathbb{P}[\\Omega]=0\\)\n\\(\\mathbb{P}[A \\cup B]=\\mathbb{P}[A]+\\mathbb{P}[B] - \\mathbb{P}[A\\cap B]\\) . Si este 칰ltimo t칠rmino \\((\\mathbb{P}[A\\cap B])\\) es cero, se dice que \\(A\\) y \\(B\\) son eventos mutuamente excluyentes.\n\\(\\mathbb{P}[A-B]=\\mathbb{P}[A\\cap B^c]\\)\n\\(\\mathbb{P}[A \\cap B]=\\mathbb{P}[A]\\mathbb{P}[B]\\). Si \\(A\\) y \\(B\\) son independientes."
  },
  {
    "objectID": "slides/lec_week3.html#ejercicio",
    "href": "slides/lec_week3.html#ejercicio",
    "title": "Probabilidades",
    "section": "Ejercicio",
    "text": "Ejercicio\nSea A el evento en el cual un hombre vivir치 10 a침os m치s y sea B el evento en el cual su esposa viva 10 a침os m치s. Supongamos que \\(\\mathbb{P}(A)=\\frac{1}{4}\\) y \\(\\mathbb{P}(B)=\\frac{1}{3}\\). Supongamos que A y B son eventos independientes, encuentre la probabilidad de que en 10 a침os:\n\nAmbos est칠n vivos.\nAl menos uno est칠 vivo.\nNinguno est칠 vivo.\nSolamente la esposa est칠 viva"
  },
  {
    "objectID": "slides/lec_week3.html#probabilidades-cl치sicas",
    "href": "slides/lec_week3.html#probabilidades-cl치sicas",
    "title": "Probabilidades",
    "section": "Probabilidades cl치sicas",
    "text": "Probabilidades cl치sicas\nEste planteamiento probabilista establece que los eventos del espacio muestral sean expresados de la forma m치s elemental posible, con el fin de poder aceptar la posibilidad de que cada posible resultado sea igualmente posible.\n\\[\\mathbb{P}[A]=\\dfrac{\\#A}{\\#\\Omega}\\]\n\nEjemplo\nConsidere que se tienen 10 sacos de semillas. Se sabe que 4 son de una variedad y el resto de otra. Un cliente compra 3 sacos. 쮺u치l es la probabilidad de que los sacos sean de las dos variedades?\n\n\\(A:{ \\text{Los sacos comprados son de las dos variedades} }\\)\n\\[\\mathbb{P}[A]=\\dfrac{\\#A}{\\#\\Omega}=\\dfrac{ C_{1}^{4} \\times C_{2}^{6} +C_{2}^{4} \\times C_{1}^{6} }{C_{3}^{10}}\\]"
  },
  {
    "objectID": "slides/lec_week3.html#probabilidades-condicionales",
    "href": "slides/lec_week3.html#probabilidades-condicionales",
    "title": "Probabilidades",
    "section": "Probabilidades condicionales",
    "text": "Probabilidades condicionales\nEl concepto de probabilidad condicional se emplea para redefinir el c치lculo de probabilidad de ocurrencia de un evento dada cierta condici칩n (o informaci칩n). Lo anotamos por:\n\\[\\mathbb{P}[B | A]=\\dfrac{\\mathbb{P}[B \\cap A]}{\\mathbb{P}[A]}\\]\nLo anterior, mide la probabilidad de que el evento \\(B\\) ocurra dado que el evento \\(A\\) ocurri칩. Notar que si los eventos \\(A\\) y \\(B\\) son independientes se tiene:\n\\[\\mathbb{P}[B|A]=\\dfrac{\\mathbb{P}[B]\\mathbb{P}[A]}{\\mathbb{P}[A]}=\\mathbb{P}[B]\\]\nPor lo que, en palabras, si los eventos son independientes, la probabilidad condicional se reduce a la probabilidad simple."
  },
  {
    "objectID": "slides/lec_week3.html#regla-multiplicativa",
    "href": "slides/lec_week3.html#regla-multiplicativa",
    "title": "Probabilidades",
    "section": "Regla multiplicativa",
    "text": "Regla multiplicativa\nSe refiere a la determinaci칩n de la probabilidad de ocurrencia conjunta de dos o m치s eventos. Para el caso de dos eventos, se tiene:\n\\[\\mathbb{P}[A\\cap B]=\\mathbb{P}[A]\\mathbb{P}[B|A]\\]\nEn el caso de tres eventos, se tiene:\n\\[\\mathbb{P}[A \\cap B \\cap C]=\\mathbb{P}[A] \\mathbb{P}[B|A] \\mathbb{P}[C| (A \\cap B)]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#diagrama-de-치rbol",
    "href": "slides/lec_week3.html#diagrama-de-치rbol",
    "title": "Probabilidades",
    "section": "Diagrama de 치rbol",
    "text": "Diagrama de 치rbol\nLos diagramas de 치rbol son particularmente 칰tiles para ilustrar los posibles eventos asociados con observaciones o ensayos secuenciales."
  },
  {
    "objectID": "slides/lec_week3.html#regla-de-bayes",
    "href": "slides/lec_week3.html#regla-de-bayes",
    "title": "Probabilidades",
    "section": "Regla de Bayes",
    "text": "Regla de Bayes\nLa regla de Bayes permite actualizar ciertas probabilidades a priori para transformarse en probabilidades posteriori de un evento (experimento). La importancia de la regla de Bayes consiste en que se aplica en contexto de eventos secuenciales y adem치s, de que proporciona la base para determinar la probabilidad condicional de un evento a la luz de un evento especifico que ha ocurrido.\n\\[\\mathbb{P}[A|B]=\\dfrac{\\mathbb{P}[A\\cap B]}{\\mathbb{P}[B]}=\\dfrac{\\mathbb{P}[A]\\mathbb{P}[B|A]}{\\mathbb{P}[A]\\mathbb{P}[B|A]+\\mathbb{P}[A^c]\\mathbb{P}[B|A^c]}\\]\n\nEjemplo\n\nUn fabricante posee dos m치quinas que producen el mismo art칤culo. Se sabe que una de ella (A) produce un \\(5\\%\\) de defectuosos y la otra (B) un \\(3\\%\\). Por otra parte el \\(60\\%\\) de las unidades es producido por la m치quina A.\n\nDefina sucesos e identifique las probabilidades.\nSi el art칤culo es no defectuoso. 쮺u치l es la probabilidad que el art칤culo provenga de la m치quina A?"
  },
  {
    "objectID": "slides/lec_week3.html#ejemplo-5",
    "href": "slides/lec_week3.html#ejemplo-5",
    "title": "Probabilidades",
    "section": "Ejemplo",
    "text": "Ejemplo\nUn grupo de estudiantes se inscribi칩 en dos asignaturas, A y B. De los resultados se observa que el 40% aprob칩 A. El 45% aprob칩 al menos una asignatura. De los que aprobaron A, el 37.5% aprob칩 B. Se elige un alumno al azar, calcule la probabilidad que:\n\nHaya aprobado ambas asignaturas.\nHaya aprobado B.\nNo haya aprobado ni A ni B"
  },
  {
    "objectID": "slides/lec_week3.html#desarrollo-ejemplo",
    "href": "slides/lec_week3.html#desarrollo-ejemplo",
    "title": "Probabilidades",
    "section": "Desarrollo ejemplo",
    "text": "Desarrollo ejemplo\nDefiniendo los eventos:\n\\[A: \\text{ El alumno aprob칩 la asignatura }A \\quad B: \\text{ El alumno aprob칩 la asignatura } B\\]\npor enunciado sabemos que:\n\\[\\mathbb{P}(A) = 0.4 \\hspace{15pt} \\mathbb{P}(A\\cup B)=0.45 \\hspace{15pt} \\mathbb{P}(B\\vert A)=0.375\\]\nPor lo que:\n\nHaya aprobado ambas asignaturas.\n\n\n\\[\\mathbb{P}(A \\cap B)= \\mathbb{P}(B\\vert A)* \\mathbb{P}(A)=0.375 * 0.4=0.15\\]\n\nHaya aprobado \\(B\\).\n\n\n\n\\[\\mathbb{P}(B)=\\mathbb{P}(A\\cup B)-\\mathbb{P}(A)+\\mathbb{P}(A\\cap B)=0.45-0.4+0.15= 0.2\\]\n\nNo haya aprobado ni \\(A\\) ni \\(B\\)\n\n\n\n\\[\\mathbb{P}(A^c \\cap B^c)=\\mathbb{P}((A \\cup B)^c)=1-\\mathbb{P}(A \\cup B)=1-0.45\\]"
  },
  {
    "objectID": "slides/lec_week3.html#pregunta-tipo-prueba",
    "href": "slides/lec_week3.html#pregunta-tipo-prueba",
    "title": "Probabilidades",
    "section": "Pregunta tipo prueba",
    "text": "Pregunta tipo prueba\nUna persona esta interesada en invertir su dinero en acciones en el mercado burs치til nacional. Estudios estad칤sticos indican que las preferencias por las distintas acciones est치n representadas por las del tipo A y tipo B. Adem치s, el 45% de preferencias son por las acciones del tipo A. Si la acci칩n es de tipo A, la probabilidad de tener una rentabilidad positiva es de 0.7. Si la acci칩n es de tipo B, la probabilidad de tener una rentabilidad positiva es de 0.6.\n\nDefina sucesos e identifique las probabilidades.\n쮺u치l es la probabilidad de tener una rentabilidad positiva?\nSi la rentabilidad es negativa, 쮺u치l es la probabilidad que no se haya invertido en acciones del tipo A?"
  },
  {
    "objectID": "slides/lec_week3.html#desarrollo-pregunta-tipo-prueba",
    "href": "slides/lec_week3.html#desarrollo-pregunta-tipo-prueba",
    "title": "Probabilidades",
    "section": "Desarrollo pregunta tipo prueba",
    "text": "Desarrollo pregunta tipo prueba\nDefiniendo eventos como:\n\n\\(A: \\text{ La persona invierte en acciones de tipo }A\\)\n\\(B: \\text{ La persona invierte en acciones de tipo }B\\)\n\\(R: \\text{ Se obtiene rentabilidad positiva tras invertir}\\)\n\n\nAs칤,\n\\[\\mathbb{P}(A)=0.45 \\hspace{15pt} \\mathbb{P}(R\\vert A)=0.7 \\hspace{15pt} \\mathbb{P}(R\\vert B)=0.6\\]\n\n\n쮺u치l es la probabilidad de tener una rentabilidad positiva?\n\n\nPor regla multiplicativa se tiene que:\n\\[\\mathbb{P}(R)=\\mathbb{P}(A)*\\mathbb{P}(R\\vert A)+\\mathbb{P}(B)*\\mathbb{P}(R\\vert B)\\]\nreemplazando, tenemos que:\n\\[\\mathbb{P}(R)=0.45*0.7+0.55*0.6=0.645\\]"
  },
  {
    "objectID": "slides/lec_week3.html#continuaci칩n-desarrollo",
    "href": "slides/lec_week3.html#continuaci칩n-desarrollo",
    "title": "Probabilidades",
    "section": "Continuaci칩n desarrollo",
    "text": "Continuaci칩n desarrollo\nSi la rentabilidad es negativa, 쮺u치l es la probabilidad que no se haya invertido en acciones del tipo A?\nPor enunciado se sabe que \\(\\displaystyle \\mathbb{P}(R\\vert B)=0.6 \\Rightarrow \\mathbb{P}(R^{c}\\vert B)=0.4= \\dfrac{\\mathbb{P}(R^{c}\\cap B)}{\\mathbb{P}(B)}\\), pero\n\\[\\mathbb{P}(B)=0.55 \\Rightarrow \\mathbb{P}(R^{c}\\cap B)= 0.4 * 0.55 = 0.22\\]\nNos piden \\(\\mathbb{P}(B\\vert R^{c})=\\dfrac{\\mathbb{P}(B\\cap R^{c})}{\\mathbb{P}(R^{c})}=\\dfrac{0.22}{(1-\\mathbb{P}(R))}\\).\nPor 칤tem anterior sabemos que \\(\\mathbb{P}(R)=0.645 \\Rightarrow \\mathbb{P}(R^{c})=1-0.645\\).\nReemplazando,\n\\[\\dfrac{\\mathbb{P}(B\\cap R^{c})}{\\mathbb{P}(R^{c})}=\\dfrac{0.22}{0.355}\\approx 0.62\\]"
  },
  {
    "objectID": "slides/lec_week6.html#tipos-de-variables-aleatorias",
    "href": "slides/lec_week6.html#tipos-de-variables-aleatorias",
    "title": "Variables aleatorias",
    "section": "Tipos de variables aleatorias",
    "text": "Tipos de variables aleatorias\nSe dice que \\(X\\) es una Variable Aleatoria si es una funci칩n que toma valores en probabilidad, es decir, no se puede predecir con certeza sus resultados.\nUna variable aleatoria es siempre cuantitativa y se puede clasificar en los siguientes grupos:\n\\[X(\\omega)\n\\begin{cases}\n\\text{Discreto}\n\\begin{cases}\n\\text{Finito}\\\\\n\\text{Infinito}\n\\end{cases}\\\\\n\\text{Continuo}\n\\begin{cases}\n\\text{Acotados}\\\\\n\\text{No Acotados}\n\\end{cases}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/lec_week6.html#variables-aleatorias-discretas",
    "href": "slides/lec_week6.html#variables-aleatorias-discretas",
    "title": "Variables aleatorias",
    "section": "Variables aleatorias discretas",
    "text": "Variables aleatorias discretas\nUna variable aleatoria \\(X\\) es llamada discreta si:\n\nSu soporte \\(R_X\\) es un conjunto numerable.\nExiste una funci칩n \\(p_X:\\mathbb{R}\\rightarrow [0,1]\\), llamada la funci칩n de masa de probabilidad de \\(X\\), tal que, para cualquier \\(x\\in \\mathbb{R}\\):\n\n\n\\[p_X(x)\\begin{cases} \\mathbb{P}(X=x) \\quad &\\text{si } x\\in R_X\\\\ 0 \\quad &\\text{si } x\\notin R_X\\end{cases}\\]\n\n\nEsta funci칩n tiene dos caracter칤sticas principales:\n\nno-negatividad: \\(p_X(x)\\geq 0\\) para cualquier \\(x\\in \\mathbb{R}\\).\nSuma sobre su soporte es 1: \\(\\sum_{x\\in R_X}p_X(x)=1\\)"
  },
  {
    "objectID": "slides/lec_week6.html#variables-aleatorias-continuas",
    "href": "slides/lec_week6.html#variables-aleatorias-continuas",
    "title": "Variables aleatorias",
    "section": "Variables aleatorias continuas",
    "text": "Variables aleatorias continuas\nUna variable aleatoria \\(X\\) es llamada continua si:\n\nSu soporte \\(R_X\\) es un conjunto no-numerable.\nExiste una funci칩n \\(f_X:\\mathbb{R}\\rightarrow [0,1]\\), llamada funci칩n de densidad de probabilidad de \\(X\\), tal que, para cualquier intervalo \\([a,b]\\subseteq \\mathbb{R}\\):\n\n\n\\[\\mathbb{P}(X\\in [a,b])=\\int_{a}^{b}f_X(x)dx\\]\n\n\nEsta funci칩n tiene dos caracter칤sticas principales:\n\nno-negatividad: \\(f_X(x)\\geq 0\\) para cualquier \\(x\\in \\mathbb{R}\\).\nIntegral sobre \\(\\mathbb{R}\\) es 1: \\(\\int_{-\\infty}^{\\infty} f_X(x)dx=1\\)."
  },
  {
    "objectID": "slides/lec_week6.html#variable-aleatoria-discreta-finita",
    "href": "slides/lec_week6.html#variable-aleatoria-discreta-finita",
    "title": "Variables aleatorias",
    "section": "Variable Aleatoria Discreta Finita",
    "text": "Variable Aleatoria Discreta Finita\n\\(\\varepsilon\\): Experimento Aleatorio: Lanzamiento de un dado cinco veces.\n\\[\\downarrow\\]\n\\(\\Omega\\): Espacio Muestral: Resultados (par o impar) del primer hasta el quinto lanzamiento.\n\\[\\left\\lbrace (I,I,I,I,I);(P,I,I,P,P);\\cdots\\right\\rbrace\\]\n\\[\\downarrow\\]\n\\(X:\\) N칰mero de pares en 5 lanzamientos.\n\\[\\downarrow\\]\n\\[\\mathbb{R}_{X}:\\left\\lbrace 0,1,2,3,4,5\\right\\rbrace\\]"
  },
  {
    "objectID": "slides/lec_week6.html#variable-aleatoria-discreta-infinita",
    "href": "slides/lec_week6.html#variable-aleatoria-discreta-infinita",
    "title": "Variables aleatorias",
    "section": "Variable Aleatoria Discreta Infinita",
    "text": "Variable Aleatoria Discreta Infinita\n\\(\\varepsilon\\): Experimento Aleatorio: Lanzamiento de un dado hasta que ocurra el primer par.\n\\[\\downarrow\\]\n\\(\\Omega\\): Espacio Muestral: Resultados (par o impar) del lanzamiento hasta que ocurra el primer par. \\[\\left\\lbrace (P);(I,P);(I,I,P)\\cdots\\right\\rbrace\\]\n\\[\\downarrow \\]\n\\(X:\\) N칰mero lanzamientos hasta que ocurra el primer par\n\\[\\downarrow\\]\n\\[\\mathbb{R}_{X}:\\left\\lbrace 1,2,3,4,5,\\cdots\\right\\rbrace\\]"
  },
  {
    "objectID": "slides/lec_week6.html#variable-aleatoria-continua-no-acotada",
    "href": "slides/lec_week6.html#variable-aleatoria-continua-no-acotada",
    "title": "Variables aleatorias",
    "section": "Variable Aleatoria Continua No Acotada",
    "text": "Variable Aleatoria Continua No Acotada\n\\(\\varepsilon\\): Experimento Aleatorio: Lanzamiento de un dado hasta que se obtenga un n칰mero par.\n\\[\\downarrow\\]\n\\(\\Omega\\): Espacio Muestral: Tiempo necesario hasta que el resultado del lanzamiento del dado sea par. \\[\\mathbb{R}^{+}:[0,\\infty[\\]\n\\[\\downarrow\\]\n\\(X:\\) Tiempo hasta la ocurrencia del primer resultado par.\n\\[\\downarrow\\]\n\\[\\mathbb{R}_{X}:\\mathbb{R}^{+}:[0,\\infty[\\]"
  },
  {
    "objectID": "slides/lec_week6.html#funci칩n-de-distribuci칩n",
    "href": "slides/lec_week6.html#funci칩n-de-distribuci칩n",
    "title": "Variables aleatorias",
    "section": "Funci칩n de distribuci칩n",
    "text": "Funci칩n de distribuci칩n\nLas variables aleatorias son usualmente caracterizadas en t칠rminos de sus funciones de distribuci칩n.\n\nSea \\(X\\) una variable aleatoria. La funci칩n de distribuci칩n de \\(X\\) es una funci칩n \\(F_X:\\mathbb{R}\\rightarrow [0,1]\\) tal que:\n\\[F_X(x)=\\mathbb{P}(X\\leq x), \\forall x\\in \\mathbb{R}\\]\n\n\nSi conocemos la funci칩n de distribuci칩n de una variable aleatoria \\(X\\), entonces podemos f치cilmente calcular la probabilidad que \\(X\\) pertenezca a un intervalo \\((a,b] \\subseteq \\mathbb{R}\\) como:\n\\[\\mathbb{P}(a<X<b)=F_X(b)-F_X(a)\\]"
  },
  {
    "objectID": "slides/lec_week6.html#valores-esperados",
    "href": "slides/lec_week6.html#valores-esperados",
    "title": "Variables aleatorias",
    "section": "Valores esperados",
    "text": "Valores esperados\nSea \\(X\\) una variable aleatoria, entonces se define el valor esperado de una funci칩n real \\(g(X)\\), como:\n\\[\\mathbb{E}[g(X)]= \\begin{cases} \\sum_{x\\in \\mathbb{R}} g(X)P(X=x)\\\\ \\int_{x\\in \\mathbb{R}} g(X)f(x)dx \\end{cases}\\]\nSi \\(g(X)=X\\), diremos que el valor esperado o esperanza matem치tica de \\(X\\) es:\n\\[\\mathbb{E}(X)=\\begin{cases}\\sum_{x\\in \\mathbb{R}} x P(X=x)\\\\ \\int_{x\\in \\mathbb{R}} x f(x)dx \\end{cases}\\]\nPara variables de tipo discreta y continua, respectivamente."
  },
  {
    "objectID": "slides/lec_week6.html#propiedades-de-los-valores-esperados",
    "href": "slides/lec_week6.html#propiedades-de-los-valores-esperados",
    "title": "Variables aleatorias",
    "section": "Propiedades de los valores esperados",
    "text": "Propiedades de los valores esperados\nSean \\(a\\) y \\(b\\) constantes, \\(X\\) una variable aleatoria entonces se cumple que:\n\n\\(\\mathbb{E}(a)=a\\)\n\\(\\mathbb{E}(X)=\\mu=\\) constante\n\\(\\mathbb{E}(aX)=a\\mathbb{E}(X)\\)\n\\(\\mathbb{E}(aX+b)=\\mathbb{E}(aX)+\\mathbb{E}(b)=a\\mathbb{E}(X)+b\\)"
  },
  {
    "objectID": "slides/lec_week6.html#varianza",
    "href": "slides/lec_week6.html#varianza",
    "title": "Variables aleatorias",
    "section": "Varianza",
    "text": "Varianza\nSea \\(X\\) una variable aleatoria, se define el la varianza de \\(X\\) como:\n\\[\\mathbb{E}[(X-\\mathbb{E}(X))^2]=V(X)=\\begin{cases}\\sum_{x\\in\\mathbb{R}} (X-\\mathbb{E}(X))^2P(X=x)\\\\ \\int_{x\\in\\mathbb{R}}(X-\\mathbb{E}(X))^2f_{X}(x)dx\\end{cases}\\]\nPara variables de tipo discreta y continua, respectivamente."
  },
  {
    "objectID": "slides/lec_week6.html#propiedades-de-la-varianza",
    "href": "slides/lec_week6.html#propiedades-de-la-varianza",
    "title": "Variables aleatorias",
    "section": "Propiedades de la varianza",
    "text": "Propiedades de la varianza\nSea \\(a\\) y \\(b\\) constantes, \\(X\\) una variable aleatoria, entonces se cumple:\n\n\\(\\mathbb{V}(a)=0\\)\n\\(\\mathbb{V}(X)=\\sigma^2=\\) constante\n\\(\\mathbb{V}(aX)=a^2 \\mathbb{V}(X)\\)\n\\(\\mathbb{V}(aX+b)=\\mathbb{V}(aX)+\\mathbb{V}(b)=a^2\\mathbb{V}(X)+0=a^2\\mathbb{V}(X)\\)\n\\(\\mathbb{V}(X)=\\mathbb{E}(X^2)-(\\mathbb{E}(X))^2\\)"
  },
  {
    "objectID": "slides/lec_week6.html#esperanza-y-varianza-condicional",
    "href": "slides/lec_week6.html#esperanza-y-varianza-condicional",
    "title": "Variables aleatorias",
    "section": "Esperanza y varianza condicional",
    "text": "Esperanza y varianza condicional\nSea \\(X\\) e \\(Y\\) variables aleatorias discretas. La esperanza condicional de \\(X\\) dado que \\(Y=y\\), donde \\(f_{Y}(y)>0\\), se define por:\n\\[\\mathbb{E}[X|Y=y]=\\sum_{x\\in\\mathbb{R}}x\\mathbb{P}(X=x|Y=y)= \\sum_{x\\in\\mathbb{R}} x \\dfrac{\\mathbb{P}(X=x,Y=y)}{\\mathbb{P}(Y=y)}\\]\nNotar que y toma todos los valores del recorrido de \\(Y\\)."
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-1",
    "href": "slides/lec_week6.html#ejemplo-1",
    "title": "Variables aleatorias",
    "section": "Ejemplo #1",
    "text": "Ejemplo #1\nSea \\(X\\) una variable aleatoria discreta que tiene la siguiente funci칩n de cuant칤a:\n\\[P_{X}(1)=\\dfrac{1}{2} \\hspace{30pt} P_{X}(2)=\\dfrac{1}{4} \\hspace{30pt} P_{X}(3)=\\dfrac{1}{8} \\hspace{30pt} P_{X}(4)=\\dfrac{1}{8}\\]\n\nEncontrar y graficar la funci칩n de distribuci칩n acumulada \\(F_{X}(x)\\) de la variable aleatoria \\(X\\).\nEncontrar \\(\\mathbb{P}(X\\leq1)\\), \\(\\mathbb{P}(1<X\\leq3)\\), \\(\\mathbb{P}(1\\leq X \\leq 3)\\)."
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-1",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-1",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo #1",
    "text": "Resoluci칩n ejemplo #1\nLa funci칩n de distribuci칩n acumulado est치 dada por: \\[\\begin{align*}\nF_{X}(x)\n\\begin{cases}\n0 \\hspace{20pt} \\text{si } X< 1\\\\\n\\dfrac{1}{2} \\hspace{20pt} \\text{si } 1 \\leq X < 2  \\\\\n\\dfrac{1}{2}+\\dfrac{1}{4}=\\dfrac{3}{4} \\hspace{20pt} \\text{si } 2\\leq X < 3\\\\\n\\dfrac{3}{4}+\\dfrac{1}{8}=\\dfrac{7}{8} \\hspace{20pt} \\text{si } 3\\leq X < 4\\\\\n1 \\hspace{20pt} \\text{si } X \\geq  4\\\\\n\\end{cases}\n\\end{align*}\\] El gr치fico de esta funci칩n es igual que graficar una funci칩n escalonada."
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-1-continuaci칩n",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-1-continuaci칩n",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo #1: continuaci칩n",
    "text": "Resoluci칩n ejemplo #1: continuaci칩n\nLuego, usando la informaci칩n dada por la funci칩n de distribuci칩n.\n\n\\(\\mathbb{P}(X\\leq 1)=F_{X}(1)=\\dfrac{1}{2}\\)\n\\(\\mathbb{P}(1<X\\leq 3)=\\mathbb{P}(X \\leq 3)-\\mathbb{P}(X \\leq 1)=F_{X}(3)-F_{X}(1)=\\dfrac{7}{8}-\\dfrac{1}{2}=\\dfrac{3}{8}\\)\n\\(\\mathbb{P}(1 \\leq X \\leq 3)=\\mathbb{P}(X\\leq 3)=\\dfrac{7}{8}\\)"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-2",
    "href": "slides/lec_week6.html#ejemplo-2",
    "title": "Variables aleatorias",
    "section": "Ejemplo #2",
    "text": "Ejemplo #2\nConsiderar la variable aleatoria discreta \\(X\\) cuya funci칩n de cuant칤a est치 dada por:\n\\[\\begin{align*}\np_{X}(x)=\\begin{cases}\n\\dfrac{1}{3} \\hspace{20pt} x=-1,0,1\\\\\n0 \\hspace{20pt} e.o.c.\n\\end{cases}\n\\end{align*}\\]\n\n\nGraficar \\(p_{X}(x)\\) y encontrar la esperanza y varianza de X.\nRepetir lo anterior considerando la funci칩n de cuant칤a como:\n\n\n\n\\[\\begin{align*}\np_{X}(x)=\\begin{cases}\n\\dfrac{1}{3} \\hspace{20pt} x=-2,0,2\\\\\n0 \\hspace{20pt} e.o.c.\n\\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-2",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-2",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo #2",
    "text": "Resoluci칩n ejemplo #2\nLa esperanza y varianza de la variable aleatoria \\(X\\) la podemos obtener por definici칩n, en el primer caso: \\[\\begin{align*}\n\\mathbb{E}(X)&=-1\\cdot \\mathbb{P}(X=-1)+0\\cdot \\mathbb{P}(X=0)+1\\cdot \\mathbb{P}(X=1)\\\\\n&= -1 \\cdot \\dfrac{1}{3} + 0 + 1\\cdot \\dfrac{1}{3} = 0\\\\\n\\end{align*}\\] y la varianza est치 dada por: \\[\\begin{align*}\n\\mathbb{V}(X)&=\\mathbb{E}(X^2)-(\\mathbb{E}(X))^2=\\mathbb{E}(X^2)\\\\\n&= -1^2 \\cdot \\mathbb{P}(X=-1)+0\\cdot \\mathbb{P}(X=0)+1^2\\cdot \\mathbb{P}(X=1)\\\\\n&= \\dfrac{1}{3}+0+\\dfrac{1}{3}=\\dfrac{2}{3}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-2-1",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-2-1",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo #2",
    "text": "Resoluci칩n ejemplo #2\nAhora en el segundo caso: \\[\\begin{align*}\n\\mathbb{E}(X)&=-2\\cdot \\mathbb{P}(X=-2)+0\\cdot \\mathbb{P}(X=0)+2\\cdot \\mathbb{P}(X=2)\\\\\n&= -2 \\cdot \\dfrac{1}{3} + 0 + 2\\cdot \\dfrac{1}{3} = 0\\\\\n\\end{align*}\\] y la varianza est치 dada por: \\[\\begin{align*}\n\\mathbb{V}(X)&=\\mathbb{E}(X^2)-(\\mathbb{E}(X))^2=\\mathbb{E}(X^2)\\\\\n&= -2^2 \\cdot \\mathbb{P}(X=-1)+0\\cdot \\mathbb{P}(X=0)+2^2\\cdot \\mathbb{P}(X=1)\\\\\n&= \\dfrac{4}{3}+0+\\dfrac{4}{3}=\\dfrac{8}{3}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-3",
    "href": "slides/lec_week6.html#ejemplo-3",
    "title": "Variables aleatorias",
    "section": "Ejemplo #3",
    "text": "Ejemplo #3\nConsidere el lanzamiento de 3 monedas con denominaci칩n de \\(1\\), \\(5\\) y \\(10\\) pesos, respectivamente. Sea \\(X\\) la suma de las monedas que caen cara.\n\n쮺u치l es el valor esperado de \\(X\\) dado que dos monedas caen cara?\nSea \\(Y\\) la suma de las monedas que caen cara, y que adem치s, tienen denominaci칩n de \\(1\\) o \\(5\\) pesos. 쮺u치l es la esperanza condicional de \\(X\\) dado \\(Y\\)?"
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-3",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-3",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo 3",
    "text": "Resoluci칩n ejemplo 3\nDefinamos primero el espacio muestral del experimento aleatorio:\n\\[\\Omega=\\{CCC,CCS,CSC,SCC,CSS,SCS,SSC,SSS\\}\\]\nSi definimos el evento \\(B\\) como el evento en que dos monedas caen cara, entonces:\n\\[B=\\{CCS,CSC,SCC\\}\\]\nNos interesa determinar el valor de \\(E(X|B)\\). Primero, notamos que cada punto del evento \\(B\\) tiene una probabilidad de ocurrencia de \\(\\dfrac{1}{8}\\). Luego, obtenemos los valores de la V.A. \\(X\\):\n\\[X(CCS)= 1+5=6, \\hspace{10pt} X(CSC)=1+10=11, \\hspace{10pt} X(SCC)=5+10=15\\]"
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-3-continuaci칩n",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-3-continuaci칩n",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo 3: continuaci칩n",
    "text": "Resoluci칩n ejemplo 3: continuaci칩n\nLuego, calculamos \\(E(X|B)\\) por definici칩n:\n\\[\\mathbb{E}(X|B)=\\dfrac{1}{3/8}\\left( 6 \\dfrac{1}{8}+11\\dfrac{1}{8}+15\\dfrac{1}{8}\\right)=\\dfrac{32}{3}\\]\nPara resolver el 칤tem b, observamos que \\(Y=\\{0,1,5,6\\}\\) con probabilidades:\n\\[\\mathbb{P}(Y=0)=\\mathbb{P}(Y=1)=\\mathbb{P}(Y=5)=\\mathbb{P}(Y=6)=\\dfrac{1}{4}\\]\nSiguiendo el mismo procedimiento que antes: \\[\\begin{align*}\n\\mathbb{E}(X|\\{Y=0\\})=5,\\hspace{10pt}\\mathbb{E}(X|\\{Y=1\\})=6\\\\\n\\mathbb{E}(X|\\{Y=5\\})=10,\\hspace{10pt}\\mathbb{E}(X|\\{Y=6\\})=11\\\\\n\\end{align*}\\] En donde \\(\\mathbb{E}(X|\\{Y=0\\})=\\dfrac{1}{1/4}\\left(\\dfrac{1}{8}\\overbrace{X(SSC)}^{10}+\\dfrac{1}{8}\\overbrace{X(SSS)}^{0}\\right)=5\\)"
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-3-1",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-3-1",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo 3",
    "text": "Resoluci칩n ejemplo 3\nAs칤, podemos expresar la esperanza condicional de \\(X\\) dado \\(Y\\) como: \\[\\begin{align*}\n\\mathbb{E}(X|Y)(\\omega)\n\\begin{cases}\n5 \\hspace{10pt}\\text{si } Y(\\omega)=0,\\\\\n6 \\hspace{10pt}\\text{si } Y(\\omega)=1,\\\\\n10 \\hspace{10pt}\\text{si } Y(\\omega)=5,\\\\\n11 \\hspace{10pt}\\text{si } Y(\\omega)=6.\\\\\n\\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-4",
    "href": "slides/lec_week6.html#ejemplo-4",
    "title": "Variables aleatorias",
    "section": "Ejemplo #4",
    "text": "Ejemplo #4\nUna variable aleatoria \\(X\\) tiene funci칩n de densidad:\n\\[\\begin{align*}\nf(x)=\n\\begin{cases}\n\\dfrac{c}{x^2+1} , & -\\infty < x < \\infty \\\\\n0 ,& e.o.c\\\\\n\\end{cases}\n\\end{align*}\\]\n\nHallar el valor de la constante c.\nHallar la probabilidad de que \\(X^2\\) est칠 entre \\(\\dfrac{1}{3}\\) y \\(1\\)."
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-4",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-4",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo #4",
    "text": "Resoluci칩n ejemplo #4\nPara hallar el valor de la constante C, utilizamos las propiedades de la funci칩n de densidad: \\(\\int_{-\\infty}^{\\infty} f(x)dx=1\\). As칤:\n\\[\\int_{-\\infty}^{\\infty} \\dfrac{c}{x^2+1}dx = c\\tan^{-1}\\Bigg\\vert_{-\\infty}^{\\infty}=c\\left[ \\dfrac{\\pi}{2}-(-\\dfrac{\\pi}{2})\\right]=1\\Rightarrow c=\\dfrac{1}{\\pi}\\]\nLuego, para hallar la probabilidad pedida en el item b:\n\\[\\text{Si } \\dfrac{1}{3}\\leq X^2 \\leq 1, \\text{ entonces } \\dfrac{\\sqrt{3}}{3}\\leq X \\leq 1 \\text{ o } -1 \\leq X \\leq -\\dfrac{\\sqrt{3}}{3}\\]\nPor lo que la probabilidad pedida est치 dada por:\n\\[\\dfrac{1}{\\pi}\\int_{-1}^{-\\dfrac{\\sqrt{3}}{3}}\\dfrac{dx}{x^2+1}+\\dfrac{1}{\\pi}\\int_{\\dfrac{\\sqrt{3}}{3}}^{1}\\dfrac{dx}{x^2+1}=\\dfrac{2}{\\pi}\\int_{\\dfrac{\\sqrt{3}}{3}}^{1} \\dfrac{dx}{x^2+1}=\\dfrac{1}{6}\\]"
  },
  {
    "objectID": "slides/lec_week6.html#momentos-de-una-variable-aleatoria",
    "href": "slides/lec_week6.html#momentos-de-una-variable-aleatoria",
    "title": "Variables aleatorias",
    "section": "Momentos de una variable aleatoria",
    "text": "Momentos de una variable aleatoria\nSean \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria con funci칩n de masa de probabilidad \\(f_{X}\\). Entonces el \\(r\\)-칠simo momento poblacional en torno a cero se define por:\n\\[\\mu_r=\\mathbb{E}[X^r]\\]\ndonde se puede observar, que para el caso de \\(r=1\\), se obtiene la esperanza matem치tica."
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-binomial",
    "href": "slides/lec_week6.html#distribuci칩n-binomial",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n binomial",
    "text": "Distribuci칩n binomial\nSea \\(X\\) una variable aleatoria que representa el n칰mero de 칠xitos en \\(n\\) ensayos y \\(p\\) la probabilidad de 칠xito con cualquiera de 칠stos. Se dice entonces que \\(X\\) tiene una distribuci칩n binomial con funci칩n de probabilidad:\n\\[\\mathbb{P}(X=k)= {{n}\\choose{k}}p^k(1-p)^{n-k} \\hspace{20pt} k=1,2,\\cdots,n\\]\nEn donde \\(\\displaystyle {{n}\\choose{k}}\\) es el coeficiente binomial, esto es:\n\\[\\displaystyle{{n}\\choose{k}}=\\dfrac{n!}{k!(n-k)!}\\]\nSi \\(n=1\\) diremos que \\(X\\) sigue una distribuci칩n Bernoulli."
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-binomial-propiedades",
    "href": "slides/lec_week6.html#distribuci칩n-binomial-propiedades",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n binomial: propiedades",
    "text": "Distribuci칩n binomial: propiedades\nSi \\(X\\) tiene una distribuci칩n binomial, entonces se cumple que:\n\n\\(\\mathbb{E}[X]=np\\)\n\\(\\mathbb{V}[X]=np(1-p)\\)\n\n\nEs claro ver que si \\(X\\) tiene una distribuci칩n bernoulli, entonces:\n\n\\(\\mathbb{E}[X]=p\\)\n\\(\\mathbb{V}[X]=p(1-p)\\)"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-binomial-gr치fico",
    "href": "slides/lec_week6.html#distribuci칩n-binomial-gr치fico",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n binomial: gr치fico",
    "text": "Distribuci칩n binomial: gr치fico"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-binomial-ejemplo",
    "href": "slides/lec_week6.html#distribuci칩n-binomial-ejemplo",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n Binomial: ejemplo",
    "text": "Distribuci칩n Binomial: ejemplo\nDurante los 칰ltimos a침os, se ha logrado establecer que el 30% de los alumnos que ingresan por primera vez a cierta Universidad, reprueban todas las asignaturas de primer semestre. Si, en el segundo semestre, se elige al azar a 15 alumnos que ingresaron el semestre anterior a la Universidad.\n\n쮺u치l es la probabilidad que s칩lo 5 de ellos hayan reprobado todas las asignaturas del primer semestre?\n쮺u치l es la probabilidad que a lo m치s 13 hayan reprobado todas las asignaturas del primer semestre?\n쮺u치l es la probabilidad de que 8 o m치s hayan reprobado todas las asignaturas?"
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo",
    "text": "Resoluci칩n ejemplo\nLo primero es reconocer que el contexto del problema es posible modelarlo mediante una distribuci칩n binomial: n칰mero de 칠xitos dada una probabilidad conocida. Sabemos que para calcular las probabilidades bajo este distribuci칩n es necesario saber dicha probabilidad de 칠xito \\(p\\). Por enunciado sabemos que \\(p=0.3\\) y \\(n=15\\). Luego, definimos la variable aleatoria:\n\\[\\begin{align*}\nX= \\text{N춿 de alumnos que reprueban todas las asignaturas al ingresar}\\\\\n\\text{ por 1ra vez a cierta Universidad.}\n\\end{align*}\\]\nAhora podemos calcular las probabilidad pedidas, de las cuales debemos reconocer:\n\n\n\\(\\mathbb{P}(X=5)=F_{X}(5)-F_{X}(4)=0.7216-0.5155=0.2061\\)\n\\(\\mathbb{P}(X\\leq 13)=F_{X}(13)\\approx 1\\)\n\\(\\mathbb{P}(X\\geq 8)=1-\\mathbb{P}(X<8)=1-\\mathbb{P}(X\\leq 7)=1-F_{X}(7)=1-0.9500=0.0173\\)\n\n\n\nUtilizamos la notaci칩n \\(X\\sim Bin(15,0.3)\\) para mostrar la distribuci칩n de la variable aleatoria."
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-de-poisson",
    "href": "slides/lec_week6.html#distribuci칩n-de-poisson",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n de Poisson",
    "text": "Distribuci칩n de Poisson\nSea \\(X\\) una variable aleatoria que representa el n칰mero de eventos aleatorios independientes que ocurren a una rapidez constante sobre el tiempo o el espacio. Se dice entonces que la variable aleatoria \\(X\\) tiene una distribuci칩n de Poisson con funci칩n de probabilidad:\n\\[\\mathbb{P}(X=k)=\\dfrac{e^{-\\lambda}\\lambda^k}{k!} \\hspace{20pt} k=0,1,\\cdots,n,\\cdots\\]\nEn donde \\(\\lambda>0\\) representa el n칰mero promedio de ocurrencias del evento aleatorio por unidad de tiempo. Adem치s, si \\(X\\) sigue una distribuci칩n de Poisson se cumple que:\n\n\\(\\mathbb{E}[X]=\\lambda\\)\n\\(\\mathbb{V}[X]=\\lambda\\)"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-poisson-gr치fico",
    "href": "slides/lec_week6.html#distribuci칩n-poisson-gr치fico",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n Poisson: gr치fico",
    "text": "Distribuci칩n Poisson: gr치fico"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-de-poisson-ejemplo",
    "href": "slides/lec_week6.html#distribuci칩n-de-poisson-ejemplo",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n de Poisson: Ejemplo",
    "text": "Distribuci칩n de Poisson: Ejemplo\nEn un estudio invernal de una tienda, se determin칩 que un articulo se pide en promedio cinco veces por semana (de 5 d칤as), de acuerdo a una distribuci칩n Poisson. 쮺u치l es la probabilidad de que en un d칤a especifico, el articulo.\n\nSe pida m치s de cinco veces.\nNo se pida."
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-5",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-5",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo",
    "text": "Resoluci칩n ejemplo\nPara resolver este tipo de problemas, lo primero es reconocere que es posible modelar la variable aleatoria mediante una distribuci칩n de Poissi칩n. Como la distribuci칩n de Poisson tiene un par치metros (\\(\\lambda\\)), este debe ser sabido para poder calcular las probabilidades. Por enunciado sabemos que la tasa de ocurrencia es 5 en una semana. Como siempre definimos la variable aleatoria antes de cualquier c치lculo.\n\\[X= \\text{N춿 de art칤culos que se pide en una tienda en una semana dada.}\\]\nPor lo que, utilizando la notaci칩n adecuada: \\(X\\sim Poisson(5)\\)"
  },
  {
    "objectID": "slides/lec_week6.html#resoluci칩n-ejemplo-continuaci칩n",
    "href": "slides/lec_week6.html#resoluci칩n-ejemplo-continuaci칩n",
    "title": "Variables aleatorias",
    "section": "Resoluci칩n ejemplo: continuaci칩n",
    "text": "Resoluci칩n ejemplo: continuaci칩n\n\nNos pregunta la probabilidad que en un d칤a espec칤fico se pida m치s de cinco veces. Nuestra informaci칩n original (V.A. \\(X\\)) refiera a una semana, por lo que si definimos una nueva variable aleatoria como:\n\n\n\\[Y= \\text{N춿 de art칤culos que se pide en una tienda en un d칤a dado.}\\]\nPodemos afirmar que \\(Y\\sim Poisson(1)\\), debido a que se asume una rapidez constante de ocurrencia. As칤, lo pedido lo podemos escribir como \\(\\mathbb{P}(Y>5)\\) y calculamos:\n\\[\\mathbb{P}(Y>5)=1-\\mathbb{P}(Y\\leq 5)=1-0.9994=0.0006\\]\n\n\\(\\mathbb{P}(Y=0)=0.3679\\)"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-geom칠trica",
    "href": "slides/lec_week6.html#distribuci칩n-geom칠trica",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n geom칠trica",
    "text": "Distribuci칩n geom칠trica\nSea \\(X\\) una variable aleatoria que representa el n칰mero de fallas que ocurren antes de que se presente el primer 칠xito.Se dice entonces que la variable aleatoria \\(X\\) tiene una distribuci칩n geom칠trica con funci칩n de probabilidad:\n\\[\\mathbb{P}(X=k)=(1-p)^{k-1}p \\hspace{20pt} k=1,2,\\cdots\\]\nEn donde \\(p\\) es la probabilidad de 칠xito. Adem치s, Si \\(X\\) sigue una distribuci칩n Geom칠trica, entonces se cumple que:\n\n\\(\\displaystyle \\mathbb{E}[X]=\\dfrac{1}{p}\\)\n\\(\\mathbb{V}[X]=\\dfrac{(1-p)}{p^2}\\)"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-geom칠trica-gr치fico",
    "href": "slides/lec_week6.html#distribuci칩n-geom칠trica-gr치fico",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n geom칠trica: gr치fico",
    "text": "Distribuci칩n geom칠trica: gr치fico"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-hipergeom칠trica",
    "href": "slides/lec_week6.html#distribuci칩n-hipergeom칠trica",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n hipergeom칠trica",
    "text": "Distribuci칩n hipergeom칠trica\nSea \\(N\\) el n칰mero total de objetos de una poblaci칩n finita, de manera tal que \\(k\\) de 칠stos es de un tipo y \\(N-k\\) de otros. Si se selecciona una muestra aleatoria de la poblaci칩n constituida por \\(n\\) objetos de la probabilidad de que \\(x\\) sea de un tipo exactamente y \\(n-x\\) sea del otro, est치 dada por la funci칩n de probabilidad hipergeom칠trica:\n\\[\\displaystyle \\mathbb{P}(X=x)= \\dfrac{           {{k}\\choose{x}} {{N-k}\\choose{n-x}}  }{  {{N}\\choose{n}}}\\hspace{20pt} x=1,2,\\cdots,n; x \\leq k, n-x\\leq N-k\\]\nSi \\(X\\) sigue una distribuci칩n Hipergeom칠trica, si \\(p=k/N\\)\n\n\\(\\mathbb{E}[X]=np\\)\n\\(\\mathbb{V}[X]=np(1-p)\\left( \\dfrac{N-n}{N-1}\\right)\\)"
  },
  {
    "objectID": "slides/lec_week6.html#distribuci칩n-hipergeom칠trica-gr치fico",
    "href": "slides/lec_week6.html#distribuci칩n-hipergeom칠trica-gr치fico",
    "title": "Variables aleatorias",
    "section": "Distribuci칩n hipergeom칠trica: gr치fico",
    "text": "Distribuci칩n hipergeom칠trica: gr치fico"
  },
  {
    "objectID": "slides/lec_week7.html#distribuci칩n-normal",
    "href": "slides/lec_week7.html#distribuci칩n-normal",
    "title": "Distribuciones continuas",
    "section": "Distribuci칩n normal",
    "text": "Distribuci칩n normal\nSea \\(X\\) una variable aleatoria que toma valores reales, esto es: \\(-\\infty<x<\\infty\\), diremos que \\(X\\) sigue una distribuci칩n normal (o Gaussiana) si su funci칩n de densidad est치 por:\n\\[f_{X}(x)=\\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\dfrac{1}{2}\\left(\\dfrac{x-\\mu}{\\sigma}\\right) ^2\\right]\\]\nEn donde los par치metros de la distribuci칩n son \\(\\mu\\) y \\(\\sigma\\) satisfacen las condiciones:\n\\[\\begin{align*}\n-\\infty<\\mu<\\infty\\\\\n\\sigma^2>0\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week7.html#propiedades-de-la-distribuci칩n-normal",
    "href": "slides/lec_week7.html#propiedades-de-la-distribuci칩n-normal",
    "title": "Distribuciones continuas",
    "section": "Propiedades de la distribuci칩n normal",
    "text": "Propiedades de la distribuci칩n normal\nSi \\(X\\) sigue una distribuci칩n normal de par치metros \\(\\mu\\) y \\(\\sigma\\), entonces se cumple que:\n\n\\(\\mathbb{E}(X)=\\mu\\)\n\\(\\mathbb{V}(X)=E[X^2]-(E[X])^2=\\sigma^2\\)\nSi \\(Y=aX+b\\) entonces \\(Y\\) sigue una distribuci칩n normal de par치metros \\(a\\mu+b\\) y \\(a^2\\sigma^2\\), respectivamente. Se escribe: \\(Y \\sim N(a\\mu+b,a^2\\sigma^2)\\)"
  },
  {
    "objectID": "slides/lec_week7.html#densidad-de-la-distribuci칩n-normal",
    "href": "slides/lec_week7.html#densidad-de-la-distribuci칩n-normal",
    "title": "Distribuciones continuas",
    "section": "Densidad de la distribuci칩n normal",
    "text": "Densidad de la distribuci칩n normal"
  },
  {
    "objectID": "slides/lec_week7.html#ejemplo-distribuci칩n-normal",
    "href": "slides/lec_week7.html#ejemplo-distribuci칩n-normal",
    "title": "Distribuciones continuas",
    "section": "Ejemplo distribuci칩n Normal",
    "text": "Ejemplo distribuci칩n Normal\nLa duraci칩n de un l치ser semiconductor a potencia constante tiene una distribuci칩n normal con media 7.000 horas y desviaci칩n est치ndar de 600 horas.\n\n쮺u치l es la probabilidad de que el l치ser falle antes de 5.000 horas?\n쮺u치l es la duraci칩n en horas excedida por el 99% de los lasers?\nSi se hace uso de tres l치ser en un producto y se supone que fallan de manera independiente. 쮺u치l es la probabilidad de que los tres sigan funcionando despu칠s de 6700 horas?"
  },
  {
    "objectID": "slides/lec_week7.html#resoluci칩n-ejemplo",
    "href": "slides/lec_week7.html#resoluci칩n-ejemplo",
    "title": "Distribuciones continuas",
    "section": "Resoluci칩n ejemplo",
    "text": "Resoluci칩n ejemplo\nPor enunciado, sabemos que si definimos la variable aleatoria \\(X\\) como:\n\\[X= \\text{Duraci칩n de un laser semiconductor a potencia constante.}\\]\nEntonces, podemos afirmar que \\(X\\sim N(7000,600^2)\\). Luego,\n\\[\\begin{align*}\n\\mathbb{P}(X<5000)&=\\mathbb{P}\\left(\\dfrac{X-7000}{600} < \\dfrac{5000-7000}{600}\\right)\\\\\n&=\\mathbb{P}(Z< -3.333); \\hspace{10pt} Z\\sim N(0,1)\\\\\n&=0.0004\n\\end{align*}\\]\ny,\n\\[\\begin{align*}\n&\\mathbb{P}(X>x)=0.99 \\Rightarrow 1-\\mathbb{P}(X\\leq x)=0.99\\\\\n&\\mathbb{P}\\left(\\dfrac{X-7000}{600} \\leq \\dfrac{x-7000}{600}\\right)=\\mathbb{P}(Z\\leq z)=0.01\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week7.html#resoluci칩n-ejemplo-continuaci칩n",
    "href": "slides/lec_week7.html#resoluci칩n-ejemplo-continuaci칩n",
    "title": "Distribuciones continuas",
    "section": "Resoluci칩n ejemplo: continuaci칩n",
    "text": "Resoluci칩n ejemplo: continuaci칩n\n\\[\\mathbb{P}(Z\\leq z)=0.01 \\Rightarrow z=-2.325\\]\nAhora nos devolvemos a la variable original \\(X\\), as칤:\n\\[-2.325=\\dfrac{x-7000}{600} \\Rightarrow x=5605\\]\nPor lo que, la duraci칩n en horas excedida por el 99% de los lasers es de 5605 horas."
  },
  {
    "objectID": "slides/lec_week7.html#resoluci칩n-ejemplo-continuaci칩n-1",
    "href": "slides/lec_week7.html#resoluci칩n-ejemplo-continuaci칩n-1",
    "title": "Distribuciones continuas",
    "section": "Resoluci칩n ejemplo: continuaci칩n",
    "text": "Resoluci칩n ejemplo: continuaci칩n\nAhora, para el item c) debemos reconocer como modelar la variable aleatoria del problema. Definimos la variable aleatoria:\n\\[Y= \\text{ N춿 de lasers que siguen funcionando despu칠s de 6700 hrs}\\]\nDebimos reconocer que esta variable aleatoria tiene distribuci칩n \\(Bin(3,p)\\) en donde \\(p\\) es la probabilidad que uno de los lasers siga funcionando despu칠s de 6700 horas. Por lo que primero calculamos este valor:\n\\[\\begin{align*}\n\\mathbb{P}(X>6700)&=1-\\mathbb{P}(X\\leq 6700)=1-\\mathbb{P}\\left(\\dfrac{X-7000}{600} < \\dfrac{6700-7000}{600}\\right)\\\\\n&=1-\\mathbb{P}(Z<-0.5)=1-.3085=0.6915\n\\end{align*}\\]\nAs칤, \\(Y\\sim Bin(3,0.6915)\\). Finalmente, lo preguntado lo planteamos como:\n\\[\\mathbb{P}(Y=3)=\\mathbb{P}(Y\\leq 3)-\\mathbb{P}(Y\\leq 2)= 1- 0.657 = 0.343\\]"
  },
  {
    "objectID": "slides/lec_week7.html#distribuci칩n-uniforme",
    "href": "slides/lec_week7.html#distribuci칩n-uniforme",
    "title": "Distribuciones continuas",
    "section": "Distribuci칩n Uniforme",
    "text": "Distribuci칩n Uniforme\nSea \\(X\\) una variable aleatoria continua, diremos que \\(X\\) sigue una distribuci칩n uniforme sobre el intervalo \\((a,b)\\) si su funci칩n de densidad de probabilidad est치 dada por:\n\\[\\begin{align*}\nf_{X}(x)=\\begin{cases}\n1/(b-a) \\hspace{20pt} a\\leq x \\leq b\\\\\n0 \\hspace{20pt} e.o.c\n\\end{cases}\n\\end{align*}\\]\nLos par치metros de la distribuci칩n cumplen las condiciones:\n\\[-\\infty<a<\\infty, \\quad  -\\infty<b<\\infty\\]\n\n\\(\\mathbb{E}(X)=\\dfrac{(a+b)}{2}\\)\n\\(\\mathbb{V}(X)=\\dfrac{(b-a)^2}{12}\\)"
  },
  {
    "objectID": "slides/lec_week7.html#densidad-de-la-distribuci칩n-uniforme",
    "href": "slides/lec_week7.html#densidad-de-la-distribuci칩n-uniforme",
    "title": "Distribuciones continuas",
    "section": "Densidad de la distribuci칩n uniforme",
    "text": "Densidad de la distribuci칩n uniforme"
  },
  {
    "objectID": "slides/lec_week7.html#distribuci칩n-exponencial",
    "href": "slides/lec_week7.html#distribuci칩n-exponencial",
    "title": "Distribuciones continuas",
    "section": "Distribuci칩n exponencial",
    "text": "Distribuci칩n exponencial\nSea \\(X\\) una variable aleatoria continua que toma valores positivos, diremos que \\(X\\) sigue una distribuci칩n exponencial con par치metro \\(\\alpha>0\\) si su funci칩n de densidad est치 dada por:\n\\[\\begin{align*}\nf_{X}(x)=\\begin{cases}\n\\alpha e^{-\\alpha x} \\hspace{20pt} x\\geq 0 \\\\\n0 \\hspace{20pt} e.o.c\n\\end{cases}\n\\end{align*}\\]\nAdem치s se cumple que:\n\n\\(\\mathbb{E}(X)=\\dfrac{1}{\\alpha}\\)\n\\(\\mathbb{V}(X)=\\dfrac{1}{\\alpha^2}\\)"
  },
  {
    "objectID": "slides/lec_week7.html#densidad-de-la-distribuci칩n-exponencial",
    "href": "slides/lec_week7.html#densidad-de-la-distribuci칩n-exponencial",
    "title": "Distribuciones continuas",
    "section": "Densidad de la distribuci칩n exponencial",
    "text": "Densidad de la distribuci칩n exponencial"
  },
  {
    "objectID": "slides/lec_week7.html#funci칩n-gamma",
    "href": "slides/lec_week7.html#funci칩n-gamma",
    "title": "Distribuciones continuas",
    "section": "Funci칩n gamma",
    "text": "Funci칩n gamma\nLa funci칩n Gamma denotada por \\(\\Gamma\\) est치 definida por:\n\\[\\Gamma(p)=\\int_{0}^{\\infty} x^{p-1} e^{-x}dx \\hspace{20pt} p>0\\]\nEsta funci칩n cumple las siguientes propiedades:\n\n\\(\\Gamma(n)=(n-1)!\\)\n\\(\\Gamma(1/2)=\\sqrt{\\pi}\\)"
  },
  {
    "objectID": "slides/lec_week7.html#distribuci칩n-gamma",
    "href": "slides/lec_week7.html#distribuci칩n-gamma",
    "title": "Distribuciones continuas",
    "section": "Distribuci칩n gamma",
    "text": "Distribuci칩n gamma\nSea \\(X\\) una variable aleatoria continua que toma valores positivos. Diremos que \\(X\\) sigue una distribuci칩n Gamma si su funci칩n de densidad est치 dada por:\n\\[\\begin{align*}\nf_{X}(x)=\\begin{cases}\n\\dfrac{\\alpha}{\\Gamma(r)}(\\alpha x)^{r-1}e^{-\\alpha x} \\hspace{20pt} x>0\\\\\n0 \\hspace{20pt} e.o.c\n\\end{cases}\n\\end{align*}\\]\nEn donde los par치metros \\(r\\) y \\(\\alpha\\) son positivos.\nEs claro ver que un caso particular de la distribuci칩n Gamma es la distribuci칩n exponencial (\\(r=1\\)). Si \\(X\\) se distribuye Gamma entonces se cumple:\n\n\\(\\mathbb{E}(X)=r/\\alpha\\)\n\\(\\mathbb{V}(X)=r/\\alpha^2\\)"
  },
  {
    "objectID": "slides/lec_week7.html#densidad-de-la-distribuci칩n-gamma",
    "href": "slides/lec_week7.html#densidad-de-la-distribuci칩n-gamma",
    "title": "Distribuciones continuas",
    "section": "Densidad de la distribuci칩n gamma",
    "text": "Densidad de la distribuci칩n gamma"
  },
  {
    "objectID": "slides/lec_week7.html#distribuci칩n-chi-cuadrado",
    "href": "slides/lec_week7.html#distribuci칩n-chi-cuadrado",
    "title": "Distribuciones continuas",
    "section": "Distribuci칩n chi-cuadrado",
    "text": "Distribuci칩n chi-cuadrado\nSea \\(X\\) una variable aleatoria continua que toma valores positivos, diremos que \\(X\\) sigue una distribuci칩n chi-cuadrado con \\(k\\) grados de libertad, si su funci칩n de densidad de probabilidad est치 dada por:\n\\[f(x;k)=\n\\begin{cases}\\displaystyle\n\\frac{1}{2^{k/2}\\Gamma(k/2)}\\,x^{(k/2) - 1} e^{-x/2}&\\text{para }x>0,\\\\\n0&\\text{para }x\\le0\n\\end{cases}\\] donde \\(\\Gamma\\) es la funci칩n gamma. Si \\(X\\) se distribuye Chi-Cuadrado entonces:\n\n\\(\\mathbb{E}[X]=k\\)\n\\(\\mathbb{V}[X]=2k\\)"
  },
  {
    "objectID": "slides/lec_week7.html#densidad-de-la-distribuci칩n-chi-cuadrado",
    "href": "slides/lec_week7.html#densidad-de-la-distribuci칩n-chi-cuadrado",
    "title": "Distribuciones continuas",
    "section": "Densidad de la distribuci칩n chi-cuadrado",
    "text": "Densidad de la distribuci칩n chi-cuadrado"
  },
  {
    "objectID": "slides/lec_week7.html#distribuci칩n-t-student",
    "href": "slides/lec_week7.html#distribuci칩n-t-student",
    "title": "Distribuciones continuas",
    "section": "Distribuci칩n t-student",
    "text": "Distribuci칩n t-student\nSea \\(X\\) una variable aleatoria continua que toma valores reales, diremos que \\(X\\) sigue una distribuci칩n t-student con \\(\\nu\\) grados de libertad, si su funci칩n de densidad de probabilidad est치 dada por:\n\\[f(t) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{t^2}{\\nu} \\right)^{\\!-\\frac{\\nu+1}{2}},\\!\\]\ndonde \\(\\Gamma\\) es la funci칩n gamma. Si \\(X\\) se distribuye t-student entonces:\n\n\\(\\mathbb{E}[X]=0\\) para \\(\\nu>1\\). Indefinida para otros valores.\n\\(\\mathbb{V}[X]=\\dfrac{\\nu}{\\nu -2}\\) para \\(\\nu>2\\). Indefinida para otros valores."
  },
  {
    "objectID": "slides/lec_week7.html#densidad-de-la-distribuci칩n-t-student",
    "href": "slides/lec_week7.html#densidad-de-la-distribuci칩n-t-student",
    "title": "Distribuciones continuas",
    "section": "Densidad de la distribuci칩n t-student",
    "text": "Densidad de la distribuci칩n t-student"
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-de-probabilidad-bivariada-continuaci칩n",
    "href": "slides/lec_week7.html#distribuciones-de-probabilidad-bivariada-continuaci칩n",
    "title": "Distribuciones continuas",
    "section": "Distribuciones de probabilidad bivariada: continuaci칩n",
    "text": "Distribuciones de probabilidad bivariada: continuaci칩n\nAn치logamente que en distribuciones univariadas, la funci칩n de distribuci칩n acumulada bivariada es la probabilidad conjunto de que \\(X\\leq x\\), y \\(Y\\leq y\\), dada por:\n\\[F_{X,Y}(x,y)=\\mathbb{P}(X \\leq x, Y \\leq y)=\\sum_{x_i \\leq x} \\sum_{y_i \\leq y} p(x_i,y_i)\\]\nLa funci칩n de probabilidad conjunta de dos variables aleatorias da origen a las probabilidad puntuales conjuntas, y la funci칩n de distribuci칩n bivariada es una funci칩n escalonada creciente para cada probabilidad puntual distinta de cero, de manera tal que \\(X=x\\) e \\(Y=y\\)."
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-de-probabilidad-bivariada-continuaci칩n-1",
    "href": "slides/lec_week7.html#distribuciones-de-probabilidad-bivariada-continuaci칩n-1",
    "title": "Distribuciones continuas",
    "section": "Distribuciones de probabilidad bivariada: continuaci칩n",
    "text": "Distribuciones de probabilidad bivariada: continuaci칩n\nDe igual manera, es posible definir lo anterior para variables aleatorias continuas. Sean \\(X\\) e \\(Y\\) dos variables aleatorias continuas. Si existe una funci칩n \\(f(x,y)\\) tal que la probabilidad conjunta:\n\\[\\mathbb{P}(a<X<b,c<Y<d)=\\int_{a}^{b}\\int_{c}^{d}f(x,y)dydx\\]\npara cualquier valor de \\(a,b,c\\) y \\(d\\) en donde \\(f(x,y)\\geq 0\\), \\(-\\infty < x,y < \\infty\\) y,\n\\[\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y)dydx =1,\\]\nentonces \\(f(x,y)\\) es la funci칩n de densidad de probabilidad bivariada de \\(X\\) e \\(Y\\)."
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-de-probabilidad-bivariada-continuaci칩n-2",
    "href": "slides/lec_week7.html#distribuciones-de-probabilidad-bivariada-continuaci칩n-2",
    "title": "Distribuciones continuas",
    "section": "Distribuciones de probabilidad bivariada: continuaci칩n",
    "text": "Distribuciones de probabilidad bivariada: continuaci칩n\nLa funci칩n de distribuci칩n bivariada acumulada de \\(X\\) e \\(Y\\) es la probabilidad conjunta de que \\(X\\leq x\\) y \\(Y\\leq y\\), dada por:\n\\[\\mathbb{P}(X \\leq x , Y \\leq y)=F(x,y)=\\int_{-\\infty}^{x} \\int_{-\\infty}^{y} f(u,v)dv,du\\]\nAs칤, la funci칩n de densidad bivariadad se encuentra diferenciando \\(F(x,y)\\) con respecto a \\(x\\) e \\(y\\), es decir:\n\\[f(x,y)=\\dfrac{\\partial^2 F(x,y)}{\\partial x \\partial y}\\]"
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-marginales-de-probabilidad",
    "href": "slides/lec_week7.html#distribuciones-marginales-de-probabilidad",
    "title": "Distribuciones continuas",
    "section": "Distribuciones marginales de probabilidad",
    "text": "Distribuciones marginales de probabilidad\nSean \\(X\\) e \\(Y\\) dos variables aleatorias discretas con una funci칩n de probabilidad conjunta \\(p(x,y)\\). Las funciones marginales de probabilidad de \\(X\\) e \\(Y\\) est치n dadas por:\n\\[p_X(x)=\\sum_{y} p(x,y)\\]\ny,\n\\[p_Y(y)=\\sum_{x} p(x,y),\\]\nrespectivamente."
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-marginales-de-probabilidad-1",
    "href": "slides/lec_week7.html#distribuciones-marginales-de-probabilidad-1",
    "title": "Distribuciones continuas",
    "section": "Distribuciones marginales de probabilidad",
    "text": "Distribuciones marginales de probabilidad\nSean \\(X\\) e \\(Y\\) dos variables aleatorias continuas con una funci칩n de densidad de probabilidad conjunta \\(f(x,y)\\). Las funciones de densidad de probabilidad de \\(X\\) E \\(Y\\) est치n dadas por:\n\\[f_X(x)=\\int_{-\\infty}^{x}f(x,y)dy\\]\ny,\n\\[f_Y(y)=\\int_{-\\infty}^{y}f(x,y)dx\\]"
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-marginales-de-probabilidad-continuaci칩n",
    "href": "slides/lec_week7.html#distribuciones-marginales-de-probabilidad-continuaci칩n",
    "title": "Distribuciones continuas",
    "section": "Distribuciones marginales de probabilidad: continuaci칩n",
    "text": "Distribuciones marginales de probabilidad: continuaci칩n\nPara variables aleatorias continuas conjuntas, si se conoce la funci칩n de distribuci칩n acumulada \\(F(x,y)\\), las distribuciones acumuladas marginales de \\(X\\) e \\(Y\\) se obtienen de la siguiente forma:\n\\[\\mathbb{P}(X\\leq x)=F_X(x)=\\int_{-\\infty}^{x} \\int_{-\\infty}^{\\infty} f(t,y)dydt\\]\ny,\n\\[F_X(x)=\\int_{-\\infty}^{x} f_X(t)dt=F(x,\\infty)\\]\nDe manera similar,\n\\[\\mathbb{P}(Y\\leq y)=F_Y(y)=\\int_{-\\infty}^{y} \\int_{-\\infty}^{\\infty} f(x,t)dydt\\]"
  },
  {
    "objectID": "slides/lec_week7.html#valores-esperados-y-momentos-para-distribuciones-bivariadas",
    "href": "slides/lec_week7.html#valores-esperados-y-momentos-para-distribuciones-bivariadas",
    "title": "Distribuciones continuas",
    "section": "Valores esperados y momentos para distribuciones bivariadas",
    "text": "Valores esperados y momentos para distribuciones bivariadas\nSean \\(X\\) e \\(Y\\) dos variables aleatorias que se distribuyen conjuntamente. El valor esperado de una funci칩n de \\(X\\) y de \\(Y\\), \\(g(x,y)\\) se define como:\n\\[\\mathbb{E}(g(X,Y))=\\sum_{x} \\sum_y g(x,y)p(x,y)\\]\nsi \\(X\\) e \\(Y\\) son V.A. discretas, o\n\\[\\mathbb{E}(g(X,Y))=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y) f(x,y) dydx\\]\nsi \\(X\\) e \\(Y\\) son continuas, en donde \\(p(x,y)\\) y \\(f(x,y)\\) son las funciones de probabilidad y de densidad de probabilidad conjuntas, respectivamente."
  },
  {
    "objectID": "slides/lec_week7.html#momentos-para-distribuciones-bivariadas",
    "href": "slides/lec_week7.html#momentos-para-distribuciones-bivariadas",
    "title": "Distribuciones continuas",
    "section": "Momentos para distribuciones bivariadas",
    "text": "Momentos para distribuciones bivariadas\nEl \\(r-\\)칠simo momento de \\(X\\) alrededor del cero es:\n\\[\\mathbb{E}(X^r)=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x^r f(x,y)dydx=\\int_{-\\infty}^{\\infty}x^r f_X(x)dx\\]\nPor lo que el \\(r\\) y \\(s-\\)칠simo momento producto de \\(X\\) e \\(Y\\) alrededor del origen es:\n\\[\\mathbb{E}(X^r Y^s)=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x^r y^s f(x,y)dydx\\]\ny alrededor de las medias es:\n\\[\\mathbb{E}((X-\\mu_X)^r(Y-\\mu_Y)^s)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_X)^r(y-\\mu_Y)^sf(x,y)dydx\\]\nen donde \\(r\\) y \\(s\\) son enteros, no negativos."
  },
  {
    "objectID": "slides/lec_week7.html#momentos-para-distribuciones-bivariadas-continuaci칩n",
    "href": "slides/lec_week7.html#momentos-para-distribuciones-bivariadas-continuaci칩n",
    "title": "Distribuciones continuas",
    "section": "Momentos para distribuciones bivariadas: continuaci칩n",
    "text": "Momentos para distribuciones bivariadas: continuaci칩n\nEs de particular importancia el momento producto alrededor de las medias cuando \\(r=s=1\\). Este momento producto recibe el nombre de covarianza de \\(X\\) e \\(Y\\), y se encuentra definido por:\n\\[COV(X,Y)=\\mathbb{E}((X-\\mu_X)(Y-\\mu_Y))\\]\nAl igual que la varianza, que es una medida de dispersi칩n de una variable aleatoria, la covarianza es una medida de variabilidad conjunta de \\(X\\) y de \\(Y\\). De esta forma, la covarianza es una medida de asociaci칩n entre los valores de \\(X\\) y de \\(Y\\) y sus respectivas dispersiones. La expresi칩n anterior puede ser reescrita de la forma:\n\\[COV(X,Y)=\\mathbb{E}(XY)-\\mathbb{E}(X)\\mathbb{E}(Y)\\]"
  },
  {
    "objectID": "slides/lec_week7.html#momentos-para-distribuciones-bivariadas-continuaci칩n-1",
    "href": "slides/lec_week7.html#momentos-para-distribuciones-bivariadas-continuaci칩n-1",
    "title": "Distribuciones continuas",
    "section": "Momentos para distribuciones bivariadas: continuaci칩n",
    "text": "Momentos para distribuciones bivariadas: continuaci칩n\nSi la covarianza de \\(X\\) y de \\(Y\\) se divide por el producto de las desviaciones est치ndar de \\(X\\) y de \\(Y\\), el resultado es una cantidad sin dimensiones que recibe el nombre de coeficiente de correlaci칩n y que se denota por \\(\\rho(X,Y)\\), esto es:\n\\[\\rho(X,Y)=\\dfrac{COV(X,Y)}{\\sigma_x \\sigma_y}\\]\nSe puede demostrar que el coeficiente de correlaci칩n \\(\\rho \\in [-1,1]\\)"
  },
  {
    "objectID": "slides/lec_week7.html#variables-aleatorias-independientes",
    "href": "slides/lec_week7.html#variables-aleatorias-independientes",
    "title": "Distribuciones continuas",
    "section": "Variables aleatorias independientes",
    "text": "Variables aleatorias independientes\nSean \\(X\\) e \\(Y\\) dos variables aleatorias con una distribuci칩n conjunta. Se dice que \\(X\\) e \\(Y\\) son estad칤sticamente independientes s칤 y s칩lo si,\n\\[p(x,y)=p_X(x)p_Y(y)\\hspace{20pt} \\text{ Si X e Y son discretas}\\]\no bien,\n\\[f(x,y)=f_X(x)f_Y(y)\\hspace{20pt} \\text{ Si X e Y son continuas}\\]\npara toda \\(x\\) e \\(y\\), en donde \\(p(x,y)\\) y \\(f(x,y)\\) son las funciones bivariadas de probabilidad y de densidad de probabilidad, respectivamente."
  },
  {
    "objectID": "slides/lec_week7.html#variables-aleatorias-independientes-continuaci칩n",
    "href": "slides/lec_week7.html#variables-aleatorias-independientes-continuaci칩n",
    "title": "Distribuciones continuas",
    "section": "Variables aleatorias independientes: continuaci칩n",
    "text": "Variables aleatorias independientes: continuaci칩n\nSe desprende de la definici칩n anterior que si \\(X\\) e \\(Y\\) son V.A. independientes, la probabilidad conjunta:\n\\[\\mathbb{P}(a<X<b,c<Y<d)=\\mathbb{P}(a<X<b)\\mathbb{P}(c<Y<d)\\]\ny por lo anterior,\n\\[\\mathbb{E}(XY)=\\mathbb{E}(X)\\mathbb{E}(Y)\\]\nY si \\(X\\) e \\(Y\\) son V.A. independientes, entonces \\(COV(X,Y)=\\rho(X,Y)=0\\),mas no el converso no es necesariamente cierto."
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-de-probabilidad-condicional",
    "href": "slides/lec_week7.html#distribuciones-de-probabilidad-condicional",
    "title": "Distribuciones continuas",
    "section": "Distribuciones de probabilidad condicional",
    "text": "Distribuciones de probabilidad condicional\nSean \\(X\\) e \\(Y\\) dos variables aleatorias con una funci칩n de densidad conjunta de probabilidad \\(f(x,y)\\). La funci칩n de densidad de probabilidad condicional de la variable aleatoria \\(X\\), denotada por \\(f(x|y)\\), para un valor fijo \\(y\\) de \\(Y\\), est치 definida por:\n\\[f(x|y)=\\dfrac{f(x,y)}{f_Y(y)}\\]\nen donde \\(f_Y(y)\\) es la funci칩n de densidad marginal de \\(Y\\) de manera tal que \\(f_Y(y)>0\\). Es claro ver que bajo independencia de estas variables aleatorias, se tiene:\n\\[f(x|y)=f_X(x)\\]"
  },
  {
    "objectID": "slides/lec_week7.html#distribuciones-de-probabilidad-condicional-continuaci칩n",
    "href": "slides/lec_week7.html#distribuciones-de-probabilidad-condicional-continuaci칩n",
    "title": "Distribuciones continuas",
    "section": "Distribuciones de probabilidad condicional: continuaci칩n",
    "text": "Distribuciones de probabilidad condicional: continuaci칩n\nLos valores esperados se definen de manera an치loga a lo visto anteriormente, esto es:\n\\[\\mathbb{E}(X|y)=\\int_{-\\infty}^{\\infty}xf(x|y)dx\\]\ny,\n\\[\\mathbb{E}(Y|x)=\\int_{-\\infty}^{\\infty}yf(y|x)dy\\]"
  },
  {
    "objectID": "slides/lec_week7.html#ejemplo",
    "href": "slides/lec_week7.html#ejemplo",
    "title": "Distribuciones continuas",
    "section": "Ejemplo",
    "text": "Ejemplo\nLa funci칩n de probabilidad conjunta de dos variables aleatorias discretas \\(X, Y\\) est치 dada por \\(f(x,y) = c(2x+y)\\), donde \\(x,y\\) pueden tomar todos los valores enteros tales que \\(0\\leq x \\leq 2,0\\leq y \\leq 3\\), y \\(f(x,y)=0\\) de otra forma.\n\nHallar el valor de la constante c\nHallar \\(\\mathbb{P}(X=2,Y=1)\\)\nHallar \\(\\mathbb{P}(X\\geq 1, Y\\leq 2)\\)"
  },
  {
    "objectID": "slides/lec_week7.html#resoluci칩n-ejemplo-1",
    "href": "slides/lec_week7.html#resoluci칩n-ejemplo-1",
    "title": "Distribuciones continuas",
    "section": "Resoluci칩n ejemplo",
    "text": "Resoluci칩n ejemplo\nNotamos que las V.A. toman s칩lo los valores entero, por lo que c lo obtenemos como:\n\\[\\begin{align*}\n\\sum_{x=0}^{2}\\sum_{y=0}^{3} c(2x+y)=1\n\\end{align*}\\]\nPodemos resumir los valores que toma la funci칩n de cuant칤a como:\n\nPor lo que \\(c=\\dfrac{1}{42}\\)"
  },
  {
    "objectID": "slides/lec_week7.html#resoluci칩n-ejemplo-continuaci칩n-2",
    "href": "slides/lec_week7.html#resoluci칩n-ejemplo-continuaci칩n-2",
    "title": "Distribuciones continuas",
    "section": "Resoluci칩n ejemplo: continuaci칩n",
    "text": "Resoluci칩n ejemplo: continuaci칩n\nPara el 칤tem 2. \\(\\mathbb{P}(X=2,Y=3)\\) basta notar la celda correspondiente en la tabla construida reemplazando c apropiadamente, por lo que \\(\\mathbb{P}(X=2,Y=3)=7c=\\dfrac{7}{42}\\)\nPara el 칤tem 3. Reconocemos que:\n\\[\\begin{align*}\n\\mathbb{P}(X\\geq 1,Y\\leq 2)&=\\mathbb{P}(X=1,Y=0)+\\mathbb{P}(X=2,Y=0)\\\\\n&+\\mathbb{P}(X=1,Y=1)+\\mathbb{P}(X=2,Y=1)\\\\\n&+\\mathbb{P}(X=1,Y=2)+\\mathbb{P}(X=2,Y=2)\\\\\n&=2c+4c+3c+5c+4c+6c=24c=\\dfrac{24}{42}\n\\end{align*}\\]"
  },
  {
    "objectID": "pages/week8.html",
    "href": "pages/week8.html",
    "title": "Semana 8",
    "section": "",
    "text": "Estudiar pruebas de a침os anteriores\nDesarrollar gu칤a de ejercicios\nLeer cap칤tulo 8, Probability and Statistics for Engineering and the Sciences, 9th Edition."
  },
  {
    "objectID": "slides/lec_week8.html#par치metro-y-espacio-param칠trico",
    "href": "slides/lec_week8.html#par치metro-y-espacio-param칠trico",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Par치metro y espacio param칠trico",
    "text": "Par치metro y espacio param칠trico\n\nPar치metro: Es una caracter칤stica num칠rica de la distribuci칩n de la poblaci칩n, que describe, parcial o completamente, la funci칩n de masa de probabilidad de la caracter칤stica de inter칠s, habitualmente se simboliza por la letra griega \\(\\theta\\).\nEspacio param칠trico: Es el conjunto de posibles valores que puede(n) ser considerado(s) para el(los) par치metro(s). Se simboliza por la letra griega may칰scula \\(\\Theta\\)."
  },
  {
    "objectID": "slides/lec_week8.html#m칠todo-de-m치xima-verosimilitud",
    "href": "slides/lec_week8.html#m칠todo-de-m치xima-verosimilitud",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m치xima verosimilitud",
    "text": "M칠todo de m치xima verosimilitud\nEl m칠todo de m치xima verosimilitud consiste en encontrar el valor(es) del par치metro(s) que maximiza la funci칩n de masa (densidad) de probabilidad conjunta de la muestra, llamada verosimilitud.\n\n\nFunci칩n de verosimilitud: Sean \\(X_1,\\cdots,X_n\\) una muestra aleatoria con funci칩n de masa(densidad) de probabilidad \\(f(X;\\theta)\\) y sea \\(L(\\theta,;X_1,\\cdots,X_n)\\) la verosimilitud de la muestra como funci칩n de \\(\\theta\\), la cual se representa por:\n\n\n\n\\[L(\\theta;x)=L(\\theta,;X_1,\\cdots,X_n)=f(x_1;\\theta)\\times f(x_2;\\theta)\\times \\cdots f(x_n;\\theta)\\]\n\n\nEl m칠todo de m치xima verosimilitud busca \\(\\widehat{\\theta}(x_1,\\cdots,x_n)\\) funci칩n que depende s칩lo de la muestra que maximiza \\(L(\\theta;x)\\). Para obtener estimadores m치ximo veros칤miles se utilizan las herramientas de c치lculo matem치tico, adem치s para simplificar los c치lculos se utiliza el logaritmo de la verosimilitud, llamada funci칩n de logverosimilitud, representado por:\n\\[l(\\theta;x)=\\ln (L(\\theta;x))\\]"
  },
  {
    "objectID": "slides/lec_week8.html#m칠todo-de-m칤nimos-cuadrados",
    "href": "slides/lec_week8.html#m칠todo-de-m칤nimos-cuadrados",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m칤nimos cuadrados",
    "text": "M칠todo de m칤nimos cuadrados\nSupongamos que tenemos \\(\\mathbb{E}(Y)=\\alpha X + \\beta\\), donde \\(\\alpha,\\beta\\) y \\(X\\) son tal como en una regresi칩n lineal simple. Sea \\((x_1,Y_1),\\dots,(x_n,Y_n)\\) una muestra aleatoria de \\(Y\\). Los estimadores m칤nimos cuadrados de los par치metros \\(\\alpha,\\beta\\) son los valores de \\(\\alpha\\) y \\(\\beta\\) que minimizan:\n\\[\\sum_{i=1}^{N} [Y_i - (\\alpha x_i +\\beta)]^2\\]\nPara poder obtener las estimaciones para \\(\\alpha\\) y \\(\\beta\\), procedemos de la siguiente manera:\nSea \\(S(\\alpha,\\beta)=\\sum_{i=1}^{N} [Y_i - (\\alpha x_i +\\beta)]^2\\). Para minimizar \\(S(\\alpha,\\beta)\\), debemos resolver las ecuaciones:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=0 \\hspace{20pt}\\text{y}\\hspace{20pt}\\dfrac{\\partial S}{\\partial \\beta}=0\\]"
  },
  {
    "objectID": "slides/lec_week8.html#m칠todo-de-m칤nimos-cuadrados-desarrollo",
    "href": "slides/lec_week8.html#m칠todo-de-m칤nimos-cuadrados-desarrollo",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m칤nimos cuadrados: desarrollo",
    "text": "M칠todo de m칤nimos cuadrados: desarrollo\nDerivando, obtenemos:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-x_i)=-2\\sum_{i=1}^{n}[x_i Y_i - \\alpha x_{i}^{2} - \\beta x_i]\\]\ny,\n\\[\\dfrac{\\partial S}{\\partial \\beta}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-1)=-2\\sum_{i=1}^{n}[Y_i - \\alpha x_{i} - \\beta]\\]\nLuego, igualando a cero, se tiene que:\n\\[\\alpha\\sum_{i=1}^{n} x_{i}^{2} + \\beta \\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} x_i Y_i \\qquad \\text{y,} \\qquad \\alpha \\sum_{i=1}^{n} x_i + n\\beta=\\sum_{i=1}^{n} Y_i\\]"
  },
  {
    "objectID": "slides/lec_week8.html#m칠todo-de-m칤nimos-cuadrados-desarrollo-1",
    "href": "slides/lec_week8.html#m칠todo-de-m칤nimos-cuadrados-desarrollo-1",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m칤nimos cuadrados: desarrollo",
    "text": "M칠todo de m칤nimos cuadrados: desarrollo\nTenemos dos ecuaciones lineales y dos inc칩gnitas, por lo que podemos obtener soluciones para \\(\\alpha\\) y \\(\\beta\\), as칤:\n\\[\\hat{\\alpha}=\\dfrac{\\sum_{i=1}^{n} Y_i (x_i - \\overline{x})}{\\sum_{i=1}^{n} (x_i-\\overline{x})^2}\\hspace{20pt}\\text{donde}\\hspace{20pt}\\overline{x}=\\dfrac{1}{n}\\sum_{i=1}^{n}x_i\\]\n\\[\\hat{\\beta}=\\overline{Y}-\\hat{\\alpha}\\overline{x}\\hspace{20pt}\\text{donde}\\hspace{20pt}\\overline{Y}=\\dfrac{1}{n}\\sum_{i=1}^{n}Y_i\\]\nEstas soluciones siempre se pueden obtener y son 칰nicas si \\(\\sum_{i=1}^{n}(x_i-\\overline{x})^2\\neq 0\\).\nSin embargo, esta condici칩n se satisface cuando no todos los \\(x_i\\) son iguales. En cuanto a la estimaci칩n de \\(\\sigma^2\\), esta no puede obtenida mediante este m칠todo."
  },
  {
    "objectID": "slides/lec_week8.html#estimadores-insesgados",
    "href": "slides/lec_week8.html#estimadores-insesgados",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Estimadores insesgados",
    "text": "Estimadores insesgados\nConsideramos una muestra aleatoria, \\(X_1,X_2,\\cdots,X_n\\) y \\(T=T(X_1,X_2,\\cdots,X_n)\\) una funci칩n de la muestra, entonces \\(T\\) es llamada un estad칤stica. Cuando una estad칤stica \\(T\\), se utiliza con fines de estimaci칩n recibe el nombre de estimador. En general, se desea que los estimadores tengan algunas propiedades especiales.\n\nEstimadores Insesgados: Sea \\(T\\) un estimador (estad칤stica) de un par치metro \\(\\theta\\), se dice que \\(T\\) es un estimador insesgado (o libre de sesgo), si \\(E[T]=\\theta\\), para todos los posibles valores de \\(\\theta\\).\n\n\nEn otras palabras, lo que se desea es que el estimador \\(T\\), en promedio (promediando sobre todas las posibles muestras), sea igual a \\(\\theta\\), 랇o que se desea estimar, bajo la hip칩tesis que la distribuci칩n de probabilidad de la poblaci칩n propuesta es correcta."
  },
  {
    "objectID": "slides/lec_week8.html#error-cuadr치tico-medio",
    "href": "slides/lec_week8.html#error-cuadr치tico-medio",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Error cuadr치tico medio",
    "text": "Error cuadr치tico medio\nSea \\(T\\) un estimador de un par치metro \\(\\theta\\), se define el error cuadr치tico medio de \\(T\\), como el valor esperado del cuadrado de la diferencia entre \\(T\\) y \\(\\theta\\), y se anota \\(ECM(T)\\), esto es:\n\\[ECM(T)=E[(T-\\theta)^2]\\]\nSi de desarrolla la expresi칩n, podemos reescribir lo anterior de la forma:\n\\[ECM(T)=V[T]+(E[T]-\\theta)^2\\]\nEl error cuadr치tico medio de un estimador \\(T\\), es la suma de dos cantidades no negativas: una es la varianza del estimador, mientras que la otra es el sesgo al cuadrado.\nUn criterio para seleccionar un estimador, es que posea el ECM m치s peque침o entre los posibles estimadores de \\(\\theta\\)."
  },
  {
    "objectID": "slides/lec_week8.html#eficiencia-relativa",
    "href": "slides/lec_week8.html#eficiencia-relativa",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Eficiencia relativa",
    "text": "Eficiencia relativa\nSean \\(T_1\\) y \\(T_2\\) dos estimadores de \\(\\theta\\). Se define la eficiencia relativa entre \\(T_1\\) y \\(T_2\\) como:\n\\[Ef(T_1;T_2)=\\dfrac{ECM(T_1)}{ECM(T_2)}\\]\nSi la eficiencia relativa es menor que uno, se concluye que el estimador \\(T_1\\) es m치s eficiente que el estimador \\(T_2\\), en caso contrario, se concluye que el estimador \\(T_1\\) es m치s eficiente que el estimador \\(T_2\\)."
  },
  {
    "objectID": "slides/lec_week8.html#consistencia",
    "href": "slides/lec_week8.html#consistencia",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Consistencia",
    "text": "Consistencia\nLa consistencia mide la capacidad del estimador de acercarse cada vez m치s al verdadero valor del par치metro, a medida que el tama침o de muestra crece.\n\\[ T_n \\overset{p}{\\to} \\theta\\]\n\nConsistencia en media cuadr치tica:\n\n\nUn estimador \\(T\\), de un par치metro desconocido \\(\\theta\\), se dice consistente en media cuadr치tica, si se cumple:\n\\[\\lim_{n\\rightarrow\\infty} ECM(T_n)=0\\]"
  },
  {
    "objectID": "slides/lec_week8.html#definici칩n-intervalo-de-confianza",
    "href": "slides/lec_week8.html#definici칩n-intervalo-de-confianza",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Definici칩n intervalo de confianza",
    "text": "Definici칩n intervalo de confianza\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria desde \\(f(x;\\theta)\\), donde \\(f(x;\\theta)\\) es una funci칩n de masa (densidad) de probabilidades dependiendo de un par치metro desconocido \\(\\theta\\). Sean \\(T_1\\) y \\(T_2\\) dos estad칤sticos tales que \\(T_1(x)<T_2(x)\\) para casi todo \\(x\\) y\n\\[\\mathbb{P}(T_1\\leq\\theta \\leq T_2)=\\gamma,\\]\ndonde \\(\\gamma\\) no depende de \\(\\theta\\). Se dice que \\([T_1,T_2]\\) es un intervalo de confianza para \\(\\theta\\) con \\(100\\gamma \\%\\) de confianza.\n\n\\(T_1\\) y \\(T_2\\) reciben el nombre de cota inferior y superior de confianza, respectivamente.\n\\(\\gamma\\) recibe el nombre de coeficiente de confianza.\n\n\\([T_1,T_2]\\) es un intervalo aleatorio, ya que sus extremos son variables aleatorias."
  },
  {
    "objectID": "slides/lec_week8.html#cantidad-pivotal",
    "href": "slides/lec_week8.html#cantidad-pivotal",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Cantidad pivotal",
    "text": "Cantidad pivotal\nExisten t칠cnicas para construir intervalos (regiones) de confianza, y una de ellas es la del pivote.\n\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) desde \\(f(x;\\theta)\\) y \\(Q=Q(X_1,X_2,\\cdots,X_n)\\). Si la distribuci칩n de \\(Q\\) es independiente de \\(\\theta\\), se dice que Q es una cantidad pivotal.\n\n\nEjemplo\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) desde una familia normal \\(F_{N}(\\mu,\\sigma^2)\\) con media \\(\\mu\\) y varianza conocida \\(\\sigma^2\\), luego:\n\\[Q=\\overline{X}-\\mu \\rightarrow Q \\approx N\\left(0,\\dfrac{\\sigma^2}{n}\\right)\\]"
  },
  {
    "objectID": "slides/lec_week8.html#intervalo-de-confianza-para-la-media-poblacional",
    "href": "slides/lec_week8.html#intervalo-de-confianza-para-la-media-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Intervalo de confianza para la media poblacional",
    "text": "Intervalo de confianza para la media poblacional\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) de una familia normal \\(F_{N}(\\mu,\\sigma^2)\\), como \\(\\overline{X}\\) es el mejor estimador de \\(\\mu\\), entonces si se conoce \\(\\sigma^2\\), se tiene que:\n\\[Z=\\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\approx N(0,1) \\Rightarrow Z \\text{ pivote}\\]\nLuego dado \\(\\gamma\\), se requiere determinar los valores m치s apropiados de \\(q_1\\) y \\(q_2\\) que cumplan con:\n\\[\\mathbb{P}\\left(q_1 \\leq \\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\leq q_2\\right)=\\gamma\\]\n\nSe desea minimizar la longitud del intervalo de confianza, los valores \\(q_1\\) y \\(q_2\\) deben ser aquellos que produzcan igualdad de probabilidades en las colas."
  },
  {
    "objectID": "slides/lec_week8.html#desarrollo-intervalo-de-confianza",
    "href": "slides/lec_week8.html#desarrollo-intervalo-de-confianza",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Desarrollo intervalo de confianza",
    "text": "Desarrollo intervalo de confianza\nEsto es:\n\\[q_2=Z_{\\dfrac{1+\\gamma}{2}} \\hspace{30pt} q_1=-q_2\\]\nLuego, si tomamos \\(\\alpha=1-\\gamma\\), se tiene:\n\\[\\mathbb{P}\\left( Z_{\\alpha /2} \\leq \\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\leq Z_{1-\\alpha/2} \\right)=1-\\alpha\\]"
  },
  {
    "objectID": "slides/lec_week8.html#i.c.-para-la-media-con-varianza-poblaci칩n-conocida",
    "href": "slides/lec_week8.html#i.c.-para-la-media-con-varianza-poblaci칩n-conocida",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. para la media con varianza poblaci칩n conocida",
    "text": "I.C. para la media con varianza poblaci칩n conocida\nDe la probabilidad del pivote, podemos despejar nuestro par치metro de inter칠s \\(\\mu\\) obteniendo:\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}-Z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1- \\alpha\\]\nPero como \\(Z_{\\alpha/2}=-Z_{1-\\alpha/2}\\)\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}+Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1-\\alpha\\]\nCon lo anterior se concluye que el intervalo de \\((1-\\alpha)\\%\\) de confianza para la media poblacional est치 dado por:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week8.html#i.c.-para-la-media-con-varianza-poblaci칩n-desconocida",
    "href": "slides/lec_week8.html#i.c.-para-la-media-con-varianza-poblaci칩n-desconocida",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. para la media con varianza poblaci칩n desconocida",
    "text": "I.C. para la media con varianza poblaci칩n desconocida\nSi se tiene una muestra aleatoria de tama침o \\(n\\), \\(X_1,X_2,\\cdots,X_n\\) tal que \\(X_i \\approx N(\\mu,\\sigma^2)\\), con varianza poblacional \\(\\sigma^2\\) desconocida, como sabemos que \\(S^2\\) es el mejor estimador de \\(\\sigma^2\\), se tiene:\n\\[T=\\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{s} \\approx \\mathcal{T}(n-1) \\Rightarrow T \\text{ pivote}\\]\nEn donde \\(\\mathcal{T}\\) es la distribuci칩n t-student con \\((n-1)\\) grados de libertad. An치logamente, podemos construir el intervalo de confianza para \\(\\mu\\) utilizando esta distribuci칩n, obteni칠ndose:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp t_{1-\\alpha/2}(n-1)\\dfrac{s}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week8.html#i.c.-para-la-media-con-tama침o-de-muestra-grande",
    "href": "slides/lec_week8.html#i.c.-para-la-media-con-tama침o-de-muestra-grande",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. para la media con tama침o de muestra grande",
    "text": "I.C. para la media con tama침o de muestra grande\nSi el tama침o de muestra es muy grande (mayor que 50), utilizando el teorema de l칤mite central, el intervalo de confianza toma la siguiente forma:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{s}{\\sqrt{n}}\\right]\\]\nNotamos que es importante distinguir cuando la varianza poblacional es conocida o desconocida. Si a partir de la muestra aleatoria se determina una varianza, 칠sta es la muestral, por lo tanto, lo correcto es utilizar un intervalo de confianza considerando la distribuci칩n t-student, caso contrario si la muestra es superior a 50, entonces empleamos el teorema de l칤mite central para aproximar por distribuci칩n normal."
  },
  {
    "objectID": "slides/lec_week8.html#intervalos-de-confianza-para-una-proporci칩n",
    "href": "slides/lec_week8.html#intervalos-de-confianza-para-una-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Intervalos de confianza para una proporci칩n",
    "text": "Intervalos de confianza para una proporci칩n\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria de tama침o \\(n\\) de una familia binomial \\(\\mathcal{B} (1,p)\\). El estimador de \\(p\\) sobre la base de la muestra es \\(\\widehat{P}=\\overline{X}\\). La distribuci칩n de \\(\\widehat{P}=\\overline{X}\\), para muestras grandes, se puede aproximar mediante una distribuci칩n normal de par치metros \\(p\\) y \\(\\dfrac{p(1-p)}{n}\\). Con esto podemos aproximar la siguiente cantidad pivotal:\n\\[Z=\\dfrac{(\\widehat{P}-p)}{\\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}}} \\approx N(0,1) \\Rightarrow Z \\text{ pivote}\\]\nLuego dado \\((1-\\alpha)\\), los valores de \\(q_1\\) y \\(q_2\\) que minimizan la longitud del intervalo ser치n:\n\\[\\mathbb{P}\\left(  \\widehat{P}-Z_{1-\\alpha /2} \\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}} \\leq p \\leq \\widehat{P}+Z_{1-\\alpha /2} \\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P}) }{n} } \\right)=\\gamma \\]"
  },
  {
    "objectID": "slides/lec_week8.html#i.c.-final-para-una-proporci칩n",
    "href": "slides/lec_week8.html#i.c.-final-para-una-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. final para una proporci칩n",
    "text": "I.C. final para una proporci칩n\nLuego, el intervalo de confianza, del \\((100*\\gamma)\\%\\) para la proporci칩n es:\n\\[IC(p):=\\left[ \\widehat{P}\\mp Z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}}\\right] \\]\nSe puede apreciar que los intervalos de confianza anteriores est치n compuestos por un estimador puntual, m치s o menos una cantidad, 칠sta cantidad recibe el nombre de error de estimaci칩n, que resultar치 칰til para determinar el tama침os de muestra."
  },
  {
    "objectID": "slides/lec_week8.html#intervalos-de-confianza-para-la-varianza-poblacional",
    "href": "slides/lec_week8.html#intervalos-de-confianza-para-la-varianza-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Intervalos de confianza para la varianza poblacional",
    "text": "Intervalos de confianza para la varianza poblacional\nSea \\(X_1,X_2,\\dots,X_n\\) una muestra aleatoria de tama침o \\(n\\) desde una familia normal \\(F_N(\\mu,\\sigma^2)\\). Existen dos posibilidades para la estimaci칩n de la varianza, cuando la media poblaci칩n es conocida (caso no pr치ctico) y cuando 칠sta es desconocida. Para ambos casos podemos definir cantidades pivotales:\n\n\\(\\dfrac{n S_{n}^{2}}{\\sigma^2} \\sim \\chi^2(n)\\)\n\n\\(\\dfrac{(n-1) S_{n-1}^{2}}{\\sigma^2} \\sim \\chi^2(n-1)\\)\n\n\nen donde:\n\\[S_{n}^{2}=\\sum_{i=1}^{n} \\dfrac{(X_i - \\mu)^2}{n},\\qquad S_{n-1}^{2}=\\sum_{i=1}^{n} \\dfrac{(X_i - \\overline{X})^2}{n-1}\\]"
  },
  {
    "objectID": "slides/lec_week8.html#i.c.-final-para-la-varianza-poblacional",
    "href": "slides/lec_week8.html#i.c.-final-para-la-varianza-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. final para la varianza poblacional",
    "text": "I.C. final para la varianza poblacional\nSiguiendo el mismo procedimiento para la cantidad pivotal, en particular, el caso donde la media poblacional es desconocida. Se tiene:\n\\[\\mathbb{P}\\left[ \\chi_{\\alpha/2}^{2}(n-1) \\leq \\dfrac{(n-1) S_{n-1}^{2}}{\\sigma^2} \\leq \\chi_{1-\\alpha/2}^{2}(n-1) \\right]=1-\\alpha\\]\nLuego, despejando el par치metro de inter칠s \\(\\sigma^2\\), podemos definir un intervalo de \\((1-\\alpha)\\%\\) de confianza para la varianza poblacional:\n\\[IC(\\sigma^2)=\\left[ \\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{1-\\alpha/2}^{2}(n-1)};\\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{\\alpha/2}^{2}(n-1)}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week9.html#par치metro-y-espacio-param칠trico",
    "href": "slides/lec_week9.html#par치metro-y-espacio-param칠trico",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Par치metro y espacio param칠trico",
    "text": "Par치metro y espacio param칠trico\n\nPar치metro: Es una caracter칤stica num칠rica de la distribuci칩n de la poblaci칩n, que describe, parcial o completamente, la funci칩n de masa de probabilidad de la caracter칤stica de inter칠s, habitualmente se simboliza por la letra griega \\(\\theta\\).\nEspacio param칠trico: Es el conjunto de posibles valores que puede(n) ser considerado(s) para el(los) par치metro(s). Se simboliza por la letra griega may칰scula \\(\\Theta\\)."
  },
  {
    "objectID": "slides/lec_week9.html#m칠todo-de-m치xima-verosimilitud",
    "href": "slides/lec_week9.html#m칠todo-de-m치xima-verosimilitud",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m치xima verosimilitud",
    "text": "M칠todo de m치xima verosimilitud\nEl m칠todo de m치xima verosimilitud consiste en encontrar el valor(es) del par치metro(s) que maximiza la funci칩n de masa (densidad) de probabilidad conjunta de la muestra, llamada verosimilitud.\n\n\nFunci칩n de verosimilitud: Sean \\(X_1,\\cdots,X_n\\) una muestra aleatoria con funci칩n de masa(densidad) de probabilidad \\(f(X;\\theta)\\) y sea \\(L(\\theta,;X_1,\\cdots,X_n)\\) la verosimilitud de la muestra como funci칩n de \\(\\theta\\), la cual se representa por:\n\n\n\n\\[L(\\theta;x)=L(\\theta,;X_1,\\cdots,X_n)=f(x_1;\\theta)\\times f(x_2;\\theta)\\times \\cdots f(x_n;\\theta)\\]\n\n\nEl m칠todo de m치xima verosimilitud busca \\(\\widehat{\\theta}(x_1,\\cdots,x_n)\\) funci칩n que depende s칩lo de la muestra que maximiza \\(L(\\theta;x)\\). Para obtener estimadores m치ximo veros칤miles se utilizan las herramientas de c치lculo matem치tico, adem치s para simplificar los c치lculos se utiliza el logaritmo de la verosimilitud, llamada funci칩n de logverosimilitud, representado por:\n\\[l(\\theta;x)=\\ln (L(\\theta;x))\\]"
  },
  {
    "objectID": "slides/lec_week9.html#m칠todo-de-m칤nimos-cuadrados",
    "href": "slides/lec_week9.html#m칠todo-de-m칤nimos-cuadrados",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m칤nimos cuadrados",
    "text": "M칠todo de m칤nimos cuadrados\nSupongamos que tenemos \\(\\mathbb{E}(Y)=\\alpha X + \\beta\\), donde \\(\\alpha,\\beta\\) y \\(X\\) son tal como en una regresi칩n lineal simple. Sea \\((x_1,Y_1),\\dots,(x_n,Y_n)\\) una muestra aleatoria de \\(Y\\). Los estimadores m칤nimos cuadrados de los par치metros \\(\\alpha,\\beta\\) son los valores de \\(\\alpha\\) y \\(\\beta\\) que minimizan:\n\\[\\sum_{i=1}^{N} [Y_i - (\\alpha x_i +\\beta)]^2\\]\nPara poder obtener las estimaciones para \\(\\alpha\\) y \\(\\beta\\), procedemos de la siguiente manera:\nSea \\(S(\\alpha,\\beta)=\\sum_{i=1}^{N} [Y_i - (\\alpha x_i +\\beta)]^2\\). Para minimizar \\(S(\\alpha,\\beta)\\), debemos resolver las ecuaciones:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=0 \\hspace{20pt}\\text{y}\\hspace{20pt}\\dfrac{\\partial S}{\\partial \\beta}=0\\]"
  },
  {
    "objectID": "slides/lec_week9.html#m칠todo-de-m칤nimos-cuadrados-desarrollo",
    "href": "slides/lec_week9.html#m칠todo-de-m칤nimos-cuadrados-desarrollo",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m칤nimos cuadrados: desarrollo",
    "text": "M칠todo de m칤nimos cuadrados: desarrollo\nDerivando, obtenemos:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-x_i)=-2\\sum_{i=1}^{n}[x_i Y_i - \\alpha x_{i}^{2} - \\beta x_i]\\]\ny,\n\\[\\dfrac{\\partial S}{\\partial \\beta}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-1)=-2\\sum_{i=1}^{n}[Y_i - \\alpha x_{i} - \\beta]\\]\nLuego, igualando a cero, se tiene que:\n\\[\\alpha\\sum_{i=1}^{n} x_{i}^{2} + \\beta \\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} x_i Y_i \\qquad \\text{y,} \\qquad \\alpha \\sum_{i=1}^{n} x_i + n\\beta=\\sum_{i=1}^{n} Y_i\\]"
  },
  {
    "objectID": "slides/lec_week9.html#m칠todo-de-m칤nimos-cuadrados-desarrollo-1",
    "href": "slides/lec_week9.html#m칠todo-de-m칤nimos-cuadrados-desarrollo-1",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "M칠todo de m칤nimos cuadrados: desarrollo",
    "text": "M칠todo de m칤nimos cuadrados: desarrollo\nTenemos dos ecuaciones lineales y dos inc칩gnitas, por lo que podemos obtener soluciones para \\(\\alpha\\) y \\(\\beta\\), as칤:\n\\[\\hat{\\alpha}=\\dfrac{\\sum_{i=1}^{n} Y_i (x_i - \\overline{x})}{\\sum_{i=1}^{n} (x_i-\\overline{x})^2}\\hspace{20pt}\\text{donde}\\hspace{20pt}\\overline{x}=\\dfrac{1}{n}\\sum_{i=1}^{n}x_i\\]\n\\[\\hat{\\beta}=\\overline{Y}-\\hat{\\alpha}\\overline{x}\\hspace{20pt}\\text{donde}\\hspace{20pt}\\overline{Y}=\\dfrac{1}{n}\\sum_{i=1}^{n}Y_i\\]\nEstas soluciones siempre se pueden obtener y son 칰nicas si \\(\\sum_{i=1}^{n}(x_i-\\overline{x})^2\\neq 0\\).\nSin embargo, esta condici칩n se satisface cuando no todos los \\(x_i\\) son iguales. En cuanto a la estimaci칩n de \\(\\sigma^2\\), esta no puede obtenida mediante este m칠todo."
  },
  {
    "objectID": "slides/lec_week9.html#estimadores-insesgados",
    "href": "slides/lec_week9.html#estimadores-insesgados",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Estimadores insesgados",
    "text": "Estimadores insesgados\nConsideramos una muestra aleatoria, \\(X_1,X_2,\\cdots,X_n\\) y \\(T=T(X_1,X_2,\\cdots,X_n)\\) una funci칩n de la muestra, entonces \\(T\\) es llamada un estad칤stica. Cuando una estad칤stica \\(T\\), se utiliza con fines de estimaci칩n recibe el nombre de estimador. En general, se desea que los estimadores tengan algunas propiedades especiales.\n\nEstimadores Insesgados: Sea \\(T\\) un estimador (estad칤stica) de un par치metro \\(\\theta\\), se dice que \\(T\\) es un estimador insesgado (o libre de sesgo), si \\(E[T]=\\theta\\), para todos los posibles valores de \\(\\theta\\).\n\n\nEn otras palabras, lo que se desea es que el estimador \\(T\\), en promedio (promediando sobre todas las posibles muestras), sea igual a \\(\\theta\\), 랇o que se desea estimar, bajo la hip칩tesis que la distribuci칩n de probabilidad de la poblaci칩n propuesta es correcta."
  },
  {
    "objectID": "slides/lec_week9.html#error-cuadr치tico-medio",
    "href": "slides/lec_week9.html#error-cuadr치tico-medio",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Error cuadr치tico medio",
    "text": "Error cuadr치tico medio\nSea \\(T\\) un estimador de un par치metro \\(\\theta\\), se define el error cuadr치tico medio de \\(T\\), como el valor esperado del cuadrado de la diferencia entre \\(T\\) y \\(\\theta\\), y se anota \\(ECM(T)\\), esto es:\n\\[ECM(T)=E[(T-\\theta)^2]\\]\nSi de desarrolla la expresi칩n, podemos reescribir lo anterior de la forma:\n\\[ECM(T)=V[T]+(E[T]-\\theta)^2\\]\nEl error cuadr치tico medio de un estimador \\(T\\), es la suma de dos cantidades no negativas: una es la varianza del estimador, mientras que la otra es el sesgo al cuadrado.\nUn criterio para seleccionar un estimador, es que posea el ECM m치s peque침o entre los posibles estimadores de \\(\\theta\\)."
  },
  {
    "objectID": "slides/lec_week9.html#eficiencia-relativa",
    "href": "slides/lec_week9.html#eficiencia-relativa",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Eficiencia relativa",
    "text": "Eficiencia relativa\nSean \\(T_1\\) y \\(T_2\\) dos estimadores de \\(\\theta\\). Se define la eficiencia relativa entre \\(T_1\\) y \\(T_2\\) como:\n\\[Ef(T_1;T_2)=\\dfrac{ECM(T_1)}{ECM(T_2)}\\]\nSi la eficiencia relativa es menor que uno, se concluye que el estimador \\(T_1\\) es m치s eficiente que el estimador \\(T_2\\), en caso contrario, se concluye que el estimador \\(T_1\\) es m치s eficiente que el estimador \\(T_2\\)."
  },
  {
    "objectID": "slides/lec_week9.html#consistencia",
    "href": "slides/lec_week9.html#consistencia",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Consistencia",
    "text": "Consistencia\nLa consistencia mide la capacidad del estimador de acercarse cada vez m치s al verdadero valor del par치metro, a medida que el tama침o de muestra crece.\n\\[ T_n \\overset{p}{\\to} \\theta\\]\n\nConsistencia en media cuadr치tica:\n\n\nUn estimador \\(T\\), de un par치metro desconocido \\(\\theta\\), se dice consistente en media cuadr치tica, si se cumple:\n\\[\\lim_{n\\rightarrow\\infty} ECM(T_n)=0\\]"
  },
  {
    "objectID": "slides/lec_week9.html#definici칩n-intervalo-de-confianza",
    "href": "slides/lec_week9.html#definici칩n-intervalo-de-confianza",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Definici칩n intervalo de confianza",
    "text": "Definici칩n intervalo de confianza\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria desde \\(f(x;\\theta)\\), donde \\(f(x;\\theta)\\) es una funci칩n de masa (densidad) de probabilidades dependiendo de un par치metro desconocido \\(\\theta\\). Sean \\(T_1\\) y \\(T_2\\) dos estad칤sticos tales que \\(T_1(x)<T_2(x)\\) para casi todo \\(x\\) y\n\\[\\mathbb{P}(T_1\\leq\\theta \\leq T_2)=\\gamma,\\]\ndonde \\(\\gamma\\) no depende de \\(\\theta\\). Se dice que \\([T_1,T_2]\\) es un intervalo de confianza para \\(\\theta\\) con \\(100\\gamma \\%\\) de confianza.\n\n\\(T_1\\) y \\(T_2\\) reciben el nombre de cota inferior y superior de confianza, respectivamente.\n\\(\\gamma\\) recibe el nombre de coeficiente de confianza.\n\n\\([T_1,T_2]\\) es un intervalo aleatorio, ya que sus extremos son variables aleatorias."
  },
  {
    "objectID": "slides/lec_week9.html#cantidad-pivotal",
    "href": "slides/lec_week9.html#cantidad-pivotal",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Cantidad pivotal",
    "text": "Cantidad pivotal\nExisten t칠cnicas para construir intervalos (regiones) de confianza, y una de ellas es la del pivote.\n\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) desde \\(f(x;\\theta)\\) y \\(Q=Q(X_1,X_2,\\cdots,X_n)\\). Si la distribuci칩n de \\(Q\\) es independiente de \\(\\theta\\), se dice que Q es una cantidad pivotal.\n\n\nEjemplo\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) desde una familia normal \\(F_{N}(\\mu,\\sigma^2)\\) con media \\(\\mu\\) y varianza conocida \\(\\sigma^2\\), luego:\n\\[Q=\\overline{X}-\\mu \\rightarrow Q \\approx N\\left(0,\\dfrac{\\sigma^2}{n}\\right)\\]"
  },
  {
    "objectID": "slides/lec_week9.html#intervalo-de-confianza-para-la-media-poblacional",
    "href": "slides/lec_week9.html#intervalo-de-confianza-para-la-media-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Intervalo de confianza para la media poblacional",
    "text": "Intervalo de confianza para la media poblacional\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) de una familia normal \\(F_{N}(\\mu,\\sigma^2)\\), como \\(\\overline{X}\\) es el mejor estimador de \\(\\mu\\), entonces si se conoce \\(\\sigma^2\\), se tiene que:\n\\[Z=\\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\approx N(0,1) \\Rightarrow Z \\text{ pivote}\\]\nLuego dado \\(\\gamma\\), se requiere determinar los valores m치s apropiados de \\(q_1\\) y \\(q_2\\) que cumplan con:\n\\[\\mathbb{P}\\left(q_1 \\leq \\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\leq q_2\\right)=\\gamma\\]\n\nSe desea minimizar la longitud del intervalo de confianza, los valores \\(q_1\\) y \\(q_2\\) deben ser aquellos que produzcan igualdad de probabilidades en las colas."
  },
  {
    "objectID": "slides/lec_week9.html#desarrollo-intervalo-de-confianza",
    "href": "slides/lec_week9.html#desarrollo-intervalo-de-confianza",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Desarrollo intervalo de confianza",
    "text": "Desarrollo intervalo de confianza\nEsto es:\n\\[q_2=Z_{\\dfrac{1+\\gamma}{2}} \\hspace{30pt} q_1=-q_2\\]\nLuego, si tomamos \\(\\alpha=1-\\gamma\\), se tiene:\n\\[\\mathbb{P}\\left( Z_{\\alpha /2} \\leq \\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\leq Z_{1-\\alpha/2} \\right)=1-\\alpha\\]"
  },
  {
    "objectID": "slides/lec_week9.html#i.c.-para-la-media-con-varianza-poblaci칩n-conocida",
    "href": "slides/lec_week9.html#i.c.-para-la-media-con-varianza-poblaci칩n-conocida",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. para la media con varianza poblaci칩n conocida",
    "text": "I.C. para la media con varianza poblaci칩n conocida\nDe la probabilidad del pivote, podemos despejar nuestro par치metro de inter칠s \\(\\mu\\) obteniendo:\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}-Z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1- \\alpha\\]\nPero como \\(Z_{\\alpha/2}=-Z_{1-\\alpha/2}\\)\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}+Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1-\\alpha\\]\nCon lo anterior se concluye que el intervalo de \\((1-\\alpha)\\%\\) de confianza para la media poblacional est치 dado por:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week9.html#i.c.-para-la-media-con-varianza-poblaci칩n-desconocida",
    "href": "slides/lec_week9.html#i.c.-para-la-media-con-varianza-poblaci칩n-desconocida",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. para la media con varianza poblaci칩n desconocida",
    "text": "I.C. para la media con varianza poblaci칩n desconocida\nSi se tiene una muestra aleatoria de tama침o \\(n\\), \\(X_1,X_2,\\cdots,X_n\\) tal que \\(X_i \\approx N(\\mu,\\sigma^2)\\), con varianza poblacional \\(\\sigma^2\\) desconocida, como sabemos que \\(S^2\\) es el mejor estimador de \\(\\sigma^2\\), se tiene:\n\\[T=\\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{s} \\approx \\mathcal{T}(n-1) \\Rightarrow T \\text{ pivote}\\]\nEn donde \\(\\mathcal{T}\\) es la distribuci칩n t-student con \\((n-1)\\) grados de libertad. An치logamente, podemos construir el intervalo de confianza para \\(\\mu\\) utilizando esta distribuci칩n, obteni칠ndose:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp t_{1-\\alpha/2}(n-1)\\dfrac{s}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week9.html#i.c.-para-la-media-con-tama침o-de-muestra-grande",
    "href": "slides/lec_week9.html#i.c.-para-la-media-con-tama침o-de-muestra-grande",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. para la media con tama침o de muestra grande",
    "text": "I.C. para la media con tama침o de muestra grande\nSi el tama침o de muestra es muy grande (mayor que 50), utilizando el teorema de l칤mite central, el intervalo de confianza toma la siguiente forma:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{s}{\\sqrt{n}}\\right]\\]\nNotamos que es importante distinguir cuando la varianza poblacional es conocida o desconocida. Si a partir de la muestra aleatoria se determina una varianza, 칠sta es la muestral, por lo tanto, lo correcto es utilizar un intervalo de confianza considerando la distribuci칩n t-student, caso contrario si la muestra es superior a 50, entonces empleamos el teorema de l칤mite central para aproximar por distribuci칩n normal."
  },
  {
    "objectID": "slides/lec_week9.html#intervalos-de-confianza-para-una-proporci칩n",
    "href": "slides/lec_week9.html#intervalos-de-confianza-para-una-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Intervalos de confianza para una proporci칩n",
    "text": "Intervalos de confianza para una proporci칩n\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria de tama침o \\(n\\) de una familia binomial \\(\\mathcal{B} (1,p)\\). El estimador de \\(p\\) sobre la base de la muestra es \\(\\widehat{P}=\\overline{X}\\). La distribuci칩n de \\(\\widehat{P}=\\overline{X}\\), para muestras grandes, se puede aproximar mediante una distribuci칩n normal de par치metros \\(p\\) y \\(\\dfrac{p(1-p)}{n}\\). Con esto podemos aproximar la siguiente cantidad pivotal:\n\\[Z=\\dfrac{(\\widehat{P}-p)}{\\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}}} \\approx N(0,1) \\Rightarrow Z \\text{ pivote}\\]\nLuego dado \\((1-\\alpha)\\), los valores de \\(q_1\\) y \\(q_2\\) que minimizan la longitud del intervalo ser치n:\n\\[\\mathbb{P}\\left(  \\widehat{P}-Z_{1-\\alpha /2} \\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}} \\leq p \\leq \\widehat{P}+Z_{1-\\alpha /2} \\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P}) }{n} } \\right)=\\gamma \\]"
  },
  {
    "objectID": "slides/lec_week9.html#i.c.-final-para-una-proporci칩n",
    "href": "slides/lec_week9.html#i.c.-final-para-una-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. final para una proporci칩n",
    "text": "I.C. final para una proporci칩n\nLuego, el intervalo de confianza, del \\((100*\\gamma)\\%\\) para la proporci칩n es:\n\\[IC(p):=\\left[ \\widehat{P}\\mp Z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}}\\right] \\]\nSe puede apreciar que los intervalos de confianza anteriores est치n compuestos por un estimador puntual, m치s o menos una cantidad, 칠sta cantidad recibe el nombre de error de estimaci칩n, que resultar치 칰til para determinar el tama침os de muestra."
  },
  {
    "objectID": "slides/lec_week9.html#intervalos-de-confianza-para-la-varianza-poblacional",
    "href": "slides/lec_week9.html#intervalos-de-confianza-para-la-varianza-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Intervalos de confianza para la varianza poblacional",
    "text": "Intervalos de confianza para la varianza poblacional\nSea \\(X_1,X_2,\\dots,X_n\\) una muestra aleatoria de tama침o \\(n\\) desde una familia normal \\(F_N(\\mu,\\sigma^2)\\). Existen dos posibilidades para la estimaci칩n de la varianza, cuando la media poblaci칩n es conocida (caso no pr치ctico) y cuando 칠sta es desconocida. Para ambos casos podemos definir cantidades pivotales:\n\n\\(\\dfrac{n S_{n}^{2}}{\\sigma^2} \\sim \\chi^2(n)\\)\n\n\\(\\dfrac{(n-1) S_{n-1}^{2}}{\\sigma^2} \\sim \\chi^2(n-1)\\)\n\n\nen donde:\n\\[S_{n}^{2}=\\sum_{i=1}^{n} \\dfrac{(X_i - \\mu)^2}{n},\\qquad S_{n-1}^{2}=\\sum_{i=1}^{n} \\dfrac{(X_i - \\overline{X})^2}{n-1}\\]"
  },
  {
    "objectID": "slides/lec_week9.html#i.c.-final-para-la-varianza-poblacional",
    "href": "slides/lec_week9.html#i.c.-final-para-la-varianza-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "I.C. final para la varianza poblacional",
    "text": "I.C. final para la varianza poblacional\nSiguiendo el mismo procedimiento para la cantidad pivotal, en particular, el caso donde la media poblacional es desconocida. Se tiene:\n\\[\\mathbb{P}\\left[ \\chi_{\\alpha/2}^{2}(n-1) \\leq \\dfrac{(n-1) S_{n-1}^{2}}{\\sigma^2} \\leq \\chi_{1-\\alpha/2}^{2}(n-1) \\right]=1-\\alpha\\]\nLuego, despejando el par치metro de inter칠s \\(\\sigma^2\\), podemos definir un intervalo de \\((1-\\alpha)\\%\\) de confianza para la varianza poblacional:\n\\[IC(\\sigma^2)=\\left[ \\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{1-\\alpha/2}^{2}(n-1)};\\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{\\alpha/2}^{2}(n-1)}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week9.html#tipos-de-error",
    "href": "slides/lec_week9.html#tipos-de-error",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Tipos de error",
    "text": "Tipos de error\nAl realizar nuestra prueba de hip칩tesis estamos sujetos al estado real de la naturaleza, es decir, la veracidad de nuestra conjetura (\\(H_0\\))\n\n\n\n\n\n\n\n\n\n\n\nEstado Real\nen la naturaleza\n\n\n\n\n\n\n\\(H_0\\) es Verdadera\n\\(H_0\\) es Falsa\n\n\nDecisi칩n\nNo se rechaza \\(H_0\\)\nDecisi칩n Correcta\nError Tipo II\n\n\n\nSe rechaza \\(H_0\\)\nError Tipo I\nDecisi칩n Correcta\n\n\n\nPodemos cometer dos tipos de errores, tipo I y tipo II.\n\nError tipo I: Se comete al rechazar la hip칩tesis nula, cuando corresponde aceptarla por ser 칠sta verdadera. Lo denotamos por \\(\\alpha\\) y es llamado nivel de significaci칩n.\nError tipo II: Se comete al no rechazar la hip칩tesis nula, cuando corresponde rechazarla por ser esta falsa. Lo denotamos por \\(\\beta\\).\n\n\nEl error tipo I es fundamental debido a que es el error que el experimentador controla y pueda manejar."
  },
  {
    "objectID": "slides/lec_week9.html#prueba-para-la-media-poblacional",
    "href": "slides/lec_week9.html#prueba-para-la-media-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Prueba para la media poblacional",
    "text": "Prueba para la media poblacional\nSea \\(X_1,X_2,\\dots,X_n\\) una muestra aleatoria de una distribuci칩n normal con media \\(\\mu\\) desconocida. En este caso el inter칠s recae en probar uno de los siguientes conjuntos de hipotesis con respecto a \\(\\mu\\).\n\\[H_0:\\mu = \\mu_0 \\qquad H_0:\\mu= \\mu_0 \\qquad H_0:\\mu = \\mu_0 \\\\H_1:\\mu \\neq \\mu_0 \\qquad H_1:\\mu > \\mu_0 \\qquad H_1:\\mu < \\mu_0\\]\nSupongamos primero que la varianza poblacional \\(\\sigma^2\\) es conocida. Utilizando la estad칤stica de prueba \\(\\overline{X}\\), bajo \\(H_0\\) se tiene que \\(\\overline{X}\\sim N\\left(\\mu_0,{\\sigma^2 \\over n}\\right)\\). La regi칩n cr칤tica de tama침o \\(\\alpha\\) para la hip칩tesis bilateral es de la forma:\n\\[\\text{Rechazar }H_0 \\text{ si}\\begin{cases} \\overline{X}\\geq \\overline{x}_{1-\\alpha /2}\\\\ \\overline{X}\\leq \\overline{x}_{\\alpha /2} \\end{cases}\\]"
  },
  {
    "objectID": "slides/lec_week9.html#prueba-para-la-media-poblacional-desarrollo",
    "href": "slides/lec_week9.html#prueba-para-la-media-poblacional-desarrollo",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Prueba para la media poblacional: desarrollo",
    "text": "Prueba para la media poblacional: desarrollo\nEn donde \\(\\overline{x}_{1-\\alpha /2}\\) y, \\(\\overline{x}_{\\alpha /2}\\) son los valores cuantiles cr칤ticos de \\(\\overline{X}\\) de manera tal que:\n\\[\\mathbb{P}(\\overline{X}\\geq  \\overline{x}_{1-\\alpha /2})= \\alpha /2 \\hspace{20pt}\\text{y}\\hspace{20pt} \\mathbb{P}(\\overline{X}\\geq  \\overline{x}_{\\alpha /2})= \\alpha /2\\]\nDado que bajo \\(H_0, \\overline{X}\\sim N(\\mu_0,{\\sigma^2 \\over n})\\), entonces de forma equivalente:\n\\[\\mathbb{P}\\left( Z \\geq \\underbrace{\\dfrac{\\overline{x}_{1-\\alpha /2}-\\mu_0}{\\sigma / \\sqrt{n}}}_{z_{1-\\alpha /2}}\\right)=\\alpha /2\\hspace{20pt}\\text{y}\\hspace{20pt}\\mathbb{P}\\left( Z \\leq \\underbrace{\\dfrac{\\overline{x}_{\\alpha /2}-\\mu_0}{\\sigma / \\sqrt{n}}}_{z_{\\alpha /2}}\\right)=\\alpha /2\\]\nPor lo que, \\(H_0\\) debe rechazarse cuando un valor de \\(\\overline{x}\\) de la media muestral \\(\\overline{X}\\) es tal que:\n\\[\\overline{x} \\geq \\dfrac{\\sigma z_{1-\\alpha /2}}{\\sqrt{n}}+\\mu_0\\hspace{20pt}\\text{o}\\hspace{20pt}\\overline{x} \\leq \\dfrac{\\sigma z_{\\alpha /2}}{\\sqrt{n}}+\\mu_0\\]"
  },
  {
    "objectID": "slides/lec_week9.html#prueba-para-la-media-poblacional-regiones-de-rechazo",
    "href": "slides/lec_week9.html#prueba-para-la-media-poblacional-regiones-de-rechazo",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Prueba para la media poblacional: regiones de rechazo",
    "text": "Prueba para la media poblacional: regiones de rechazo\nDe manera equivalente, se rechazar치 \\(H_0\\) cuando,\n\\[z\\geq z_{1-\\alpha /2}\\hspace{20pt}\\text{o}\\hspace{20pt}z\\leq z_{\\alpha /2}\\]\nDonde \\(z=\\dfrac{\\overline{x}-\\mu_0}{\\sigma / \\sqrt{n}}\\) es el valor de la correspondiente normal est치ndar al valor \\(\\overline{x}\\) de \\(\\overline{X}\\).\nPara la hip칩tesis alternativa unilateral, \\(H_1: \\mu > \\mu_0\\), la regi칩n cr칤tica de tama침o \\(\\alpha\\) es el extremo derecho de la distribuci칩n de muestreo de \\(\\overline{X}\\), 칠sta es de la forma:\n\\[\\text{Rechazar } H_0  \\text{ si } \\overline{X} \\geq \\overline{x}_{1-\\alpha}\\]\nDe forma similar, para la hip칩tesis alternativa unilateral \\(H_1:\\mu < \\mu_0\\), la regi칩n cr칤tica es de la forma:\n\\[\\text{Rechazar } H_0 \\text{ si } \\overline{X} \\leq \\overline{x}_{1-\\alpha}\\]"
  },
  {
    "objectID": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-media-con-varianza-conocida",
    "href": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-media-con-varianza-conocida",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Tabla resumen pruebas para la media con varianza conocida",
    "text": "Tabla resumen pruebas para la media con varianza conocida\nLo anterior puede ser resumido en:"
  },
  {
    "objectID": "slides/lec_week9.html#observaciones",
    "href": "slides/lec_week9.html#observaciones",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Observaciones",
    "text": "Observaciones\nNotar que lo anterior, s칩lo fue posible debido a que sab칤amos \\(\\sigma^2\\). En caso de no conocerlo, si utilizamos la misma estad칤stica de prueba \\(\\overline{X}\\), se tiene que:\n\\[T=\\dfrac{\\overline{X}-\\mu_0}{S / \\sqrt{n}} \\sim t(n-1)\\]\nPor lo que siguiendo el mismo procedimiento que antes, podemos llegar a regiones cr칤ticas similares."
  },
  {
    "objectID": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-media-con-varianza-desconocida",
    "href": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-media-con-varianza-desconocida",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Tabla resumen pruebas para la media con varianza desconocida",
    "text": "Tabla resumen pruebas para la media con varianza desconocida\nLo anterior puede ser resumido en:"
  },
  {
    "objectID": "slides/lec_week9.html#pruebas-para-la-varianza-poblacional",
    "href": "slides/lec_week9.html#pruebas-para-la-varianza-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Pruebas para la varianza poblacional",
    "text": "Pruebas para la varianza poblacional\nSea \\(X_1,\\dots,X_n\\) una muestra aleatoria de una distribuci칩n normal con media \\(\\mu\\) desconocida y varianza \\(\\sigma^2\\). Se considera el siguiente test de hip칩tesis:\n\\[H_0:\\sigma^2=\\sigma_{0}^{2}\\]\ncontra una de las siguientes alternativas:\n\\[H_1:\\sigma^2\\neq \\sigma_{0}^{2},\\hspace{5pt}H_1:\\sigma^2> \\sigma_{0}^{2},\\hspace{5pt}H_1:\\sigma^2< \\sigma_{0}^{2}\\]\ndonde \\(\\sigma_{0}^{2}\\) es el valor propuesto para \\(\\sigma^2\\). La estad칤stica de inter칠s es la varianza muestral \\(S^2\\). La hip칩tesis nula ser치 rechazada si la realizaci칩n de \\(s^2\\) calculada a partir de la muestra, es suficientemente diferente, mayor que o menos que \\(\\sigma_{0}^{2}\\), dependiendo de la hip칩tesis alternativa. Bajo \\(H_0\\):\n\\[\\dfrac{(n-1)s^2}{\\sigma_{0}^{2}}\\sim  \\chi^2(n-1)\\]"
  },
  {
    "objectID": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-varianza",
    "href": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-varianza",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Tabla resumen pruebas para la varianza",
    "text": "Tabla resumen pruebas para la varianza\nAs칤, conforme la misma construcci칩n realizada anteriormente, es posible encontrar las criterios de rechazo, en resumen:"
  },
  {
    "objectID": "slides/lec_week9.html#pruebas-para-la-proporci칩n-poblacional",
    "href": "slides/lec_week9.html#pruebas-para-la-proporci칩n-poblacional",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Pruebas para la proporci칩n poblacional",
    "text": "Pruebas para la proporci칩n poblacional\nSea \\(X_1,\\dots,X_n\\) una muestra aleatoria de una distribuci칩n Bernoulli \\(Ber(1,p)\\). Consideramos el siguiente test de hip칩tesis:\n\\[H_0:p=p_0\\]\ncontra una de las siguientes alternativas:\n\\[H_1:p\\neq p_0,\\hspace{5pt}H_1:p> p_0,\\hspace{5pt}H_1:p<p_0\\]\ndonde \\(p_0\\) es el valor propuesto para \\(p\\). La estad칤stica de inter칠s bajo \\(H_0\\) es:\n\\[E=\\dfrac{\\hat{p}-p_0}{\\sqrt{\\dfrac{p_0(1-p_0)}{n}}}\\sim N(0,1)\\]\nPara \\(n>>50\\) y \\(\\hat{p}=\\sum_{i=1}^{n}X_i/n\\)"
  },
  {
    "objectID": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-proporci칩n",
    "href": "slides/lec_week9.html#tabla-resumen-pruebas-para-la-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Tabla resumen pruebas para la proporci칩n",
    "text": "Tabla resumen pruebas para la proporci칩n\nAs칤, conforme la misma construcci칩n realizada anteriormente, es posible enconrtar los criterios de rechazo, en resumen:"
  },
  {
    "objectID": "slides/lec_week9.html#definici칩n-formal",
    "href": "slides/lec_week9.html#definici칩n-formal",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Definici칩n formal",
    "text": "Definici칩n formal\nUn modelo de regresi칩n b치sico donde s칩lo hay una variable predictora y la funci칩n de regresi칩n es lineal se define como:\n\\[Y_i=\\beta_0+\\beta_1 X_i + \\varepsilon_i\\]\ndonde,\n\n\\(Y_i\\) es el valor de la varible respuesta en la i-칠sima observaci칩n\n\\(\\beta_0\\) y \\(\\beta_1\\) son par치metros\n\\(X_i\\) es una constante conocida: el valor de la variable predictora en la i-칠sima observaci칩n.\n\\(\\varepsilon_i\\) es un t칠rmino de error aleatorio con meadia \\(\\mathbb{E}(\\varepsilon_i)=0\\) y varianza \\(\\mathbb{V}(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) no est치n correlacionados, por lo que su covarianza es cero.\n\n\nEsto modelo se le conoce como modelo de regresi칩n lineal simple."
  },
  {
    "objectID": "slides/lec_week9.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple",
    "href": "slides/lec_week9.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple",
    "text": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple\nLa respuesta \\(Y_i\\) en la i-칠simo ensayo es la suma de dos componentes: - El t칠rmino constante \\(\\beta_0+\\beta_1 X_i\\) y, - El t칠rmino aleatorio \\(\\varepsilon_i\\). Por lo que \\(Y_i\\) es una variable aleatoria\nDebido a que \\(\\mathbb{E}(\\varepsilon_i)=0\\), sigue que:\n\\[\\mathbb{E}(Y_i)=\\mathbb{E}(\\beta_0+\\beta_1 X_i + \\varepsilon_i)=\\beta_0+\\beta_1 X_i + \\mathbb{E}(\\varepsilon_i)= \\beta_0+\\beta_1 X_i\\]\nAs칤, la respuesta \\(Y_i\\), cuando el nivel de \\(X\\) en el i-칠simo ensayo es \\(X_i\\), viene desde una distribuci칩n de probabilidad cuya media est치 dada por:\n\\[\\mathbb{E}(Y_i)=\\beta_0+\\beta_1 X_i\\]\nSabremos que la funci칩n de regresi칩n para este modelo es \\(\\mathbb{E}(Y)=\\beta_0+\\beta_1 X\\)\nDebido a que la funci칩n de regresi칩n relaciona la media de la distribuci칩n de probabilidad de \\(Y\\) para un \\(X\\) dado para el nivel de \\(X\\)."
  },
  {
    "objectID": "slides/lec_week9.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple-continuaci칩n",
    "href": "slides/lec_week9.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple-continuaci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple: continuaci칩n",
    "text": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple: continuaci칩n\nLa respuesta \\(Y_i\\) en el i-칠simo ensayo excede o queda bajo el valor de la funci칩n de regresi칩n por la cantidad del t칠rmino \\(\\varepsilon_i\\). Adem치s, Los errores \\(\\varepsilon_i\\) se asumen que tienen varianza constante \\(\\sigma^2\\), por lo que la variable respuesta \\(Y_i\\) tiene la misma varianza constante.\nAs칤, el modelo de regresi칩n lineal simple asume que la distribuci칩n de probabilidad de \\(Y\\) tiene la misma varianza \\(\\sigma^2\\), independiente del nivel de la variable predictora \\(X\\).\nLos errores se asumen independientes. Debido a que los t칠rminos \\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) no est치n correlacionados, tambi칠n no lo estar치n las respuestas \\(Y_i\\) e \\(Y_j\\).\nEn resumen, el modelo de regresi칩n lineal simple implica que la respuesta \\(Y_i\\) viene desde una distribuci칩n de probabilidad cuyas medias son \\(\\mathbb{E}(Y_i)=\\beta_0+\\beta_1 X_i\\) y cuyas varianzas son \\(\\sigma^2\\), para todos los niveles de \\(X\\). Adem치s, dos respuestas distintas \\(Y_i\\) e \\(Y_j\\) no est치n correlacionadas."
  },
  {
    "objectID": "slides/lec_week9.html#interpretaci칩n-de-los-par치metros-de-regresi칩n",
    "href": "slides/lec_week9.html#interpretaci칩n-de-los-par치metros-de-regresi칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Interpretaci칩n de los par치metros de regresi칩n",
    "text": "Interpretaci칩n de los par치metros de regresi칩n\nLos par치metros de regresi칩n \\(\\beta_0\\) y \\(\\beta_1\\) en un modelo de regresi칩n lineal simple son llamados coeficientes de regresi칩n, siendo \\(\\beta_1\\) la pendiente y \\(\\beta_0\\) el intercepto. El primero indica el cambio en la media de la distribuci칩n de probabilidad de \\(Y\\) por el incremento unitario en \\(X\\).\nCuando el alcance del modelo incluye \\(X=0\\), \\(\\beta_0\\) entrega la media de la distribuci칩n de probabilidad de \\(Y\\) en \\(X=0\\). Cuando el alcance del modelo no incluye \\(X=0\\), \\(\\beta_0\\) no tienen ninguna interpretaci칩n particular como termino separado en la regresi칩n."
  },
  {
    "objectID": "slides/lec_week9.html#teorema-de-gauss-markov",
    "href": "slides/lec_week9.html#teorema-de-gauss-markov",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Teorema de Gauss-Markov",
    "text": "Teorema de Gauss-Markov\n\nBajo las condiciones de un modelo de regresi칩n lineal simple, los estimadores de m칤nimos cuadrados \\(b_0\\) y \\(b_1\\) son estimadores insesgados y tienen m칤nima varianza entre los estimadores insesgados lineales.\n\nEste teorema establece que \\(b_0\\) y \\(b_1\\) son estimadores insesgados, por lo que:\n\\[\\mathbb{E}(b_0)=\\beta_0 \\hspace{40pt} \\mathbb{E}(b_1)=\\beta_1\\]\nPor lo que ninguno de estos estimadores tiende a sobrestimar o subestimar sistem치ticamente. Segundo, el teorema establece que los estimadores \\(b_0\\) y \\(b_1\\) son m치s precisos (esto es, su distribuci칩n muestral es menos variable) que cualquier otro estimador perteneciente a la clase de estimadores insesgados que son funciones lineales de las observaciones \\(Y_1,\\dots,Y_n\\).\nLos estimadores \\(b_0\\) y \\(b_1\\) son funciones lineal de \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week9.html#propiedades-del-ajuste-de-regresi칩n",
    "href": "slides/lec_week9.html#propiedades-del-ajuste-de-regresi칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Propiedades del ajuste de regresi칩n",
    "text": "Propiedades del ajuste de regresi칩n\nEl ajuste de regresi칩n lineal al usar el m칠todo de m칤nimos cuadrados tiene un n칰mero de propiedades que valen la pena mencionar. Estas propiedades de los estimadores de m칤nimos cuadrados de una funci칩n de regresi칩n no aplican para todos los modelos de regresi칩n.\n\nLa suma de los residuos es cero: \\(\\sum_{i=1}^{n} e_i = 0\\)\nLa suma de los valores observados \\(Y_i\\) es igual a la suma de los valores ajustados \\(\\widehat{Y}_i\\):\n\n\n\\[\\sum_{i=1}^{n} Y_i = \\sum_{i=1}^{n} \\widehat{Y}_i\\]\n\n\nDe esto 칰ltimo, se desprende que la media de los valores ajustados \\(\\widehat{Y}_i\\) es la misma que la media de los valores observados \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week9.html#propiedades-del-ajuste-de-regresi칩n-continuaci칩n",
    "href": "slides/lec_week9.html#propiedades-del-ajuste-de-regresi칩n-continuaci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Propiedades del ajuste de regresi칩n: continuaci칩n",
    "text": "Propiedades del ajuste de regresi칩n: continuaci칩n\n\nLa suma de los residuos ponderados es cero cuando el i-칠simo residuo es ponderado con el nivel de la variable predictora i-칠sima, esto es:"
  },
  {
    "objectID": "slides/lec_week9.html#funci칩n-de-potencia",
    "href": "slides/lec_week9.html#funci칩n-de-potencia",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Funci칩n de potencia",
    "text": "Funci칩n de potencia\nEs la probabilidad de rechazar la hip칩tesis nula dado que la alternativa es correcta. Para los posibles valores de la hip칩tesis alternativa.\nCuando se toma un punto espec칤fico de la hip칩tesis alternativa, ya no se habla de funci칩n potencia, sino de potencia de la prueba. Relaci칩n entre la funci칩n potencia y el error tipo II.\n\\[\\begin{align*}\n\\pi(\\theta)&=1-\\beta\\\\\n&= 1-\\mathbb{P}(\\text{No rechazar }H_0 | H_0 \\text{ es Falsa})\\\\\n&=1-( 1-\\mathbb{P}(\\text{rechazar }H_0 | H_0 \\text{ es Falsa}))\\\\\n&=\\mathbb{P}(\\text{rechazar }H_0 | H_0 \\text{ es Falsa})\\\\\n&=\\mathbb{P}(\\text{rechazar }H_0 | H_1 \\text{ es Verdadera})\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week9.html#ejemplo",
    "href": "slides/lec_week9.html#ejemplo",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejemplo",
    "text": "Ejemplo\nSea \\(X\\) la duraci칩n del tiempo necesario para armar una unidad se distribuye de forma normal con desviaci칩n est치ndar \\(\\sigma=1.4\\) minutos. Se observan los tiempos de armado de 25 unidades seleccionadas aleatoriamente y se escoge la media muestral \\(\\overline{X}\\) como el estad칤stico de prueba. Se plantea la siguiente hip칩tesis nula:\n\\[H_0: \\mu = 10\\]\ny la hip칩tesis alternativa:\n\\[H_1: \\mu > 10\\]\nSe est치 interesado en comparar las siguientes regiones cr칤ticas, con un nivel de significancia del \\(6\\%\\)\n\nPrueba A: Rechazas \\(H_0\\) si \\(\\overline{X} > 10.65\\)\nPrueba B: Rechazas \\(H_0\\) si \\(\\overline{X} > 10.45\\)\nPrueba C: Rechazas \\(H_0\\) si \\(\\overline{X} > 10.25\\)"
  },
  {
    "objectID": "slides/lec_week9.html#ejemplo-continuaci칩n",
    "href": "slides/lec_week9.html#ejemplo-continuaci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejemplo: continuaci칩n",
    "text": "Ejemplo: continuaci칩n\nPara determinar la probabilidad del error de tipo I, Asumimos que \\(H_0\\) es cierta y calculamos:\n\\[\\mathbb{P}\\left(\\overline{X}>c|\\mu =10\\right)=\\alpha\\]\nEn donde \\(c\\) es el valor cr칤tico o frontera de la regi칩n cr칤tica. Si asumimos \\(H_0\\) verdadera, entonces \\(\\overline{X}\\sim N\\left(10,{1.4^2 \\over 25}\\right)\\). Por lo que:\n\\[\\begin{align*}\n\\alpha&=\\mathbb{P}\\left(\\overline{X} > 10.65 | \\mu =10\\right)\\\\\n&=\\mathbb{P}\\left(Z>2.32 | \\mu =10\\right) = 0.0102\n\\end{align*}\\] De igual manera para los otros casos:\n\n\\(\\alpha=0.0537\\)\n\n\\(\\alpha=0.1867 > 0.06\\). Por lo que no la consideramos."
  },
  {
    "objectID": "slides/lec_week9.html#ejemplo-continuaci칩n-1",
    "href": "slides/lec_week9.html#ejemplo-continuaci칩n-1",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejemplo: continuaci칩n",
    "text": "Ejemplo: continuaci칩n\nYa que ni la prueba A ni B han sobrepasado el tama침o m치ximo del error tipo I (nivel de significancia), se determinar치 cu치l es estas dos tiene el tama침o m치s peque침o para el error de tipo II.\n(Error Tipo II \\(\\Rightarrow H_0\\) es falsa.)\n\\[\\beta(\\mu)=\\mathbb{P}\\left(\\overline{X} \\leq c \\Bigg\\vert\\  \\mu > 10\\right)\\]\nLo cual no es cuantificable en general. Consideramos que el valor real de \\(\\mu\\) es \\(10.4\\). Entonces para la prueba A y B, respectivamente.\n\n\\(\\mathbb{P}\\left(\\overline{X} \\leq 10.65 \\Bigg\\vert\\ \\mu = 10.4\\right)= \\mathbb{P}\\left(Z \\leq 0.89 \\Bigg\\vert\\ \\mu= 10.4\\right)=0.8133\\)\n\\(\\mathbb{P}\\left(\\overline{X} \\leq 10.45 \\Bigg\\vert\\ \\mu = 10.4\\right)= \\mathbb{P}\\left(Z \\leq 0.18 \\Bigg\\vert\\ \\mu= 10.4\\right)=0.5714\\)\n\n\nPor lo que, si \\(\\mu = 10.4\\), la probabilidad de que la prueba A se equivoque al rechazar la hip칩tesis nula de que \\(\\mu=10\\) es de \\(0.8133\\), an치logamente para B es de \\(0.5714\\). Por lo que para este valor particular, la prueba B es mejor que la A."
  },
  {
    "objectID": "slides/lec_week9.html#tipos-de-regiones-cr칤ticas",
    "href": "slides/lec_week9.html#tipos-de-regiones-cr칤ticas",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Tipos de regiones cr칤ticas",
    "text": "Tipos de regiones cr칤ticas\nSi consideramos una hipotesis nula simple, esto es:\n\\[H_0: \\theta = \\theta_0\\]\nEn donde \\(\\theta\\) es el par치metro de inter칠s, cuando se muestrea una distribuci칩n cuya funci칩n de densidad de probabilidad es \\(f(x;\\theta)\\), en donde \\(\\theta_0\\) es el valor propuesto de \\(\\theta\\). Si la hip칩tesis alternativa es de la forma:\n\\[H_1: \\theta > \\theta_0\\hspace{20pt}\\text{o}\\hspace{20pt} H_1: \\theta < \\theta_0\\]\nSe dice que \\(H_1\\) es una hipotesis alternativa unilateral. En caso contrario, si la hip칩tesis alternativa no proporciona una direcci칩n con respecto al valor propuesto de \\(\\theta_0\\), entonces se dice que \\(H_1\\) es una hipotesis alternativa bilateral de la forma:\n\\[H_1: \\theta \\neq \\theta_0\\]"
  },
  {
    "objectID": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-media",
    "href": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-media",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejercicio Intervalo de confianza para la media",
    "text": "Ejercicio Intervalo de confianza para la media\nEl 칤ndice de resistencia a la rotura, expresado en kg, de un determinado tipo de cuerda sigue una distribuci칩n Normal con desviaci칩n t칤pica 15.6 kg. Con una muestra de 5 de estas cuerdas, seleccionadas al azar, se obtuvieron los siguientes 칤ndices:\n\\[280, 240, 270, 285, 270.\\]\n\nObtenga un intervalo de confianza para la media del 칤ndice de resistencia a la rotura de este tipo de cuerdas, utilizando un nivel de confianza del 95%.\nSi, con el mismo nivel de confianza, se desea obtener un error m치ximo en la estimaci칩n de la media de 5 kg, 쯥er치 suficiente con elegir una muestra de 30 cuerdas?"
  },
  {
    "objectID": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-media",
    "href": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-media",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n Ejercicio Intervalo de confianza para la media",
    "text": "Resoluci칩n Ejercicio Intervalo de confianza para la media\nSabemos que:\n\\[X:\\{ \\text{칈ndice de resistencia a la rotura en kg}\\} \\sim N(\\mu,15.6^2)\\]\nAdicionalmente que \\(n=5\\). Como sabemos la desviaci칩n t칤pica, nuestro I.C para la media real de la poblaci칩n estar치 dado por:\n\\[IC(\\mu)=[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}]\\]\nAs칤, podemos calcular \\(\\overline{X}=269\\) y por construcci칩n sabemos que \\(\\alpha=0.05 \\Rightarrow 1-\\alpha/2=0.975\\). Luego, reemplazamos en el intervalo correspondiente:\n\\[IC(\\mu)=[269 \\mp Z_{0.975} \\dfrac{15.6}{\\sqrt{5}}]\\]\nEn donde \\(Z_{0.975}=1.96\\). Finalmente, el intervalo de confianza del 95\\(\\%\\) estar치 dado por:\n\\[IC(\\mu)=[255.326;282.674]\\]"
  },
  {
    "objectID": "slides/lec_week8.html#resoluci칩n-continuaci칩n",
    "href": "slides/lec_week8.html#resoluci칩n-continuaci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n: continuaci칩n",
    "text": "Resoluci칩n: continuaci칩n\nPara el item b), por construcci칩n -nuevamente- sabemos que \\(\\alpha=0.05\\) y que:\n\\[1.96 \\dfrac{15.6}{\\sqrt{n}} = 5\\]\nLuego, despejando \\(n \\approx 37.39567\\). Por lo que con elegir una muestra de 30 cuerdas no ser치 suficiente para obtener un error m치ximo en la estimaci칩n de la media de a lo m치s 5 kg."
  },
  {
    "objectID": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-media-1",
    "href": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-media-1",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejercicio Intervalo de confianza para la media",
    "text": "Ejercicio Intervalo de confianza para la media\nEn un hospital se ha tomado la temperatura a una muestra de 64 pacientes para estimar la temperatura media de sus enfermos. La media de la muestra ha sido 37.1 췈C y se sabe que la desviaci칩n t칤pica de toda la poblaci칩n es 1.04 췈C.\n\nObtenga un intervalo de confianza, al 90%, para la media poblacional.\n쮺on qu칠 nivel de confianza podemos afirmar que la media de la poblaci칩n est치 comprendida entre 36.8췈C y 37.4 췈C?"
  },
  {
    "objectID": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-media-1",
    "href": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-media-1",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n Ejercicio Intervalo de confianza para la media",
    "text": "Resoluci칩n Ejercicio Intervalo de confianza para la media\nSabemos que:\n\\[X:\\{\\text{Temperatura de los enfermos en un hospital}\\} \\sim N(\\mu,1.04^2)\\]\nAdicionalmente que \\(n=64\\) y \\(\\overline{x}=37.1\\) . Como sabemos la desviaci칩n t칤pica, nuestro I.C para la media real de la poblaci칩n estar치 dado por:\n\\[IC(\\mu)=[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}]\\]\nPor construcci칩n sabemos que \\(\\alpha=0.1 \\Rightarrow 1-\\alpha/2=0.95\\). Luego, reemplazamos en el intervalo correspondiente:\n\\[IC(\\mu)=[37.1 \\mp Z_{0.95} \\dfrac{1.04}{\\sqrt{64}}]\\]\nEn donde \\(Z_{0.95}=1.64\\). Finalmente, el intervalo de confianza del 90% estar치 dado por:\n\\[IC(\\mu)=[36.4933;37.7067]\\]"
  },
  {
    "objectID": "slides/lec_week8.html#resoluci칩n-continuaci칩n-1",
    "href": "slides/lec_week8.html#resoluci칩n-continuaci칩n-1",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n: continuaci칩n",
    "text": "Resoluci칩n: continuaci칩n\nPara el item b), nos preguntan por \\(1-\\alpha\\), por lo que debemos hacer el proceso inverso. Nos entregan un intervalo dado por:\n\\[[36.8;37.4]\\]\nPor lo que, \\(37.1 + Z_{1-\\alpha/2} \\dfrac{1.04}{\\sqrt{64}}= 37.4 \\Rightarrow Z_{1-\\alpha/2}=2.307692 \\approx 2.3\\)\nAs칤, \\(1-\\alpha/2=0.9893\\Rightarrow \\alpha=0.0214\\)\nLuego, podemos afirmar con un nivel de 97.86% de confianza que la media real de los enfermos de un hospital est치 entre 36.8 y 37.4 grados."
  },
  {
    "objectID": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-proporci칩n",
    "href": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejercicio Intervalo de confianza para la proporci칩n",
    "text": "Ejercicio Intervalo de confianza para la proporci칩n\nUn sondeo de 100 votantes elegidos al azar en un distrito indica que el 55% de ellos estaban a favor de un cierto candidato.\n\nHallar los l칤mites de confianza (a) 95% (b) 99% (c) 99.73% para la proporci칩n de todos los votantes favorables a ese candidato.\n쮻e qu칠 tama침o hay tomar el sondeo para tener al 95% de confianza que el candidato saldr치 elegido?"
  },
  {
    "objectID": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-proporci칩n",
    "href": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n Ejercicio Intervalo de confianza para la proporci칩n",
    "text": "Resoluci칩n Ejercicio Intervalo de confianza para la proporci칩n\nHaremos s칩lo el l칤mite de confianza al 95\\(\\%\\), el intervalo general est치 dado por:\n\\[IC(p):=\\left[ \\widehat{P}\\mp Z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}}\\right]\\]\nreemplazando con \\(\\hat{P}=0.55, n=100, Z_{0.975}=1.96\\) tendremos que el IC estar치 dado por:\n\\[IC(p):= \\left[ 0.55 \\mp 1.96 \\sqrt{\\dfrac{0.55*0.45}{100}}\\right]=[ 0.4524912;0.6475088]\\]"
  },
  {
    "objectID": "slides/lec_week8.html#resoluci칩n-continuaci칩n-2",
    "href": "slides/lec_week8.html#resoluci칩n-continuaci칩n-2",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n: continuaci칩n",
    "text": "Resoluci칩n: continuaci칩n\npara el item b), debemos realizar el mismo procedimiento que antes. Sabemos que el candidato ser치 elegido si la proporci칩n es mayor a 0.5, por lo que:\n\\[0.55-1.96\\sqrt{\\dfrac{0.55*0.45}{n}}>0.5  \\Rightarrow n= 380.3184 \\approx 381\\]\nAs칤, el sondeo debe tomar al menos 381 encuestados."
  },
  {
    "objectID": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-varianza",
    "href": "slides/lec_week8.html#ejercicio-intervalo-de-confianza-para-la-varianza",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejercicio Intervalo de confianza para la varianza",
    "text": "Ejercicio Intervalo de confianza para la varianza\nEn un estudio sobre el llenado de botellas de pl치stico con detergente, se midi칩 el contenido de una muestra de 25. Los resultados fueron: Promedio 0.38 litros y desviaci칩n est치ndar 0.06 litros. Estime, mediante un intervalo de confianza de coeficiente 99%, la varianza poblacional. Asuma que el contenido tiene distribuci칩n normal."
  },
  {
    "objectID": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-varianza",
    "href": "slides/lec_week8.html#resoluci칩n-ejercicio-intervalo-de-confianza-para-la-varianza",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n Ejercicio Intervalo de confianza para la varianza",
    "text": "Resoluci칩n Ejercicio Intervalo de confianza para la varianza\nEl intervalo de confianza para la varianza general est치 dado por:\n\\[IC(\\sigma^2)=\\left[ \\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{1-\\alpha/2}^{2}(n-1)};\\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{\\alpha/2}^{2}(n-1)}\\right]\\]\nEn donde \\(n=25, S^2_{24}=0.06^2\\). As칤, reemplazando en el intervalo tenemos que:\n\\[IC(\\sigma^2)=\\left[ \\dfrac{(25-1)0.06^{2}}{\\chi_{1-0.01/2}^{2}(25-1)};\\dfrac{(25-1)0.06^{2}}{\\chi_{0.01/2}^{2}(25-1)}\\right]\\]\nEn donde \\(\\chi_{0.995}^{2}=45.56, \\chi_{0.005}^{2}=9.886\\). As칤,\n\\[IC(\\sigma^2)\\approx [0.0000731;0.000364]\\]"
  },
  {
    "objectID": "slides/lec_week9.html#ejercicio-test-de-hip칩tesis-para-la-media",
    "href": "slides/lec_week9.html#ejercicio-test-de-hip칩tesis-para-la-media",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejercicio test de hip칩tesis para la media",
    "text": "Ejercicio test de hip칩tesis para la media\nLos siguientes datos representan los tiempos de armado para 20 unidades seleccionadas aleatoriamente:\n9.8, 10.4, 10.6, 9.6, 9.7, 9.9, 10.9, 11.1, 9,6, 10.2, 10.3, 9.6, 9.9, 11,2, 10.6, 9.8, 10.5, 10.1, 10.5, 9.7.\nSup칩ngase que el tiempo necesario para armar una unidad es una variable aleatoria normal con una desviaci칩n est치ndar de 0.6 minutos. Con base en esta muestra, 쯘xiste alguna raz칩n para creer, a un nivel de significancia del 0.05, que el tiempo de armado promedio es mayor de 10 minutos?"
  },
  {
    "objectID": "slides/lec_week9.html#resoluci칩n-ejercicio-test-de-hip칩tesis-para-la-media",
    "href": "slides/lec_week9.html#resoluci칩n-ejercicio-test-de-hip칩tesis-para-la-media",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n Ejercicio Test de hip칩tesis para la media",
    "text": "Resoluci칩n Ejercicio Test de hip칩tesis para la media\nEn este caso sabemos la desviaci칩n est치ndar real: \\(\\sigma=0.6\\) y que el test de hip칩tesis est치 dado por:\n\\[H_0: \\mu \\leq 10 \\hspace{15pt} H_1: \\mu > 10\\]\nAs칤, nuestro Estad칤stico de prueba estar치 dado por:\n\\[z=\\dfrac{\\overline{x}-\\mu_0}{\\sigma /\\sqrt{n}}\\]\nen donde \\(\\overline{x}=9.6\\) por lo que \\(z=\\dfrac{9.6-10}{0.6/\\sqrt{20}}=-2.981424\\) As칤, rechazaremos \\(H_0\\) si \\(z \\geq z_{1-\\alpha}\\) en donde \\(\\alpha=0.05 \\Rightarrow z_{0.95}=1.65\\). Por lo que, debido a que \\(-2.981424 < 1.65\\) no se rechaza la hip칩tesis nula."
  },
  {
    "objectID": "slides/lec_week9.html#ejercicio-test-de-hip칩tesis-para-la-proporci칩n",
    "href": "slides/lec_week9.html#ejercicio-test-de-hip칩tesis-para-la-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Ejercicio Test de hip칩tesis para la proporci칩n",
    "text": "Ejercicio Test de hip칩tesis para la proporci칩n\nEn el a침o \\(2016\\), el \\(16\\%\\) de las embarazadas fueron adolecentes menores de \\(20\\) a침os. El Ministerio de Salud desea saber si esa cifra ha aumentado, para ello se considera una muestra de \\(704\\) partos ocurridos en el Hospital Metropolitano, donde \\(132\\) de ellos corresponden a embarazadas que son adolecentes menores de \\(20\\) a침os. Con la informaci칩n anterior. 쮼s posible aseverar que el porcentaje ha aumentado?"
  },
  {
    "objectID": "slides/lec_week9.html#resoluci칩n-ejercicio-test-de-hip칩tesis-para-la-proporci칩n",
    "href": "slides/lec_week9.html#resoluci칩n-ejercicio-test-de-hip칩tesis-para-la-proporci칩n",
    "title": "Introducci칩n a inferencia estad칤stica",
    "section": "Resoluci칩n Ejercicio Test de hip칩tesis para la proporci칩n",
    "text": "Resoluci칩n Ejercicio Test de hip칩tesis para la proporci칩n\nEn este caso sabemos que el test de hip칩tesis a realizar es:\n\\[H_0: p\\leq 0.16 \\hspace{15pt} H_1: p > 0.16\\]\nAdicionalmente, por enunciado sabemos que \\(\\hat{p}=\\dfrac{132}{704}=0.1875\\). As칤, nuestro estad칤stico de prueba estar치 dado por:\n\\[E=\\dfrac{0.1875-0.16}{\\sqrt{\\dfrac{0.16(1-0.16)}{704}}}=144.0476\\]\nY rechazaremos \\(H_0\\) si \\(E> Z_{1-0.05}=Z_{0.95}=1.65\\), por lo que como \\(144.0476 > 1.65\\) rechazamos \\(H_0\\) y en consecuencia podemos aseverar que el porcentaje ha aumentado."
  },
  {
    "objectID": "slides/lec_week10.html#definici칩n-formal",
    "href": "slides/lec_week10.html#definici칩n-formal",
    "title": "Regresi칩n lineal simple",
    "section": "Definici칩n formal",
    "text": "Definici칩n formal\nUn modelo de regresi칩n b치sico donde s칩lo hay una variable predictora y la funci칩n de regresi칩n es lineal se define como:\n\\[Y_i=\\beta_0+\\beta_1 X_i + \\varepsilon_i\\]\ndonde,\n\n\\(Y_i\\) es el valor de la varible respuesta en la i-칠sima observaci칩n\n\\(\\beta_0\\) y \\(\\beta_1\\) son par치metros\n\\(X_i\\) es una constante conocida: el valor de la variable predictora en la i-칠sima observaci칩n.\n\\(\\varepsilon_i\\) es un t칠rmino de error aleatorio con meadia \\(\\mathbb{E}(\\varepsilon_i)=0\\) y varianza \\(\\mathbb{V}(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) no est치n correlacionados, por lo que su covarianza es cero.\n\n\nEsto modelo se le conoce como modelo de regresi칩n lineal simple."
  },
  {
    "objectID": "slides/lec_week10.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple",
    "href": "slides/lec_week10.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple",
    "title": "Regresi칩n lineal simple",
    "section": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple",
    "text": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple\nLa respuesta \\(Y_i\\) en la i-칠simo ensayo es la suma de dos componentes: - El t칠rmino constante \\(\\beta_0+\\beta_1 X_i\\) y, - El t칠rmino aleatorio \\(\\varepsilon_i\\). Por lo que \\(Y_i\\) es una variable aleatoria\nDebido a que \\(\\mathbb{E}(\\varepsilon_i)=0\\), sigue que:\n\\[\\mathbb{E}(Y_i)=\\mathbb{E}(\\beta_0+\\beta_1 X_i + \\varepsilon_i)=\\beta_0+\\beta_1 X_i + \\mathbb{E}(\\varepsilon_i)= \\beta_0+\\beta_1 X_i\\]\nAs칤, la respuesta \\(Y_i\\), cuando el nivel de \\(X\\) en el i-칠simo ensayo es \\(X_i\\), viene desde una distribuci칩n de probabilidad cuya media est치 dada por:\n\\[\\mathbb{E}(Y_i)=\\beta_0+\\beta_1 X_i\\]\nSabremos que la funci칩n de regresi칩n para este modelo es \\(\\mathbb{E}(Y)=\\beta_0+\\beta_1 X\\)\nDebido a que la funci칩n de regresi칩n relaciona la media de la distribuci칩n de probabilidad de \\(Y\\) para un \\(X\\) dado para el nivel de \\(X\\)."
  },
  {
    "objectID": "slides/lec_week10.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple-continuaci칩n",
    "href": "slides/lec_week10.html#caracter칤sticas-importantes-del-modelo-de-regresi칩n-lineal-simple-continuaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple: continuaci칩n",
    "text": "Caracter칤sticas importantes del modelo de regresi칩n lineal simple: continuaci칩n\nLa respuesta \\(Y_i\\) en el i-칠simo ensayo excede o queda bajo el valor de la funci칩n de regresi칩n por la cantidad del t칠rmino \\(\\varepsilon_i\\). Adem치s, Los errores \\(\\varepsilon_i\\) se asumen que tienen varianza constante \\(\\sigma^2\\), por lo que la variable respuesta \\(Y_i\\) tiene la misma varianza constante.\nAs칤, el modelo de regresi칩n lineal simple asume que la distribuci칩n de probabilidad de \\(Y\\) tiene la misma varianza \\(\\sigma^2\\), independiente del nivel de la variable predictora \\(X\\).\nLos errores se asumen independientes. Debido a que los t칠rminos \\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) no est치n correlacionados, tambi칠n no lo estar치n las respuestas \\(Y_i\\) e \\(Y_j\\).\nEn resumen, el modelo de regresi칩n lineal simple implica que la respuesta \\(Y_i\\) viene desde una distribuci칩n de probabilidad cuyas medias son \\(\\mathbb{E}(Y_i)=\\beta_0+\\beta_1 X_i\\) y cuyas varianzas son \\(\\sigma^2\\), para todos los niveles de \\(X\\). Adem치s, dos respuestas distintas \\(Y_i\\) e \\(Y_j\\) no est치n correlacionadas."
  },
  {
    "objectID": "slides/lec_week10.html#interpretaci칩n-de-los-par치metros-de-regresi칩n",
    "href": "slides/lec_week10.html#interpretaci칩n-de-los-par치metros-de-regresi칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Interpretaci칩n de los par치metros de regresi칩n",
    "text": "Interpretaci칩n de los par치metros de regresi칩n\nLos par치metros de regresi칩n \\(\\beta_0\\) y \\(\\beta_1\\) en un modelo de regresi칩n lineal simple son llamados coeficientes de regresi칩n, siendo \\(\\beta_1\\) la pendiente y \\(\\beta_0\\) el intercepto. El primero indica el cambio en la media de la distribuci칩n de probabilidad de \\(Y\\) por el incremento unitario en \\(X\\).\nCuando el alcance del modelo incluye \\(X=0\\), \\(\\beta_0\\) entrega la media de la distribuci칩n de probabilidad de \\(Y\\) en \\(X=0\\). Cuando el alcance del modelo no incluye \\(X=0\\), \\(\\beta_0\\) no tienen ninguna interpretaci칩n particular como termino separado en la regresi칩n."
  },
  {
    "objectID": "slides/lec_week10.html#teorema-de-gauss-markov",
    "href": "slides/lec_week10.html#teorema-de-gauss-markov",
    "title": "Regresi칩n lineal simple",
    "section": "Teorema de Gauss-Markov",
    "text": "Teorema de Gauss-Markov\n\nBajo las condiciones de un modelo de regresi칩n lineal simple, los estimadores de m칤nimos cuadrados \\(b_0\\) y \\(b_1\\) son estimadores insesgados y tienen m칤nima varianza entre los estimadores insesgados lineales.\n\nEste teorema establece que \\(b_0\\) y \\(b_1\\) son estimadores insesgados, por lo que:\n\\[\\mathbb{E}(b_0)=\\beta_0 \\hspace{40pt} \\mathbb{E}(b_1)=\\beta_1\\]\nPor lo que ninguno de estos estimadores tiende a sobrestimar o subestimar sistem치ticamente. Segundo, el teorema establece que los estimadores \\(b_0\\) y \\(b_1\\) son m치s precisos (esto es, su distribuci칩n muestral es menos variable) que cualquier otro estimador perteneciente a la clase de estimadores insesgados que son funciones lineales de las observaciones \\(Y_1,\\dots,Y_n\\).\nLos estimadores \\(b_0\\) y \\(b_1\\) son funciones lineal de \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week10.html#propiedades-del-ajuste-de-regresi칩n",
    "href": "slides/lec_week10.html#propiedades-del-ajuste-de-regresi칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Propiedades del ajuste de regresi칩n",
    "text": "Propiedades del ajuste de regresi칩n\nEl ajuste de regresi칩n lineal al usar el m칠todo de m칤nimos cuadrados tiene un n칰mero de propiedades que valen la pena mencionar. Estas propiedades de los estimadores de m칤nimos cuadrados de una funci칩n de regresi칩n no aplican para todos los modelos de regresi칩n.\n\nLa suma de los residuos es cero: \\(\\sum_{i=1}^{n} e_i = 0\\)\nLa suma de los valores observados \\(Y_i\\) es igual a la suma de los valores ajustados \\(\\widehat{Y}_i\\):\n\n\n\\[\\sum_{i=1}^{n} Y_i = \\sum_{i=1}^{n} \\widehat{Y}_i\\]\n\n\nDe esto 칰ltimo, se desprende que la media de los valores ajustados \\(\\widehat{Y}_i\\) es la misma que la media de los valores observados \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week10.html#propiedades-del-ajuste-de-regresi칩n-continuaci칩n",
    "href": "slides/lec_week10.html#propiedades-del-ajuste-de-regresi칩n-continuaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Propiedades del ajuste de regresi칩n: continuaci칩n",
    "text": "Propiedades del ajuste de regresi칩n: continuaci칩n\n\nLa suma de los residuos ponderados es cero cuando el i-칠simo residuo es ponderado con el nivel de la variable predictora i-칠sima, esto es:\n\n\n\\[\\sum_{i=1}^{n} X_i e_i = 0\\]\n\nUna consecuencia de las propiedades 1 y 3, es que la suma de los pesos ponderados es cero cuando el i-칠simo residuo es ponderado con el valor ajustado de la i-칠sima variable respuesta, esto es:\n\n\n\n\\[\\sum_{i=1}^{n} \\widehat{Y}_i e_i = 0\\]\n\nLa recta de regresi칩n siempre pasa por el punto \\((\\overline{X},\\overline{Y})\\)."
  },
  {
    "objectID": "slides/lec_week10.html#inferencia-sobre-la-pendiente",
    "href": "slides/lec_week10.html#inferencia-sobre-la-pendiente",
    "title": "Regresi칩n lineal simple",
    "section": "Inferencia sobre la pendiente",
    "text": "Inferencia sobre la pendiente\nFrecuentemente es de particular inter칠s la inferencia sobre el par치metro de la pendiente de regresi칩n, pues nos entrega una noci칩n de cambio medio por unidad en la variable regresora. Un tipo de test relevante en este contexto es:\n\\[H_0: \\beta_1=0 \\hspace{20pt} H_1: \\beta_1\\ne 0\\]\nEste test de hip칩tesis es relevante debido a que cuando \\(\\beta_1=0\\), no existe una asociaci칩n lineal entre las variables \\(X\\) e \\(Y\\).\nEn el caso de que el t칠rmino de error en el modelo de regresi칩n sea normal, la condici칩n de que \\(\\beta_1=0\\) implica a칰n m치s cosas. Debido a que en este modelo todas las distribuci칩n de probabilidades de \\(Y\\) son normales con varianza constante, y que las medias son iguales cuando \\(\\beta_1=0\\), sigue que las distribuciones de probabilidad de \\(Y\\) son id칠nticas cuando \\(\\beta_1=0\\).\n\nAs칤, \\(\\beta_1=0\\) para el modelo de regresi칩n lineal normal implica que no s칩lo no existe relaci칩n lineal entre \\(X\\) e \\(Y\\), pero adem치s no existe ning칰n tipo de relaci칩n entre \\(Y\\) y \\(X\\), dado que las distribuciones de probabilidad de \\(Y\\) son id칠nticas para todos los niveles de \\(X\\)."
  },
  {
    "objectID": "slides/lec_week10.html#distribuci칩n-muestral-de-b_1",
    "href": "slides/lec_week10.html#distribuci칩n-muestral-de-b_1",
    "title": "Regresi칩n lineal simple",
    "section": "Distribuci칩n muestral de \\(b_1\\)",
    "text": "Distribuci칩n muestral de \\(b_1\\)\nPor lo visto antes, sabemos que el estimador puntual de \\(b_1\\) est치 dado por:\n\\[b_1=\\dfrac{\\sum (X_i-\\overline{X})(Y_i - \\overline{Y})}{\\sum (X_i-\\overline{X})^2}\\]\nLa distribuci칩n muestral de \\(b_1\\) hace referencia a los diferentes valores de \\(b_1\\) que ser칤an obtenidos con un muestreo repetido cuando los niveles de la variable predictora \\(X\\) se mantiene constante entre las diferentes muestras. Para el modelo de regresi칩n normal, la distribuci칩n muestral de \\(b_1\\) es normal con media y varianza dada por:\n\\[\\mathbb{E}(b_1)=\\beta_1\\qquad \\qquad\\mathbb{V}(b_1)=\\dfrac{\\sigma^2}{\\sum (X_i-\\overline{X})^2}\\]\nPara mostrar esto, debemos identificar que \\(b_1\\) es una combinaci칩n lineal de las observaciones \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week10.html#normalidad-media-y-varianza",
    "href": "slides/lec_week10.html#normalidad-media-y-varianza",
    "title": "Regresi칩n lineal simple",
    "section": "Normalidad, media y varianza",
    "text": "Normalidad, media y varianza\nDebido a que el t칠rmino \\(b_1\\) es una combinaci칩n lineal de \\(Y_i\\), y este 칰ltimo son variables aleatoria normales independientes, sigue que \\(b_1\\) tambi칠n lo es.\nLa insesgadez del estimador puntual de \\(b_1\\) es debido al teorema de Gauss-Markov, sigue que:\n\\[\\begin{align*}\n\\mathbb{E}(b_1)&=\\mathbb{E}\\left(\\sum k_i Y_i\\right)=\\sum k_i \\mathbb{E}(Y_i)= \\sum k_i(\\beta_0+\\beta_1 X_i)\\\\\n&= \\beta_0 \\sum k_i + \\beta_1 \\sum k_i X_i = \\beta_1\n\\end{align*}\\] En cuanto a la varianza de \\(b_1\\), s칩lo necesitamos recordar que \\(Y_i\\) son variables aleatorias independientes, cada una con varianza \\(\\sigma^2\\) y que \\(k_i\\) son constantes. Por lo que: \\[\\begin{align*}\n\\mathbb{V}(b_1)&=\\mathbb{V}\\left(\\sum k_i Y_i\\right)=\\sum k_{i}^{2} \\mathbb{V}(Y_i)\\\\\n&=\\sum k_{i}^{2} \\sigma^2=\\sigma^2 \\sum k_{i}^{2}\\\\\n&= \\dfrac{\\sigma^2}{\\sum (X_i -\\overline{X})^2}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week10.html#varianza-estimada",
    "href": "slides/lec_week10.html#varianza-estimada",
    "title": "Regresi칩n lineal simple",
    "section": "Varianza estimada",
    "text": "Varianza estimada\nPodemos estimar la varianza de la distribuci칩n muestral de \\(b_1\\):\n\\[\\mathbb{V}(b_1)=\\dfrac{\\sigma^2}{\\sum (X_i - \\overline{X})^2}\\]\nReemplazando el par치metro \\(\\sigma^2\\) con el ECM, el estimador insesgado de \\(\\sigma^2\\):\n\\[\\widehat{\\mathbb{V}(b_1)}=\\dfrac{MSE}{\\sum (X_i - \\overline{X})^2}\\]\nEsta estimaci칩n puntual es un estimador insesgada de \\(\\mathbb{V}(b_1)\\). Tomando la ra칤z cuadrado podemos obtener la estimaci칩n puntual para la desviaci칩n est치ndar."
  },
  {
    "objectID": "slides/lec_week10.html#distribuci칩n-muestral-칰til",
    "href": "slides/lec_week10.html#distribuci칩n-muestral-칰til",
    "title": "Regresi칩n lineal simple",
    "section": "Distribuci칩n muestral 칰til",
    "text": "Distribuci칩n muestral 칰til\nCon vistas en obtener intervalos de confianza para los par치metros de regresi칩n, necesitamos obtener las distribuciones muestrales de cantidades pivotales, entre ellas la cantidad:\n\\[(b_1-\\beta_1)/\\sqrt{\\widehat{\\mathbb{V}(b_1)}}\\]\nDebido a que \\(b_1\\) est치 distribuido normalmente, sabemos que la estandarizaci칩n:\n\\[\\dfrac{(b_1-\\beta_1)}{\\sqrt{\\mathbb{V}(b_1)}}\\]\nes una variable aleatoria normal est치ndar. En la pr치ctica, no se tiene acceso a la varianza te칩rica por lo que esta cantidad debe ser estimada por \\(\\widehat{\\mathbb{V}(b_1)}\\) por que estamos particularmente interesados en la distribuci칩n de \\((b_1-\\beta_1)/\\sqrt{\\widehat{\\mathbb{V}(b_1)}}\\)"
  },
  {
    "objectID": "slides/lec_week10.html#distribuci칩n-muestral-칰til-continuaci칩n",
    "href": "slides/lec_week10.html#distribuci칩n-muestral-칰til-continuaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Distribuci칩n muestral 칰til: continuaci칩n",
    "text": "Distribuci칩n muestral 칰til: continuaci칩n\nCuando una estad칤stico est치 estandarizado pero el denominador es una estimaci칩n de la desviaci칩n est치ndar en vez de su valor real, se le llama estad칤stico estudentizado. Un teorema importante en estad칤stica establece que el estad칤stico:\n\\[\\dfrac{(b_1-\\beta_1)}{\\sqrt{\\widehat{\\mathbb{V}(b_1)}}}\\sim t(n-2)\\]\nPara el modelo de regresi칩n que estamos estudiando. Esto viene del hecho que \\(SSE/\\sigma^2 \\sim \\chi^2(n-2)\\) y es independiente de \\(b_0\\) y \\(b_1\\)."
  },
  {
    "objectID": "slides/lec_week10.html#intervalo-de-confianza-para-la-pendiente",
    "href": "slides/lec_week10.html#intervalo-de-confianza-para-la-pendiente",
    "title": "Regresi칩n lineal simple",
    "section": "Intervalo de confianza para la pendiente",
    "text": "Intervalo de confianza para la pendiente\nDebido a que esta cantidad sigue una distribuci칩n t-student, podemos establecer que:\n\\[\\mathbb{P}(t(\\alpha/2,n-2)\\leq (b_1-\\beta_1)/\\sqrt{\\widehat{\\mathbb{V}(b_1)}} \\leq  t(1-\\alpha/2,n-2))=1-\\alpha\\]\nLuego, operando de igual manera que en la construcci칩n de intervalos de confianza usual (v칤a pivote). Podemos llegar a un intervalo de confianza para \\(\\beta_1\\):\n\\[\\left[ b_1 \\pm t(1-\\alpha/2, n-2) \\sqrt{\\widehat{\\mathbb{V}(b_1)}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week10.html#test-de-hip칩tesis-para-la-pendiente",
    "href": "slides/lec_week10.html#test-de-hip칩tesis-para-la-pendiente",
    "title": "Regresi칩n lineal simple",
    "section": "Test de hip칩tesis para la pendiente",
    "text": "Test de hip칩tesis para la pendiente\nDebido a que:\n\\[\\dfrac{(b_1-\\beta_1)}{\\sqrt{\\widehat{\\mathbb{V}(b_1)}}}\\sim t(n-2)\\]\nToda la teor칤a de test de hip칩tesis usuales es v치lida (tests unilaterales y bilaterales).\nTenemos particular inter칠s en un test del tipo:\n\\[H_0: \\beta_1 = 0 \\hspace{20pt} H_1:\\beta_1 \\neq 0\\]\nPues con ello probamos si existe una asociaci칩n lineal entre las variables del modelo bajo un cierto nivel de confianza."
  },
  {
    "objectID": "slides/lec_week10.html#inferencia-sobre-el-intercepto",
    "href": "slides/lec_week10.html#inferencia-sobre-el-intercepto",
    "title": "Regresi칩n lineal simple",
    "section": "Inferencia sobre el intercepto",
    "text": "Inferencia sobre el intercepto\nComo lo mencionamos antes, rara vez tendremos inter칠s en hacer inferencia sobre el par치metro \\(\\beta_0\\), y estos son s칩lo v치lidos cuando el rango de la variable predictora incluye \\(X=0\\).\nComo hemos visto antes la estimaci칩n puntal del intercepto est치 dado por:\n\\[b_0=\\overline{Y}-b_1\\overline{X}\\]\nPara el modelo de regresi칩n en estudio, la distribuci칩n muestral de \\(b_0\\) es normal, con media y varianza:\n\\[\\mathbb{E}(b_0)=\\beta_0\\qquad \\qquad \\mathbb{V}(b_0)=\\sigma^2\\left[ \\dfrac{1}{n}+\\dfrac{\\overline{X}^2}{\\sum (X_i-\\overline{X})^2}\\right]\\]\nLa normalidad es obtenida debido a que \\(b_0\\) al igual que \\(b_1\\), es una combinaci칩n lineal de observaciones \\(Y_i\\). Al igual que antes, una estimador de la varianza viene dado al reemplazar \\(\\sigma^2\\) por su estimaci칩n puntual (ECM). El estimador de la desviaci칩n est치ndar es obtenido aplicando ra칤z cuadrada."
  },
  {
    "objectID": "slides/lec_week10.html#intervalo-de-confianza-para-el-intercepto",
    "href": "slides/lec_week10.html#intervalo-de-confianza-para-el-intercepto",
    "title": "Regresi칩n lineal simple",
    "section": "Intervalo de confianza para el intercepto",
    "text": "Intervalo de confianza para el intercepto\nAl igual que antes, se tiene que:\n\\[\\dfrac{b_0-\\beta_0}{\\sqrt{\\widehat{\\mathbb{V}(b_0)}}}\\sim t(n-2)\\]\npara este modelo de regresi칩n. As칤, los intervalos de confianza pueden ser construidos al igual que para \\(\\beta_1\\). Esto es:\n\\[\\left[ b_0 \\pm t(1-\\alpha/2, n-1)\\sqrt{\\widehat{\\mathbb{V}(b_0)}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week10.html#introducci칩n",
    "href": "slides/lec_week10.html#introducci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Introducci칩n",
    "text": "Introducci칩n\nCon lo anterior, ya hemos visto gran parte de la teor칤a de un modelo de regresi칩n b치sico. En lo que sigue, estudiaremos el an치lisis de regresi칩n desde la perspectiva de an치lisis de varianza.\nNociones b치sicas: El enfoque desde el an치lisis de varianza se base en particionar la suma de cuadrado y grados de libertad asociados con la variable respuesta \\(Y\\). Identificaremos 3 t칠rminos que usaremos frecuentemente:\n\nSuma de cuadrados total (SSTO): \\(\\sum (Y_i - \\overline{Y})^2\\)\nSuma de los cuadrados del error (SSE): \\(\\sum (Y_i - \\hat{Y}_i)^2\\)\nSuma de los cuadrados de la regresi칩n (SSR): \\(\\sum (\\hat{Y}_i-\\overline{Y})^2\\)\n\n\nen donde se tiene la relaci칩n:\n\\[SSTO=SSE+SSR\\]"
  },
  {
    "objectID": "slides/lec_week10.html#desglose-de-los-grados-de-libertad",
    "href": "slides/lec_week10.html#desglose-de-los-grados-de-libertad",
    "title": "Regresi칩n lineal simple",
    "section": "Desglose de los grados de libertad",
    "text": "Desglose de los grados de libertad\nAl igual que para la varianza, podemos desglosar los grados de libertad. Es claro ver que:\n\nSSTO tiene asociado \\(n-1\\) grados de libertad, debido a que estimamos la media poblacional.\nSSE tiene asociado \\(n-2\\) grados de libertad, debido a que para obtener \\(\\hat{Y}_i\\) debemos estimar \\(\\beta_0\\) y \\(\\beta_1\\)\nSSR tiene asociado \\(1\\) grado de libertad debido a que los valores ajustados son calculados a partir de la recta de regresi칩n, por lo que \\(2\\) grados de libertad est치n a asociado a esta, pero uno de ello es perdido debido a la estimaci칩n \\(\\overline{Y}\\).\n\n\nAs칤, se tiene que:\n\\[n-1=1+(n-2)\\]"
  },
  {
    "objectID": "slides/lec_week10.html#cuadrados-medios",
    "href": "slides/lec_week10.html#cuadrados-medios",
    "title": "Regresi칩n lineal simple",
    "section": "Cuadrados medios",
    "text": "Cuadrados medios\nLlamamos cuadrados medios a las sumas cuadradas divididas por sus grados de libertad respectivos. Por lo que tenemos:\n\nError cuadr치tico medio: \\(\\dfrac{SSE}{n-2}\\)\nCuadrado medio de regresi칩n: \\(\\dfrac{SSR}{1}\\)\n\n\nEn este caso, los cuadrados medios no son aditivos"
  },
  {
    "objectID": "slides/lec_week10.html#tabla-anova",
    "href": "slides/lec_week10.html#tabla-anova",
    "title": "Regresi칩n lineal simple",
    "section": "Tabla ANOVA",
    "text": "Tabla ANOVA\nLo que hemos visto anteriormente, puede ser resumido en la tabla ANOVA usual, en donde se incorpor칩 adem치s la esperanza de los cuadrados medios.\n\n\n\n\n\n\n\n\n\n\nF.V.\nSS\ng.l.\nMS\n\\(\\mathbf{\\mathbb{E}(MS)}\\)\n\n\n\n\nRegresi칩n\n\\(SSR = \\sum (\\hat{Y}_i-\\overline{Y})^2\\)\n\\(1\\)\n\\(MSR=SSR\\)\n\\(\\sigma^2+\\beta_{1}^{2}\\sum (X_i-\\overline{X})^2\\)\n\n\nError\n\\(SSE = \\sum (Y_i - \\hat{Y}_i)^2\\)\n\\(n-2\\)\n\\(MSE=\\dfrac{SSE}{n-2}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(SSTO=\\sum (Y_i - \\overline{Y})^2\\)\n\\(n-1\\)"
  },
  {
    "objectID": "slides/lec_week10.html#test-f",
    "href": "slides/lec_week10.html#test-f",
    "title": "Regresi칩n lineal simple",
    "section": "Test F",
    "text": "Test F\nEl enfoque de an치lisis de varianza nos permite realizar f치cilmente test para modelos de regresi칩n (y otros modelos lineales). Por ejemplo, consideremos:\n\\[H_0: \\beta_1 = 0 \\hspace{20pt} H_1:\\beta_1 \\neq 0\\]\nEstad칤stico de prueba\nBajo este enfoque consideramos el estad칤stico \\(F^*\\), definido como:\n\\[F^*=\\dfrac{MSR}{MSE}\\]\nDistribuci칩n muestral de \\(F^*\\)\nEs posible mostrar que bajo \\(H_0\\), \\(F^*\\) sigue una distribuci칩n \\(F(1,n-2)\\)"
  },
  {
    "objectID": "slides/lec_week10.html#test-f-continuaci칩n",
    "href": "slides/lec_week10.html#test-f-continuaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Test F: continuaci칩n",
    "text": "Test F: continuaci칩n\nRegla de decisi칩n\nDebido a que \\(F^*\\) sigue una distribuci칩n \\(F(1,n-2)\\) bajo \\(H_0\\), la regla de decisi칩n ser치:\n\nSi \\(F^* \\leq F(1-\\alpha; 1,n-2)\\), optamos por \\(H_0\\)\nSi \\(F^* > F(1-\\alpha; 1,n-2)\\), optamos por \\(H_1\\)"
  },
  {
    "objectID": "slides/lec_week10.html#coeficiente-de-determinaci칩n",
    "href": "slides/lec_week10.html#coeficiente-de-determinaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Coeficiente de determinaci칩n",
    "text": "Coeficiente de determinaci칩n\nEl coeficiente de determinaci칩n lo definimos como:\n\\[R^2=\\dfrac{SSR}{SSTO}=1-\\dfrac{SSE}{SSTO}\\]\ny lo interpretamos como la proporci칩n de la variabilidad que es explicada por el ajuste de regresi칩n lineal.\nEste coeficiente se mueve entre 0 y 1, siendo 1 un ajuste perfecto. Un buen ajuste de regresi칩n suele estar entre 0.7 - 0.9, pero esto puede variar dependiendo del contexto del problema."
  },
  {
    "objectID": "slides/lec_week10.html#limitaciones-del-coeficiente-de-determinaci칩n",
    "href": "slides/lec_week10.html#limitaciones-del-coeficiente-de-determinaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Limitaciones del coeficiente de determinaci칩n",
    "text": "Limitaciones del coeficiente de determinaci칩n\n\nUn coeficiente de determinaci칩n alto no indica que se puedan hacer predicciones buenas\nUn coeficiente de determinaci칩n alto no indica que el ajuste es necesariamente bueno\nUn coeficiente de determinaci칩n cercano a cero no indica que \\(X\\) e \\(Y\\) no est칠n relacionados."
  },
  {
    "objectID": "slides/lec_week10.html#coeficiente-de-correlaci칩n",
    "href": "slides/lec_week10.html#coeficiente-de-correlaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Coeficiente de correlaci칩n",
    "text": "Coeficiente de correlaci칩n\nEste coeficiente puede ser definido como la ra칤z del coeficiente de determinaci칩n.\n\\[r=\\pm \\sqrt{R^2}\\]\ny lo interpretamos como el coeficiente de correlaci칩n de Pearson."
  },
  {
    "objectID": "slides/lec_week10.html#aplicaci칩n-computacional",
    "href": "slides/lec_week10.html#aplicaci칩n-computacional",
    "title": "Regresi칩n lineal simple",
    "section": "Aplicaci칩n computacional",
    "text": "Aplicaci칩n computacional\n\nrequire(tidyverse)\nrequire(MASS)\nrequire(car)\nrequire(mosaic)\nset.seed(163)\ndata(UScereal)\nplot<-ggplot(UScereal,aes(x=fibre,y=calories)) + geom_point() +\n  geom_smooth(method=lm,se=FALSE,color=\"red\")"
  },
  {
    "objectID": "slides/lec_week10.html#aplicaci칩n-computacional-continuaci칩n",
    "href": "slides/lec_week10.html#aplicaci칩n-computacional-continuaci칩n",
    "title": "Regresi칩n lineal simple",
    "section": "Aplicaci칩n computacional: continuaci칩n",
    "text": "Aplicaci칩n computacional: continuaci칩n\n\nmodel <- lm(calories~fibre,data=UScereal)\nsummary(model)\n\n\nCall:\nlm(formula = calories ~ fibre, data = UScereal)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-144.73  -28.07  -17.48   15.51  258.48 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  134.117      8.522  15.738   <2e-16 ***\nfibre          3.950      1.181   3.344   0.0014 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57.97 on 63 degrees of freedom\nMultiple R-squared:  0.1507,    Adjusted R-squared:  0.1372 \nF-statistic: 11.18 on 1 and 63 DF,  p-value: 0.001396"
  },
  {
    "objectID": "slides/lec_week10.html#aplicaci칩n-computacional-continuaci칩n-1",
    "href": "slides/lec_week10.html#aplicaci칩n-computacional-continuaci칩n-1",
    "title": "Regresi칩n lineal simple",
    "section": "Aplicaci칩n computacional: continuaci칩n",
    "text": "Aplicaci칩n computacional: continuaci칩n\n\nconfint(model)\n\n                 2.5 %    97.5 %\n(Intercept) 117.087793 151.14595\nfibre         1.589422   6.31138\n\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: calories\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nfibre      1  37572   37572   11.18 0.001396 **\nResiduals 63 211723    3361                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/lec_week10.html#diagn칩stico-para-las-variables-predictoras",
    "href": "slides/lec_week10.html#diagn칩stico-para-las-variables-predictoras",
    "title": "Regresi칩n lineal simple",
    "section": "Diagn칩stico para las variables predictoras",
    "text": "Diagn칩stico para las variables predictoras\nPrimero debemos analizar las variables predictora para detectar la presencia de datos an칩malos o outliers, que puedan influenciar la viabilidad del modelo.\n\nLa presencia de outliers, puede provocar residuos grandes en magnitud, influenciando enormemente el ajuste de regresi칩n."
  },
  {
    "objectID": "slides/lec_week10.html#diagn칩stico-para-residuos",
    "href": "slides/lec_week10.html#diagn칩stico-para-residuos",
    "title": "Regresi칩n lineal simple",
    "section": "Diagn칩stico para residuos",
    "text": "Diagn칩stico para residuos\nEn general, los gr치ficos de diagn칩sticos utilizando directamente la variable respuesta \\(Y\\) no son muy 칰tiles en el an치lisis de regresi칩n debido a que el valor de las observaciones en la variable respuesta son una funci칩n del nivel de la variable predictora. Por lo que usualmente, se analizan indirectamente mediante la inspecci칩n de los residuos.\nLos residuos \\(e_i\\) son la diferencia entre el valor observado \\(Y_i\\) y el valor ajustado \\(\\hat{Y}_i\\):\n\\[e_i=Y_i-\\hat{Y}_i\\]\nEstos pueden ser considerados como el error observado, a diferencia de valor real del error \\(\\varepsilon_i\\) en el modelo de regresi칩n:\n\\[\\varepsilon_i=Y_i - \\mathbb{E}(Y_i)\\]\nPara el modelo de regresi칩n lineal simple, los errores \\(\\varepsilon_i\\) se asumen variables aleatorias normales independientes, con media 0 y varianza constante \\(\\sigma^2\\). Si el modelo es apropiado para los datos disponibles, el residuo observado \\(e_i\\) deben reflejar las propiedades que se asumieron para \\(\\varepsilon_i\\).\nEsta es la idea b치sica del an치lisis de residuos, una herramienta 칰til para evaluar la viabilidades de los modelos."
  },
  {
    "objectID": "slides/lec_week10.html#propiedades-de-los-residuos-media",
    "href": "slides/lec_week10.html#propiedades-de-los-residuos-media",
    "title": "Regresi칩n lineal simple",
    "section": "Propiedades de los residuos: media",
    "text": "Propiedades de los residuos: media\nLa media de los \\(n\\) residuos \\(e_i\\) para el modelo de regresi칩n lineal simple es:\n\\[\\overline{e}=\\dfrac{\\sum e_i}{n}=0\\]\ndonde \\(\\overline{e}\\) denota la media de los residuos. As칤, debido a que \\(\\overline{e}\\) es siempre 0, este no provee informaci칩n sobre si los errores reales \\(\\varepsilon_i\\) tienen valor esperado \\(\\mathbb{E}(\\varepsilon_i)=0\\)."
  },
  {
    "objectID": "slides/lec_week10.html#propiedades-de-los-residuos-varianza",
    "href": "slides/lec_week10.html#propiedades-de-los-residuos-varianza",
    "title": "Regresi칩n lineal simple",
    "section": "Propiedades de los residuos: varianza",
    "text": "Propiedades de los residuos: varianza\nLa varianza de los \\(n\\) residuos \\(e_i\\) est치 definida como:\n\\[s^2=\\dfrac{\\sum (e_i - \\overline{e})^2}{n-2}=\\dfrac{\\sum e_{i}^{2}}{n-2}=\\dfrac{SSE}{n-2}=MSE\\]\nSi el modelo es apropiado, el error cuadr치tico medio es un estimador insesgado de la varianza del error \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/lec_week10.html#propiedades-de-los-residuos-no-independencia",
    "href": "slides/lec_week10.html#propiedades-de-los-residuos-no-independencia",
    "title": "Regresi칩n lineal simple",
    "section": "Propiedades de los residuos: no independencia",
    "text": "Propiedades de los residuos: no independencia\nLos residuos \\(e_i\\) no son variables aleatorias independientes debido a que involucran los valores ajustados \\(\\hat{Y}_i\\), los cuales est치n basado en la misma funci칩n de regresi칩n ajustada. Como resultado de lo anterior, los residuos para el modelo de regresi칩n est치n sujetos a dos restricciones:\n\nLa suma de \\(e_i\\) debe ser 0\nla suma de \\(X_i e_i\\) debe ser 0\n\n\nCuando el tama침o de muestra es grande en comparaci칩n con el n칰mero de par치metros en el modelo de regresi칩n, la efecto de dependencia entre los residuos \\(e_i\\) no tiene mayor importancia y puede ser ignorado."
  },
  {
    "objectID": "slides/lec_week10.html#propiedades-de-los-residuos-residuos-semi-studentizados",
    "href": "slides/lec_week10.html#propiedades-de-los-residuos-residuos-semi-studentizados",
    "title": "Regresi칩n lineal simple",
    "section": "Propiedades de los residuos: residuos semi-studentizados",
    "text": "Propiedades de los residuos: residuos semi-studentizados\nFrecuentemente, sirve estandarizar los residuos para realizar el an치lisis. debido a que la desviaci칩n est치ndar de los t칠rminos de error \\(\\varepsilon_i\\) es \\(\\sigma\\), el cual puede ser estimado mediante \\(\\sqrt{MSE}\\), por lo que es natural considerar la estandarizaci칩n:\n\\[e_{i}^{*}=\\dfrac{e_i-\\overline{e}}{\\sqrt{MSE}}=\\dfrac{e_i}{\\sqrt{MSE}}\\]\nSi \\(\\sqrt{MSE}\\) fuese una estimaci칩n de la desviaci칩n est치ndar de los residuos \\(e_i\\), llamar칤amos \\(e_{i}^{*}\\) residuos studentizados. Sin embargo, la desviaci칩n est치ndar de \\(e_i\\) es compleja y var칤a para los diferentes residuos \\(e_i\\), y \\(\\sqrt{MSE}\\) es s칩lo una aproximaci칩n de la desviaci칩n est치ndar de \\(e_i\\).\nPor lo que llamamos el estad칤stico \\(e_{i}^{*}\\) un residuo semi-studentizado. Estos tipo de residuos nos sirven para identificar la presencia de datos an칩malos."
  },
  {
    "objectID": "slides/lec_week10.html#diferencias-con-el-modelo-estudiado",
    "href": "slides/lec_week10.html#diferencias-con-el-modelo-estudiado",
    "title": "Regresi칩n lineal simple",
    "section": "Diferencias con el modelo estudiado",
    "text": "Diferencias con el modelo estudiado\nUsualmente, estaremos en busca de 6 formas en la cuales un modelo de regresi칩n lineal simple con errores normales no es adecuado.\n\nLa funci칩n de regresi칩n no es lineal\nLos errores no tienen varianza constante\nLos errores no son independientes\nEl modelo ajusta todas las observaciones exceptuando algunas\nLos errores no se distribuyen de manera normal\nUnas o varias variables predictoras fueron omitidas del modelo"
  },
  {
    "objectID": "slides/lec_week10.html#diagn칩stico-de-los-residuos",
    "href": "slides/lec_week10.html#diagn칩stico-de-los-residuos",
    "title": "Regresi칩n lineal simple",
    "section": "Diagn칩stico de los residuos",
    "text": "Diagn칩stico de los residuos\nUtilizaremos varios gr치ficos para identificar si ocurre alguna de las 6 situaciones antes planteadas. Los siguientes gr치ficos son usualmente usados para este fin\n\nGr치ficos de los residuos vs la variable predictora\nGr치fico del valor absoluto o el cuadrado de los residuos vs la variable predictora\nGr치fico de los residuos vs valores ajustados\nGr치fico de los residuos vs tiempo u otra secuencia\nGr치fico de los residuos vs variables predictoras omitidas\nBox-Plot de los residuos\nGr치fico de probabilidad normal de los residuos"
  },
  {
    "objectID": "slides/lec_week10.html#test-relacionados-con-los-residuos",
    "href": "slides/lec_week10.html#test-relacionados-con-los-residuos",
    "title": "Regresi칩n lineal simple",
    "section": "Test relacionados con los residuos",
    "text": "Test relacionados con los residuos\nEl an치lisis de residuos mediante gr치ficos es inherentemente subjetivo. A칰n as칤, este an치lisis subjetivo de una variedad de gr치ficos de residuos frecuentemente revela dificultades en la implementaci칩n del modelo m치s claramente que un test formal.\n\nTest de aleatoriedad: Durbin-Watson Test\nTest para la consistencia de varianza: Brown-Forsythe test y Breusch-Pagan test\nTest de normalidad: Test Chi-cuadrado, Kolmogorov-Smirnov, Lilliefors test."
  },
  {
    "objectID": "slides/lec_week10.html#medidas-correctivas",
    "href": "slides/lec_week10.html#medidas-correctivas",
    "title": "Regresi칩n lineal simple",
    "section": "Medidas correctivas",
    "text": "Medidas correctivas\nSi el modelo de regresi칩n lineal simple no es apropiado para el conjunto de datos que se est치 analizando, se tienen dos opciones:\n\nAbandonar el modelo de regresi칩n lineal simple y desarrollar otro modelo\nAplicar alguna transformaci칩n a los datos tal que el modelo de regresi칩n lineal simple sea apropiado para los datos transformados."
  }
]